********************************************
def getTicket():
    # put the ip address or dns of your apic-em controller in this url
    url = "https://" + controller + "/api/v1/ticket"

    #the username and password to access the APIC-EM Controller
    payload = {"username":"usernae","password":"password"}

    #Content type must be included in the header
    header = {"content-type": "application/json"}

    #Performs a POST on the specified url to get the service ticket
    response= requests.post(url,data=json.dumps(payload), headers=header, verify=False)

    #convert response to json format
    r_json=response.json()

    #parse the json to get the service ticket
    ticket = r_json["response"]["serviceTicket"]

    return ticket 
********************************************
********************************************
def resolve_reference_http(cls, design_uri):
        """Retrieve design documents from http/https endpoints.

        Return a byte array of the response content. Support unsecured or
        basic auth

        :param design_uri: Tuple as returned by urllib.parse for the design reference
        """
        if design_uri.username is not None and design_uri.password is not None:
            response = requests.get(
                design_uri.geturl(),
                auth=(design_uri.username, design_uri.password),
                timeout=get_client_timeouts())
        else:
            response = requests.get(
                design_uri.geturl(), timeout=get_client_timeouts())

        return response.content 
********************************************
********************************************
def open_url(url: str) -> Iterator[BytesIO]:
    parsed_url = requests.utils.urlparse(url)
    if parsed_url.scheme == 'file':
        assert parsed_url.netloc == '', f'Bad file URL: {url}'
        with open(requests.utils.unquote(parsed_url.path), 'rb') as infile:
            yield infile
    elif parsed_url.scheme in ['http', 'https']:
        # verify=True is the default, but I want to be explicit about HTTPS,
        # since this function receives GPG key material.
        with requests.get(url, stream=True, verify=True) as r:
            r.raise_for_status()
            yield r.raw  # A file-like `io`-style object for the HTTP stream
            if r.raw.isclosed():   # Proxy for "all data was consumed"
                # Sadly, requests 2.x does not verify content-length :/
                # We could check r.raw.length_remaining, likely equivalent.
                actual_size = r.raw.tell()
                header_size = int(r.headers['content-length'])
                assert actual_size == header_size, (actual_size, header_size)
    else:  # pragma: no cover
        raise RuntimeError(f'Unknown URL scheme in {url}') 
********************************************
********************************************
def __init__(self, url, method='get', data=None, params=None,
                 headers=None, content_type='application/json', **kwargs):
        self.url = url
        self.method = method
        self.params = params or {}
        self.kwargs = kwargs

        if not isinstance(headers, dict):
            headers = {}
        self.headers = CaseInsensitiveDict(headers)
        if content_type:
            self.headers['Content-Type'] = content_type
        if data:
            self.data = json.dumps(data)
        else:
            self.data = {} 
********************************************
********************************************
def __call__(self, client, dnode):
        logger.info('Test download speed :  running...')
        start = time.clock()
        r = requests.get('http://{}'.format(dnode), stream=True)
        total_length = int(r.headers.get('content-length'))
        if total_length is None:
            logger.error("Empty file!")
        else:
            array_speed = []
            start_chunk = time.clock()
            for chunk in r.iter_content(1024):  # 1kB1024 1MB 1048576
                end_chunk = time.clock()
                delta = end_chunk - start_chunk
                start_chunk = end_chunk
                if delta <= 0:
                    break
                else:
                    array_speed.append(1//delta)  # kB / s

            end = time.clock()
            yield from self._queue.put(self.get_result(dnode, start, end, total_length, array_speed)) 
********************************************
********************************************
def acceleration(self, array_speed):
        """Caculate acceleration.

        By get the highest speed in the first cycle.

        Args:
            array_speed (list): list download times for each 1024 Byte

        Returns:
            acceleration (kB/s) : the deviation between highest speed and first byte speed
        """

        if len(array_speed) == 0:
            return 0
        speed_before = array_speed[0]
        for speed in array_speed:
            if speed < speed_before:
                break
            else:
                speed_before = speed

        return speed_before - array_speed[0] 
********************************************
********************************************
def get_by_url(url):
    # Get show data from svtplay.se.
    r = requests.get('%s?type=embed&output=json' % (url))
    r.raise_for_status()

    response_json = r.json()
    video = response_json.get('video')

    # Get the highest quality video stream.
    for vr in video.get('videoReferences'):
        if vr.get('playerType') == 'ios':
            unscrubbed_url = vr.get('url')
            try:
                # remove all getvars from link
                scrubbed_url = unscrubbed_url[:unscrubbed_url.index('.m3u8') + 5]
                return scrubbed_url
            except IndexError:
                if unscrubbed_url:
                    print('Stream url used old format without alt getvar. Trying old style...')
                    return unscrubbed_url
                else:
                    print('Empty url to stream. Exiting.') 
********************************************
********************************************
def test_repomd(self):
        content = b'An abacus falls from a fig tree'
        timestamp = 1234567890
        with self.repo_server_thread({
            'repomd.xml': {
                'size': len(content),
                'build_timestamp': timestamp,
                'content_bytes': content,
            }
        }) as (host, port):
            req = requests.get(f'http://{host}:{port}/repomd.xml')
            req.raise_for_status()
            self.assertEqual(content, req.content)
            self.assertEqual(
                timestamp,
                email.utils.parsedate_to_datetime(req.headers['last-modified'])
                    .timestamp(),
            )
            self.assertEqual('text/xml', req.headers['content-type']) 
********************************************
********************************************
def _check_bad_blob(self, bad_blob):
        with self.repo_server_thread({'bad_blob': bad_blob}) as (host, port):
            # Drive-by test of HEAD requests -- note that this doesn't
            # detect the error yet, so the next GET "succeeds".
            req_head = requests.head(f'http://{host}:{port}/bad_blob')
            req = requests.get(f'http://{host}:{port}/bad_blob')
            self.assertEqual(req_head.status_code, req.status_code)
            self.assertEqual(_no_date(req_head.headers), _no_date(req.headers))
            # You'd think that `requests` would error on this, but, no...
            # https://blog.petrzemek.net/2018/04/22/
            #   on-incomplete-http-reads-and-the-requests-library-in-python/
            self.assertEqual(200, req.status_code)
            self.assertLess(
                req.raw.tell(),  # Number of bytes that were read
                int(req.headers['content-length']),
            )
            # Ok, so we didn't get enough bytes, let's retry. This verifies
            # that the server memoizes integrity errors correctly.
            req = requests.get(f'http://{host}:{port}/bad_blob')
            self.assertEqual(500, req.status_code)
            self.assertIn(b'file_integrity', req.content)
            return req.content.decode() 
********************************************
********************************************
def init_inventory_container(container,headers=None, org_name=None):
    
    initialize_config()
    
    headers = headers if headers else TSC_HEADERS
    org_name = org_name if org_name else ORG_NAME
    
    def _container_url(container_id):
            return 'https://secure.transcriptic.com/{}/samples/{}.json'.format(org_name, container_id)

    response = requests.get(_container_url(container.id), headers=headers, verify=False)
    response.raise_for_status()

    container_json = response.json()   
    
    container.cover = container_json['cover']
    
    for well in container.all_wells():
        init_inventory_well(well,container_json=container_json)
    
   
#@TODO: this needs to be mocked in tests since it hits the transcriptic api 
********************************************
********************************************
def __init__(self, email: str, api_key: str, domain: str, proxied: bool):
        """
        Initialization. It will set the zone information of the domain for operation.
        It will also get dns records of the current zone.
        :param email:
        :param api_key:
        :param domain:
        :param proxied:
        """
        self.email = email
        self.api_key = api_key
        self.domain = domain
        self.proxied = proxied
        self.headers = {
            'X-Auth-Key': api_key,
            'X-Auth-Email': email
        }
        self.setup_zone() 
********************************************
********************************************
def setup_zone(self):
        """
        Setup zone for current domain.
        It will also setup the dns records of the zone
        :return:
        """
        # Initialize current zone
        zones_content = self.request(self.api_url, 'get')
        try:
            if len(self.domain.split('.')) == 3:
                domain = self.domain.split('.', 1)[1]
            else:
                domain = self.domain
            zone = [zone for zone in zones_content['result'] if zone['name'] == domain][0]
        except IndexError:
            raise ZoneNotFound('Cannot find zone information for the domain {domain}.'
                               .format(domain=self.domain))
        self.zone = zone

        # Initialize dns_records of current zone
        dns_content = self.request(self.api_url + zone['id'] + '/dns_records', 'get')
        self.dns_records = dns_content['result'] 
********************************************
********************************************
def convert_ensembl_to_entrez(self, ensembl):
        """Convert Ensembl Id to Entrez Gene Id"""
        if 'ENST' in ensembl:
            pass
        else:
            raise (IndexError)
        # Submit resquest to NCBI eutils/Gene database
        server = "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?" + self.options + "&db=gene&term={0}".format(
            ensembl)
        r = requests.get(server, headers={"Content-Type": "text/xml"})
        if not r.ok:
            r.raise_for_status()
            sys.exit()
        # Process Request
        response = r.text
        info = xmltodict.parse(response)
        try:
            geneId = info['eSearchResult']['IdList']['Id']
        except TypeError:
            raise (TypeError)
        return geneId 
********************************************
********************************************
def convert_hgnc_to_entrez(self, hgnc):
        """Convert HGNC Id to Entrez Gene Id"""
        entrezdict = {}
        server = "http://rest.genenames.org/fetch/hgnc_id/{0}".format(hgnc)
        r = requests.get(server, headers={"Content-Type": "application/json"})
        if not r.ok:
            r.raise_for_status()
            sys.exit()
        response = r.text
        info = xmltodict.parse(response)
        for data in info['response']['result']['doc']['str']:
            if data['@name'] == 'entrez_id':
                entrezdict[data['@name']] = data['#text']
            if data['@name'] == 'symbol':
                entrezdict[data['@name']] = data['#text']
        return entrezdict 
********************************************
********************************************
def convert_uniprot_to_entrez(self, uniprot):
        """Convert Uniprot Id to Entrez Id"""
        # Submit request to NCBI eutils/Gene Database
        server = "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?" + self.options + "&db=gene&term={0}".format(
            uniprot)
        r = requests.get(server, headers={"Content-Type": "text/xml"})
        if not r.ok:
            r.raise_for_status()
            sys.exit()
        # Process Request
        response = r.text
        info = xmltodict.parse(response)
        geneId = info['eSearchResult']['IdList']['Id']
        # check to see if more than one result is returned
        # if you have more than more result then check which Entrez Id returns the same uniprot Id entered.
        if len(geneId) > 1:
            for x in geneId:
                c = self.convert_entrez_to_uniprot(x)
                c = c.lower()
                u = uniprot.lower()
                if c == u:
                    return x
        else:
            return geneId 
********************************************
********************************************
def resolve_reference_ucp(cls, design_uri):
        """Retrieve artifacts from a Airship service endpoint.

        Return a byte array of the response content. Assumes Keystone
        authentication required.

        :param design_uri: Tuple as returned by urllib.parse for the design reference
        """
        ks_sess = KeystoneUtils.get_session()
        (new_scheme, foo) = re.subn(r'^[^+]+\+', '', design_uri.scheme)
        url = urllib.parse.urlunparse(
            (new_scheme, design_uri.netloc, design_uri.path, design_uri.params,
             design_uri.query, design_uri.fragment))
        LOG.debug("Calling Keystone session for url %s" % str(url))
        resp = ks_sess.get(url, timeout=get_client_timeouts())
        if resp.status_code >= 400:
            raise errors.InvalidDesignReference(
                "Received error code for reference %s: %s - %s" %
                (url, str(resp.status_code), resp.text))
        return resp.content 
********************************************
********************************************
def client():
    with vcr.use_cassette('tests/fixtures/vcr_cassettes/client.yaml'):
        request = requests.get(URL,
                            headers={
                                'Host': 'swapi.graphene-python.org',
                                'Accept': 'text/html',
                            })
        request.raise_for_status()
        csrf = request.cookies['csrftoken']

        return Client(
            transport=RequestsHTTPTransport(url=URL,
                                            cookies={"csrftoken": csrf},
                                            headers={'x-csrftoken':  csrf}),
            fetch_schema_from_transport=True
        ) 
********************************************
********************************************
def __init__(self, name, site='https://roll20.net/compendium/dnd5e/'):
        self.name = name.rstrip().title()
        formatted_name = self.name.replace(' ', '_')
        url = site + formatted_name
        page = requests.get(url)
        if page.status_code != 200:
            raise IOError('{:s} not found at {:s}.'.format(name,
                                                           url))
        if 'marketplace' in page.url:
              raise IOError('{:s} not found at {:s}, '
                            'likely because this content is behind a paywall. '
                            'Encourage developer to add alternative back-ends to Roll20'.format(name, url))
        html = page.text
        soup = bs(html, 'html.parser')
        self.attributes = ({stringify(a.text):
                            stringify(a.find_next(attrs={'class':
                                                         'value'}).text)
                            for a in soup.find_all(attrs={'class':
                                                          'col-md-3 attrName'})})
        self.desc = '\n'.join([stringify(val.text)
                               for val in soup.find_all(id='origpagecontent',
                                                        attrs={'type':
                                                               'text/html'})]) 
********************************************
********************************************
def str_attributes(self):
        res = ''
        for k in ['HP', 'AC', 'Speed', 'Challenge Rating']:
            res += k + ': ' + self.get(k, 'EMPTY') + '\n'
        res += '\n'
        for k in ['STR', 'DEX', 'CON', 'INT', 'WIS', 'CHA']:
            res += k + '\t'
        res += '\n'
        for k in ['STR', 'DEX', 'CON', 'INT', 'WIS', 'CHA']:
            s = self.get(k, None)
            res += '{:s} ({:s})\t'.format(s, score_to_mod(int(s)))
        res += '\n\n'
        for k in ['Type', 'Size', 'Alignment', 'Senses', 'Skills',
                  'Languages']:
            res += k + ': ' + self.get(k, 'EMPTY') + '\n'
        return res 
********************************************
********************************************
def as_dungeonsheets_class(self):
        spell_name = self.name
        class_name = spell_name.replace(' ', '').replace('-', '')
        res = 'class {:s}(Spell):\n'.format(class_name)
        res += "    \"\"\"{:s}\n    \"\"\"\n".format(self.desc.replace('\n', '\n    '))
        res += "    name = \"{:s}\"\n".format(spell_name)
        res += "    level = {:d}\n".format(int(self.get('Level', -1)))
        res += "    casting_time = \"{:s}\"\n".format(self.get('Casting Time', '1 action'))
        res += "    casting_range = \"{:s}\"\n".format(self.get('Range', ''))
        str_components = self.get('Components', '').upper().split(' ')
        if len(str_components) == 0:
            res += "    components = ()\n"
        else:
            res += "    components = {:s}\n".format(str(tuple(str_components)))
        res += "    materials = \"\"\"{:s}\"\"\"\n".format(self.get('Material', ''))
        dur_text = "\"{:s}\"\n".format(self.get('Duration', 'Instantaneous'))
        dur_text = ("\"Concentration, {:s}".format(dur_text.lstrip('\"')) if
                    self.get('Concentration', '') else dur_text)
        duration = "    duration = " + dur_text
        res += duration
        res += "    ritual = {:}\n".format(bool(self.get('Ritual', '')))
        res += "    magic_school = \"{:s}\"\n".format(self.get('School', ''))
        res += "    classes = {:s}\n".format(str(tuple(self.get('Classes', '').split(', '))))
        return res + "\n" 
********************************************
********************************************
def get_vmprofiles(self):
        url = 'https://%s/php/vmprofiles.php' % self.atdserver

        try:
            r = requests.get(url, headers=self.sessionhdr, verify=False)
        except Exception as e:
            error_info = 'Error getting vmprofiles:\n%s' % e
            return(0, error_info)

        if r.status_code == 200:
            server_info = json.loads(r.content)

            if server_info['success'] is True:
                return (1, server_info['results'])

            else:
                error_info = 'Error getting vmprofiles, check credentials or content type header'
                return (0, error_info)
        else:
            error_info = 'Error getting vmprofiles, status code: %d' % r.status_code
            return (0, error_info) 
********************************************
********************************************
def getNetworkDevices(ticket):
    # URL for network device REST API call to get list of existing devices on the network.
    url = "https://" + controller + "/api/v1/network-device"

    #Content type must be included in the header as well as the ticket
    header = {"content-type": "application/json", "X-Auth-Token":ticket}

    # this statement performs a GET on the specified network-device url
    response = requests.get(url, headers=header, verify=False)

    # json.dumps serializes the json into a string and allows us to
    # print the response in a 'pretty' format with indentation etc.
    print ("Network Devices = ")
    print (json.dumps(response.json(), indent=4, separators=(',', ': ')))

  #convert data to json format.
    r_json=response.json()

  #Iterate through network device data and print the id and series name of each device
    for i in r_json["response"]:
        print(i["id"] + "   " + i["series"])

#call the functions 
********************************************
********************************************
def __call__(self, client, dnode):
        logger.info('Caculating time for download first byte...')
        r = requests.get('http://{}'.format(dnode), stream=True)
        total_length = int(r.headers.get('content-length'))
        if total_length is None:
            logger.info("empty file!")
        else:
            start_chunk = time.clock()
            for chunk in r.iter_content(1024):  # 1kB1024 1MB 1048576
                end_chunk = time.clock()
                break

            delta = end_chunk - start_chunk  # time to first byte
            yield from self._queue.put(self.get_result(dnode, delta)) 
********************************************
********************************************
def get_client_id():
    r = requests.get(CDN_BASE + 'global.js')
    if r.status_code >= 400:
        raise Exception('Error fetching global.js script.')

    # Find the client ID with a regex that totally wont match anything else /s
    client_ids = re.findall(r'clientID:"(\w*)"', r.text)
    if len(client_ids) != 1:
        raise Exception(
            'Error finding client ID in twitch global-frontend script. Got {}'.format(client_ids))
    return client_ids[0] 
********************************************
********************************************
def get_token_and_signature(channel, client_id):
    url = TOKEN_API.format(channel=channel)
    headers = {'Client-ID': client_id}
    r = requests.get(url, headers=headers)
    if r.status_code >= 400:
        raise Exception('Error requesting token from twitch: {}'.format(r.text))
    data = r.json()
    return data['token'], data['sig'] 
********************************************
********************************************
def get_live_stream(channel):
    client_id = get_client_id()
    token, sig = get_token_and_signature(channel, client_id)
    url = USHER_API.format(channel=channel, sig=sig, token=token, random=random.randint(0, 1E7))
    r = requests.get(url)
    m3u8_obj = m3u8.loads(r.text)
    return m3u8_obj 
********************************************
********************************************
def get_api_data(self):
        """
        Gets json file containing server information

        :return: server information in json format
        """
        try:
            resp = requests.get(api, timeout=5)
            if resp.status_code == requests.codes.ok:
                return resp.json()
            else:
                self.statusbar.showMessage("Get API failed", 2000)
        except Exception as ex:
            self.statusbar.showMessage("Get API failed", 2000) 
********************************************
********************************************
def get_ovpn(self):
        """
        Gets ovpn file from nord servers and saves it to a temporary location
        """
        # https://downloads.nordcdn.com/configs/files/ovpn_udp/servers/sg173.nordvpn.com.udp.ovpn
        self.ovpn_path = None
        ovpn_url = None
        udp_url = 'https://downloads.nordcdn.com/configs/files/ovpn_udp/servers/'
        tcp_url = 'https://downloads.nordcdn.com/configs/files/ovpn_tcp/servers/'
        udp_xor_url = 'https://downloads.nordcdn.com/configs/files/ovpn_xor_udp/servers/'
        tcp_xor_url = 'https://downloads.nordcdn.com/configs/files/ovpn_xor_tcp/servers/'

        if (self.server_type_select.currentText() == 'Obfuscated Server') and (self.connection_type_select.currentText() == 'UDP'):
            ovpn_url = udp_xor_url
        elif (self.server_type_select.currentText() == 'Obfuscated Server') and (self.connection_type_select.currentText() == 'TCP'):
            ovpn_url = tcp_xor_url
        elif (self.server_type_select.currentText() != 'Obfuscated Server') and (self.connection_type_select.currentText() == 'UDP'):
            ovpn_url = udp_url
        elif (self.server_type_select.currentText() != 'Obfuscated Server') and (self.connection_type_select.currentText() == 'TCP'):
            ovpn_url = tcp_url

        if self.connection_type_select.currentText() == 'UDP':
            filename = self.domain_list[self.server_list.currentRow()] + '.udp.ovpn'
            ovpn_file = requests.get(ovpn_url + filename, stream=True)
            if ovpn_file.status_code == requests.codes.ok:
                self.ovpn_path = os.path.join(self.config_path, filename)
                with open(self.ovpn_path, 'wb') as out_file:
                    shutil.copyfileobj(ovpn_file.raw, out_file)
            else: self.statusbar.showMessage('Error fetching configuration files', 2000)

        elif self.connection_type_select.currentText() == 'TCP':
            filename = self.domain_list[self.server_list.currentRow()] + '.tcp.ovpn'
            ovpn_file = requests.get(ovpn_url + filename, stream=True)
            if ovpn_file.status_code == requests.codes.ok:
                self.ovpn_path = os.path.join(self.config_path, filename)
                with open(self.ovpn_path, 'wb') as out_file:
                    shutil.copyfileobj(ovpn_file.raw, out_file)
            else: self.statusbar.showMessage('Error fetching configuration files', 2000)

        self.server_list.setFocus() 
********************************************
********************************************
def updateFactorio():
	

	file_name = "/tmp/latestFactorio.tar.gz"
	print("Downloading %s" % file_name)

	r = requests.get(DOWNLOADURL, stream=True)
	total_length = int(r.headers.get('content-length'))

	if not os.path.isfile(file_name) or total_length != os.path.getsize(file_name):
		with open(file_name, 'wb') as f:
			for chunk in progress.bar(r.iter_content(chunk_size=1024), expected_size=(total_length/1024) + 1): 
				if chunk:
					f.write(chunk)
					f.flush()
			#os.chmod(file_name, stat.S_IWUSR | stat.S_IRUSR)
	else:
		print("File already exists and file sizes match. Skipping download.")	

	if os.access(FACTORIOPATH, os.W_OK):
		if os.path.isfile(file_name):
			tar = tarfile.open(file_name, "r:gz")
			tar.extractall(path="/tmp")
			tar.close()

			copytree("/tmp/factorio", FACTORIOPATH)
			print("Success.")
		else:
			print("Help! Can't find %s, but I should have!" % (file_name))
			sys.exit(1)			
	else:
		print("Can't write to %s" % (FACTORIOPATH))
		sys.exit(1) 
********************************************
********************************************
def get_dict_optional_value(d,keys_to_try_in_order, default_value=None):
    """
    Tries each key in order, if not value is found, returns default_value
    """
    
    for key in keys_to_try_in_order:
        if key in d and d.get(key):
            return d[key]
        
    return default_value 
********************************************
********************************************
def get_melting_temp(dna_sequence, oligo_concentration_uM,
                     pcr_settings='q5'):
    global IDT_COOKIE
    
    #@TODO: update this to use http://tmcalculator.neb.com/#!/ since its probably accounting for the salt concentration in the buffer
    
    if not IDT_COOKIE:
        r = requests.get('http://www.idtdna.com/calc/analyzer',allow_redirects=False)
        IDT_COOKIE = r.cookies['ASP.NET_SessionId']       
        
    headers = {"content-type":'application/json','Cookie':'ASP.NET_SessionId=%s'%IDT_COOKIE}
    
    idt_response = requests.post('https://www.idtdna.com/calc/analyzer/home/analyze',json={
        "settings": {
            "Sequence": dna_sequence,
            "NaConc": 0,
            "MgConc": 2,
            "DNTPsConc": 5, 
            "OligoConc": oligo_concentration_uM,
            "NucleotideType": "DNA"
        }
    }, verify=False
    ,headers=headers)    
    
    idt_response.raise_for_status()
    
    return idt_response.json()['MeltTemp'] 
********************************************
********************************************
def do(self):
        # logging.debug("{} {}: \n\tHeaders {}, \n\tData {}, \n\tParams {}, \n\tOther: {}".format(
        #     self.method.upper(), self.url, self.headers,
        #     self.data, self.params, self.kwargs
        # ))
        result = self.methods.get(self.method)(
            url=self.url, headers=self.headers,
            data=self.data, params=self.params,
            **self.kwargs
        )
        return result 
********************************************
********************************************
def do(self, api_name=None, pk=None, method='get', use_auth=True,
           data=None, params=None, content_type='application/json', **kwargs):

        if api_name in API_URL_MAPPING:
            path = API_URL_MAPPING.get(api_name)
            if pk and '%s' in path:
                path = path % pk
        else:
            path = api_name

        request_headers = kwargs.get('headers', {})
        default_headers = self.default_headers or {}
        headers = {k: v for k, v in default_headers.items()}
        headers.update(request_headers)
        kwargs['headers'] = headers
        url = self.endpoint.rstrip('/') + path
        req = HttpRequest(url, method=method, data=data,
                          params=params, content_type=content_type,
                          **kwargs)
        if use_auth:
            if not self.auth:
                msg = 'Authentication required, but not provide'
                logger.error(msg)
                raise RequestError(msg)
            else:
                self.auth.sign_request(req)

        try:
            resp = req.do()
        except (requests.ConnectionError, requests.ConnectTimeout) as e:
            msg = "Connect endpoint {} error: {}".format(self.endpoint, e)
            logger.error(msg)
            raise RequestError(msg)

        return self.clean_result(resp) 
********************************************
********************************************
def get(self, *args, **kwargs):
        kwargs['method'] = 'get'
        return self.do(*args, **kwargs) 
********************************************
********************************************
def create_record(self, dns_type, name, content, **kwargs):
        """
        Create a dns record
        :param dns_type:
        :param name:
        :param content:
        :param kwargs:
        :return:
        """
        data = {
            'type': dns_type,
            'name': name,
            'content': content
        }
        if kwargs.get('ttl') and kwargs['ttl'] != 1:
            data['ttl'] = kwargs['ttl']
        if kwargs.get('proxied') is True:
            data['proxied'] = True
        else:
            data['proxied'] = False
        content = self.request(
            self.api_url + self.zone['id'] + '/dns_records',
            'post',
            data=data
        )
        print('DNS record successfully created')
        return content['result'] 
********************************************
********************************************
def update_record(self, dns_type, name, content, **kwargs):
        """
        Update dns record
        :param dns_type:
        :param name:
        :param content:
        :param kwargs:
        :return:
        """
        record = self.get_record(dns_type, name)
        data = {
            'type': dns_type,
            'name': name,
            'content': content
        }
        if kwargs.get('ttl') and kwargs['ttl'] != 1:
            data['ttl'] = kwargs['ttl']
        if kwargs.get('proxied') is True:
            data['proxied'] = True
        else:
            data['proxied'] = False
        content = self.request(
            urllib.parse.urljoin(self.api_url, self.zone['id'] + '/dns_records/' + record['id']),
            'put',
            data=data
        )
        print('DNS record successfully updated')
        return content['result'] 
********************************************
********************************************
def search(self, type, q, territory='TW'):
        response = requests.get(self.API_BASE_URL + 'search', params={'type': type, 'q': q, 'territory': territory},
                                headers={'Authorization': 'Bearer ' + self.token})
        response.raise_for_status()
        response_json = response.json()

        if type == 'artist':
            return response_json['artists']['data'][0]['url']
        else:
            id = response_json[type + 's']['data'][0]['id']
            return 'https://widget.kkbox.com/v1/?id=' + id \
                   + '&type=' + ('song' if type == 'track' else type) 
********************************************
********************************************
def _get_cache_stale_secs(cache_stale=None):
    # overrides config
    caching_val = config.CONFIG.parser.get('cache', 'normal')
    if caching_val in ('never', 'false', 'False', 'off', 'Off'):
        return 0
    if caching_val in ('test', 'forever'):
        return CACHE_FOREVER
    if cache_stale is None:
        return 0
    return cache_stale 
********************************************
********************************************
def request_json(url, output_filename=None, cache_stale=None):
    """Sends a request expecting a json-formatted response.
    If output_filename is given, then the output is saved to file.
    This also enables basic caching, where cache_stale is the number of seconds
    since file is last modified before the cached file is considered stale (0 means disable the cache).
    """
    cache_stale = _get_cache_stale_secs(cache_stale)
    # Guard against very long filenames:
    if output_filename and len(output_filename) >= MAX_CACHE_FILENAME_LEN:
        output_filename = output_filename[0:MAX_CACHE_FILENAME_LEN-1]
    if output_filename and cache_stale:
        if output_filename in CACHE:
            return CACHE[output_filename]
        json_file = os.path.join(_get_cachedir(), '{}.json'.format(output_filename))
        if os.path.exists(json_file) and (int(time.time()) - os.path.getmtime(json_file) < cache_stale):
            with open(json_file) as jfh:
                CACHE[output_filename] = json.load(jfh)
            if config.DEBUG:
                LOG.info('Loaded from cache: %s', output_filename)
            return CACHE[output_filename]

    LOG.debug('Getting url=%s ...', url)
    headers = {
        'User-Agent': config.CONFIG.ua_iphone,
        'Connection': 'close'
    }
    util.log_http(url, 'get', headers, sys._getframe().f_code.co_name)
    response = requests.get(url, headers=headers, verify=config.VERIFY_SSL)
    response.raise_for_status()

    # Note: this fails on windows in some cases https://github.com/kennethreitz/requests-html/issues/171
    if output_filename is not None or (config.DEBUG and config.SAVE_JSON_FILE):
        json_file = os.path.join(_get_cachedir(), '{}.json'.format(output_filename))
        with open(json_file, 'w', encoding='utf-8') as out:  # write date to json_file
            out.write(response.text)
    if cache_stale:
        LOG.debug('Caching url=%s, filename=%s', url, output_filename)
        CACHE[output_filename] = response.json()
        return CACHE[output_filename]
    return response.json() 
********************************************
********************************************
def get_attrib(self, et_node, prefixed_attrib):
        """Get a prefixed attribute like 'rdf:resource' from ET node."""
        prefix, attrib = prefixed_attrib.split(':')
        return et_node.get('{{{0}}}{1}'.format(self.namespaces[prefix],
                                               attrib)) 
********************************************
********************************************
def __init__(self, namespaces=None, cc_resolver=None, source=None):
        """Init the remote loader."""
        super(RemoteFundRefLoader, self).__init__(
            namespaces=namespaces, cc_resolver=cc_resolver)
        self.source = source or \
            current_app.config['OPENAIRE_FUNDREF_ENDPOINT']
        headers = {"Content-Type": "application/rdf+xml"}
        obj = requests.get(self.source, stream=True, headers=headers)
        funders_xml = obj.text.encode('utf-8')
        self.doc_root = ET.fromstring(funders_xml) 
********************************************
********************************************
def resolve_by_id(self, funder_id):
        """Resolve the funder from the OpenAIRE funder id.

        If funder_id can be resolved, return a URI otherwise return None.
        """
        return self.data.get(funder_id) 
********************************************
********************************************
def resolve_by_oai_id(self, oai_id):
        """Resolve the funder from the OpenAIRE OAI record id.

        Hack for when funder is not provided in OpenAIRE.
        """
        if oai_id.startswith('oai:dnet:'):
            oai_id = oai_id[len('oai:dnet:'):]
        prefix = oai_id.split("::")[0]
        suffix = prefix.replace("_", "").upper()
        oaf = "{0}::{1}".format(prefix, suffix)
        return self.data.get(oaf) 
********************************************
********************************************
def resolve_by_doi(self, doi):
        """Resolve a DOI to an OpenAIRE id."""
        return self.inverse_data.get(doi) 
********************************************
********************************************
def forward(self, text):

        '''
        In PyTorch RNNs want the input with batch dim second, CNNs want the batch dim first
        we permute the input to make it the right shape for the CNN
        '''
        text = text.permute(1, 0)

        # Text passed through embedding layer to get embeddings
        embedded = self.embedding(text)

        '''
        A conv layer wants the second dim of the input to be a channel dim
        text does not have a channel dim, so the tensor is unsqueezed to create one
        '''
        embedded = embedded.unsqueeze(1)

        # Iterates through the list of conv layers applying each conv layer to get list of conv outputs
        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]

        '''
        Conv outputs are passed through a max pooling that takes the maximum value over a dimension
        the idea being that the "maximum value" is the most important feature for determining the sentiment
        which corresponds to the most important n-gram in the review
        '''
        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]

        '''
        The model has 100 filters of 3 different sizes, therefore 300 n-grams that could be important
        which we concatenate into a single vector and pass through a dropout layer and finally a linear layer
        (NOTE: dropout is set to 0 during inference time)
        '''
        cat = self.dropout(torch.cat(pooled, dim = 1))

        # passed through linear layer to make predictions
        return self.fc(cat) 
********************************************
********************************************
def get_soup(any_url):
    # 传入链接
    # 返回BeautifulSoup对象
    result = requests.get(any_url, headers=header[random.randint(0, 4)])
    html_doc = result.content
    try:
        html_doc = html_doc.decode('utf-8')
    except UnicodeDecodeError:
        html_doc = html_doc.decode('gbk')
    soup = BeautifulSoup(html_doc, 'html.parser')
    return soup 
********************************************
********************************************
def get_soup(any_url):
    # 传入链接
    # 返回BeautifulSoup对象
    result = requests.get(any_url, headers=header[random.randint(0, 4)])
    html_doc = result.content
    try:
        html_doc = html_doc.decode('utf-8')
    except UnicodeDecodeError:
        html_doc = html_doc.decode('gbk')
    soup = BeautifulSoup(html_doc, 'html.parser')
    return soup 
********************************************
********************************************
def get_soup(any_url):
    # 传入链接
    # 返回BeautifulSoup对象
    result = requests.get(any_url, headers=header[random.randint(0, 4)])
    html_doc = result.content
    try:
        html_doc = html_doc.decode('utf-8')
    except UnicodeDecodeError:
        html_doc = html_doc.decode('gbk')
    soup = BeautifulSoup(html_doc, 'html.parser')
    return soup 
********************************************
********************************************
def nodeLatestVersion(dependency, project_id):
    r = requests.get('%s%s/latest' % (app.config['NPM_REGISTRY'], dependency))
    latestVersion = r.json().get('version')

    try:
        dep = ProjectDependency.by_project(project_id, dependency)
        dep.latest_version = latestVersion
        if LooseVersion(dep.actual_version) < LooseVersion(latestVersion):
            dep.status = 'ko'
        else:
            dep.status = 'ok'
        db.session.commit()
    except Exception, e:
        app.logger.error(e)
        db.session.rollback() 
********************************************
********************************************
def nodeDepsFetcher(project_id):
    # Get dependencies from package.json
    project = git.getproject(project_id)

    depFileEncoded = git.getfile(project_id, 'package.json',
                                 project['default_branch'])

    # Decode from base64
    deps = json.loads(depFileEncoded.get('content').decode('base64'))

    mainDeps = deps.get('dependencies')
    devDeps = deps.get('devDependencies')

    # Insert in project_dependency
    # TODO create single function for that
    for mDep, mVersion in list(mainDeps.items()):
        mdep, created = get_or_create(db.session, ProjectDependency,
                                      project_id=project_id, name=mDep,
                                      actual_version=mVersion)

        if not created:
            app.logger.info('[%s] Dep %s already exist' % (project_id, mDep))

        db.session.commit()
        nodeLatestVersion(mDep, project_id)

    for devDep, devVersion in list(devDeps.items()):
        ddep, created = get_or_create(db.session, ProjectDependency,
                                      project_id=project_id, name=devDep,
                                      actual_version=devVersion, dev=True)

        if not created:
            app.logger.info('[%s] Dev dep %s already exist' %
                            (project_id, devDep))

        db.session.commit()
        nodeLatestVersion(devDep, project_id)
    return True 
********************************************
********************************************
def PS1cutouts(ra,dec,filt):

    print '\nSearching for PS1 images of field...\n'

    ps1_url = 'http://ps1images.stsci.edu/cgi-bin/ps1filenames.py?'

    ps1_url += '&ra='+str(ra)
    ps1_url += '&dec='+str(dec)
    ps1_url += '&filters='+filt

    ps1_im = requests.get(ps1_url)

    try:
        image_name = ps1_im.text.split()[17]

        print 'Image found: ' + image_name + '\n'

        cutout_url = 'http://ps1images.stsci.edu/cgi-bin/fitscut.cgi?&filetypes=stack&size=2500'

        cutout_url += '&ra='+str(ra)
        cutout_url += '&dec='+str(dec)
        cutout_url += '&filters='+filt
        cutout_url += '&format=fits'
        cutout_url += '&red='+image_name

        dest_file = filt + '_template.fits'

        cmd = 'wget -O %s "%s"' % (dest_file, cutout_url)

        os.system(cmd)

        print 'Template downloaded as ' + dest_file + '\n'

    except:
        print '\nPS1 template search failed!\n' 
********************************************
********************************************
def address_to_coords(self, address):
        """Convert address to coordinates"""

        base_coords = self.BASE_COORDS[self.region]
        get_cord = self.COORD_SERVERS[self.region]
        url_options = {
            "q": address,
            "lang": "eng",
            "origin": "livemap",
            "lat": base_coords["lat"],
            "lon": base_coords["lon"]
        }

        response = requests.get(self.WAZE_URL + get_cord, params=url_options, headers=self.HEADERS)
        for response_json in response.json():
            if response_json.get('city'):
                lat = response_json['location']['lat']
                lon = response_json['location']['lon']
                bounds = response_json['bounds']  # sometimes the coords don't match up
                if bounds is not None:
                    bounds['top'], bounds['bottom'] = max(bounds['top'], bounds['bottom']), min(bounds['top'], bounds['bottom'])
                    bounds['left'], bounds['right'] = min(bounds['left'], bounds['right']), max(bounds['left'], bounds['right'])
                else:
                    bounds = {}
                return {"lat": lat, "lon": lon, "bounds": bounds}
        raise WRCError("Cannot get coords for %s" % address) 
********************************************
********************************************
def get_route(self, npaths=1, time_delta=0):
        """Get route data from waze"""

        routing_server = self.ROUTING_SERVERS[self.region]

        url_options = {
            "from": "x:%s y:%s" % (self.start_coords["lon"], self.start_coords["lat"]),
            "to": "x:%s y:%s" % (self.end_coords["lon"], self.end_coords["lat"]),
            "at": time_delta,
            "returnJSON": "true",
            "returnGeometries": "true",
            "returnInstructions": "true",
            "timeout": 60000,
            "nPaths": npaths,
            "options": ','.join('%s:t' % route_option for route_option in self.route_options),
        }
        if self.vehicle_type:
            url_options["vehicleType"] = self.vehicle_type
        # Handle vignette system in Europe. Defaults to false (show all routes)
        if self.avoid_subscription_roads is False:
            url_options["subscription"] = "*"

        response = requests.get(self.WAZE_URL + routing_server, params=url_options, headers=self.HEADERS)
        response.encoding = 'utf-8'
        response_json = self._check_response(response)
        if response_json:
            if 'error' in response_json:
                raise WRCError(response_json.get("error"))
            else:
                if response_json.get("alternatives"):
                    return [alt['response'] for alt in response_json['alternatives']]
                if npaths > 1:
                    return [response_json['response']]
                return response_json['response']
        else:
            raise WRCError("empty response") 
********************************************
********************************************
def _add_up_route(self, results, real_time=True, stop_at_bounds=False):
        """Calculate route time and distance."""

        start_bounds = self.start_coords['bounds']
        end_bounds = self.end_coords['bounds']

        def between(target, min, max):
            return target > min and target < max

        time = 0
        distance = 0
        for segment in results:
            if stop_at_bounds and segment.get('path'):
                x = segment['path']['x']
                y = segment['path']['y']
                if (
                    between(x, start_bounds.get('left', 0), start_bounds.get('right', 0)) or
                    between(x, end_bounds.get('left', 0), end_bounds.get('right', 0))
                ) and (
                    between(y, start_bounds.get('bottom', 0), start_bounds.get('top', 0)) or
                    between(y, end_bounds.get('bottom', 0), end_bounds.get('top', 0))
                ):
                    continue
            time += segment['crossTime' if real_time else 'crossTimeWithoutRealTime']
            distance += segment['length']
        route_time = time / 60.0
        route_distance = distance / 1000.0
        return route_time, route_distance 
********************************************
********************************************
def convert_entrez_to_uniprot(self, entrez):
        """Convert Entrez Id to Uniprot Id"""
        server = "http://www.uniprot.org/uniprot/?query=%22GENEID+{0}%22&format=xml".format(entrez)
        r = requests.get(server, headers={"Content-Type": "text/xml"})
        if not r.ok:
            r.raise_for_status()
            sys.exit()
        response = r.text
        info = xmltodict.parse(response)
        try:
            data = info['uniprot']['entry']['accession'][0]
            return data
        except TypeError:
            data = info['uniprot']['entry'][0]['accession'][0]
            return data 
********************************************
********************************************
def convert_accession_to_taxid(self, accessionid):
        """Convert Accession Id to Tax Id """
        # Submit request to NCBI eutils/Taxonomy Database
        server = "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?" + self.options + "&db=nuccore&id={0}&retmode=xml".format(
            accessionid)
        r = requests.get(server, headers={"Content-Type": "text/xml"})
        if not r.ok:
            r.raise_for_status()
            sys.exit()
        # Process Request
        response = r.text
        records = xmltodict.parse(response)
        try:
            for i in records['GBSet']['GBSeq']['GBSeq_feature-table']['GBFeature']['GBFeature_quals']['GBQualifier']:
                for key, value in i.items():
                    if value == 'db_xref':
                        taxid = i['GBQualifier_value']
                        taxid = taxid.split(':')[1]
                        return taxid
        except:
            for i in records['GBSet']['GBSeq']['GBSeq_feature-table']['GBFeature'][0]['GBFeature_quals']['GBQualifier']:
                for key, value in i.items():
                    if value == 'db_xref':
                        taxid = i['GBQualifier_value']
                        taxid = taxid.split(':')[1]
                        return taxid
        return 
********************************************
********************************************
def resolve_reference(cls, design_ref):
        """Resolve a reference to a design document.

        Locate a schema handler based on the URI scheme of the data reference
        and use that handler to get the data referenced.

        :param design_ref: A URI-formatted reference to a data entity
        """
        try:
            design_uri = urllib.parse.urlparse(design_ref)

            handler = cls.scheme_handlers.get(design_uri.scheme, None)

            if handler is None:
                raise errors.InvalidDesignReference(
                    "Invalid reference scheme %s: no handler." %
                    design_uri.scheme)
            else:
                tries = 0
                while tries < config_mgr.conf.network.http_client_retries:
                    try:
                        # Have to do a little magic to call the classmethod as a pointer
                        return handler.__get__(None, cls)(design_uri)
                    except Exception as ex:
                        tries = tries + 1
                        if tries < config_mgr.conf.network.http_client_retries:
                            LOG.debug("Retrying reference after failure: %s" %
                                      str(ex))
                            time.sleep(5**tries)
        except ValueError:
            raise errors.InvalidDesignReference(
                "Cannot resolve design reference %s: unable to parse as valid URI."
                % design_ref) 
********************************************
********************************************
def scrawl_kernel(arch):
    re_href = re.compile('href="?({arch}[^ <>"]*)"?'.format(arch=arch))
    url = "https://toolchains.bootlin.com/downloads/releases/toolchains/{arch}/test-system/".format(arch=arch)
    response = requests.get(url + "?C=M;O=D")
    text = response.text
    links = re_href.findall(text)
    links_dict = defaultdict(lambda: defaultdict(dict))
    for link in links:
        version = get_link_version(link)
        libc = get_link_libc(link)
        filetype = get_link_filetype(link)

        # TODO: make sure they have been compiled at the same time
        if filetype not in links_dict[version][libc]:
            if filetype is None:
                return None, None, None
            links_dict[version][libc][filetype] = url + link

    state = "bleeding-edge"
    if "stable" in links_dict:
        state = "stable"

    for libc in ["glibc", "uclibc", "musl"]:
        if libc in links_dict[state]:
            break
    else:
        libc = None

    target = links_dict[state][libc]

    dtb = target.get("dtb", None)
    rootfs = target.get("rootfs", None)
    kernel = target.get("kernel", None)

    return kernel, dtb, rootfs 
********************************************
********************************************
def indexof_parse(url):
    re_href = re.compile('\[DIR\].*href="?([^ <>"]*)"?')
    response = requests.get(url)
    text = response.text
    links = re_href.findall(text)
    return links 
********************************************
********************************************
def get(self, key, alt=None):
        if key == 'desc':
            return self.desc or alt
        else:
            return self.attributes.get(key, alt) 
********************************************
********************************************
def str_attributes(self):
        res = ''
        for k in ['Level', 'School', 'Classes']:
            if self.get(k) is not None:
                res += k + ': ' + self.get(k, 'EMPTY') + '\n'
        res += '\n'
        for k in ['Casting Time', 'Duration', 'Concentration', 'Ritual',
                  'Components', 'Material']:
            if self.get(k) is not None:
                res += k + ': ' + self.get(k, 'EMPTY') + '\n'
        res += '\n'
        for k in ['Range', 'Damage', 'Damage Type', 'Save', 'Target']:
            if self.get(k) is not None:
                res += k + ': ' + self.get(k, 'EMPTY') + '\n'
        return res 
********************************************
********************************************
def heartbeat(self):
        '''
        Description: Hearbeat value
        Input:       No input
        Output:      Two possible values:
                 (0, error_info): Error getting heartbeat value
                 (1, heartbeat_value): Heartbeat value
        '''
        url = 'https://%s/php/heartbeat.php' % self.atdserver

        try:
            r = requests.get(url, headers=self.sessionhdr, verify=False)
        except Exception as e:
            error_info = 'Error getting heartbeat:\n%s' % e
            return(0, error_info)

        if r.status_code == 200:
            server_info = json.loads(r.content)

            if server_info['success'] is True:
                return (1, server_info['results']['heartBeat'])

            else:
                error_info = 'Error getting heartbeat, check credentials or content type header'
                return (0, error_info)
        else:
            error_info = 'Error getting heartbeat, status code: %d' % r.status_code
            return (0, error_info) 
********************************************
********************************************
def get_report(self, job_id):
        '''
        Description: Get the final result of the inspection of the sample submitted
        Input:       jobId, identification of the job
        Output:      Possible values:

                 (0, error_info): Unsucessful procedure
                 (2, 'Result is not ready')
                 (3, 'Report not found, Ex. file not supported')
                 (1, {}): The dic includes all the json report
        '''

        url = 'https://%s/php/showreport.php' % self.atdserver

        payload = {'jobId': job_id, 'iType': 'json'}

        custom_header = {
            'Accept': 'application/vnd.ve.v1.0+json',
            'VE-SDK-API': '%s' % self.b64(self.session, self.userId)
        }

        try:
            r = requests.get(url, params=payload, headers=custom_header, verify=False)
        except Exception as e:
            error_info = 'Can not get report of jobId: %d,\nReturned error: %s ' % (job_id, e)
            return (0, error_info)

        if r.status_code == 400:
            info = 'Inspection not yet finished'
            return(2, info)

        if r.content.split('\n')[0] == 'Result is not ready':
            info = 'Result is not ready'
            return (2, info)
        else:
            if 'report file not found' in r.content.lower():
                server_info = 'Report not found - Ex. file not supported'
                return (3, server_info)
            else:
                server_info = json.loads(r.content)
                return (1, server_info) 
********************************************
********************************************
def b64(self, user, password):
        '''
        Description: Internal procedure to get the base64 values used for authentication
        Input:       user and password
        Output:      base64('user:pass'): The dic includes all the json report
        '''
        import base64
        auth_string = user + ':' + password
        return base64.b64encode(auth_string) 
********************************************
********************************************
def fetch_json(self, url):
        response = requests.get(url)
        return response.json()

# This method will be used by the mock to replace requests.get 
********************************************
********************************************
def test_fetch(self, mock_get):
        # Assert requests.get calls
        my_class = MyClass()
        # call to url-1
        json_data = my_class.fetch_json('http://url-1.com/test.json')
        self.assertEqual(json_data, {"key1": "value1"})
        # call to url-2
        json_data = my_class.fetch_json('http://url-2.com/test.json')
        self.assertEqual(json_data, {"key2": "value2"})
        # call to url-3 that we did not mock
        json_data = my_class.fetch_json('http://url-3.com/test.json')
        self.assertIsNone(json_data) 
********************************************
********************************************
def wait(self):
        while self.status != "error" and self.status != "completed":
            time.sleep(1)
            reply = requests.get("%s/job/%d" % (self.url, self.data["id"]))
            if reply.status_code != 200:
                try:
                    message = reply.json()["message"]
                except ValueError:
                    message = reply.content
                raise RuntimeError("Server returned error code %d: %s" % (reply.status_code, message))
            self.data = reply.json()["job"]
        return self.status 
********************************************
********************************************
def list_jobs(self):
        reply = requests.get("%s/job" % self.url)
        if reply.status_code == 200:
            return [Job(self.url, job) for job in reply.json()["jobs"]]
        else:
            try:
                message = reply.json()["message"]
            except ValueError:
                message = reply.content
            raise RuntimeError("Server returned error code %d: %s" % (reply.status_code, message))

################# Command line interface ########################### 
********************************************
********************************************
def parse_nodes_json_v1(data, *kwargs):
    out = []
    for k, n in data['nodes'].items():
        model = n.get("nodeinfo", {}).get("hardware", {}).get("model", None)
        if model is None:
            continue
        out.append({"model": model})

    return out 
********************************************
********************************************
def parse_nodes_json_v2(data, *kwargs):
    out = []
    for n in data['nodes']:
        if type(n) is str:
            continue
        model = n.get("nodeinfo", {}).get("hardware", {}).get("model", None)
        if model is None:
            continue
        out.append({"model": model})

    return out 
********************************************
********************************************
def verify_credentials(self):
        """
        Requests a token, salt and key from Nord api
        Sends a final hash of (salt+password)+key and token to Nord api
        Verifies responses and updates GUI
        """
        if self.user_input.text() and self.password_input.text():
            self.statusbar.showMessage('Login Success', 2000)
            self.username = self.user_input.text()
            self.password = self.password_input.text()
            self.repaint()
            time.sleep(0.5)
            self.hide()
            self.main_ui()
        else:
            self.statusbar.showMessage('Username or password field cannot be empty, 2000')
        # try:
        #     resp = requests.get('https://apself.statusbar.showMessage('Login Success', 2000)
    #                         self.username = self.user_input.text()
    #                         self.password = self.password_input.text()
    #                         self.repaint()
    #                         time.sleep(0.5)
    #                         self.hide()
    #                         self.main_ui()i.nordvpn.com/token/token/' + self.user_input.text(), timeout=5)
        #
        #     if resp.status_code == requests.codes.ok:
        #         token_json = json.loads(resp.text)
        #         token = token_json['token']
        #         salt = token_json['salt']
        #         key = token_json['key']
        #
        #         password_hash = hashlib.sha512(salt.encode() + self.password_input.text().encode())
        #         final_hash = hashlib.sha512(password_hash.hexdigest().encode() + key.encode())
        #
        #         try:
        #             resp = requests.get('https://api.nordvpn.com/token/verify/' + token + '/' + final_hash.hexdigest(), timeout=5)
        #             if resp.status_code == requests.codes.ok:
        #                 self.statusbar.showMessage('Login Success', 2000)
        #                 self.username = self.user_input.text()
        #                 self.password = self.password_input.text()
        #                 self.repaint()
        #                 time.sleep(0.5)
        #                 self.hide()
        #                 self.main_ui()
        #             else:
        #                 self.statusbar.showMessage('Invalid Credentials', 2000)
        #                 self.user_input.clear()
        #                 self.password_input.clear()
        #                 self.user_input.setFocus()
        #         except Exception as ex:
        #             self.statusbar.showMessage('Invalid Credentials', 2000)
        #             self.user_input.clear()
        #             self.password_input.clear()
        #             self.user_input.setFocus()
        #     else:
        #         self.statusbar.showMessage("API Error: could not fetch token", 2000)
        # except Exception as ex:
        #     self.statusbar.showMessage("API Error: could not fetch token", 2000)
        #     self.get_api_data() 
********************************************
********************************************
def _get(self, url, extra_params=None, verbose=False, first_request_time=None, retry_counter=0, ignore_fail=False):
        if verbose and not first_request_time:
            print("Import on url %s " % url)

        if not first_request_time:
            first_request_time = datetime.now()

        elapsed = datetime.now() - first_request_time
        if elapsed > timedelta(seconds=self.retry_timeout):
            raise navitia_client.exceptions.Timeout()

        if retry_counter > 0:
            # 0.5 * (1.5 ^ i) is an increased sleep time of 1.5x per iteration,
            # starting at 0.5s when retry_counter=0. The first retry will occur
            # at 1, so subtract that first.
            delay_seconds = 0.5 * 1.5 ** (retry_counter - 1)
            time.sleep(delay_seconds)

        full_url = os.path.join(self.core_url, url)

        try:
            response = requests.get(
                url=full_url, auth=(self.user, self.password), params=(extra_params or {}))
            self.requested_urls.append(response.url)

        except requests.exceptions.Timeout:
            if not ignore_fail:
                raise navitia_client.exceptions.Timeout()
            else:
                return False
        except Exception as e:
            if not ignore_fail:
                raise navitia_client.exceptions.TransportError(e)
            else:
                return False

        # Warn if not 200
        if response.status_code != 200:
            print("WARNING: response status_code is %s" % response.status_code)

        if response.status_code in _RETRIABLE_STATUSES:
            # Retry request.
            print("WARNING: retry number %d" % retry_counter)
            return self._get(url=url, extra_params=extra_params, first_request_time=first_request_time, retry_counter=retry_counter + 1, verbose=verbose, ignore_fail=ignore_fail)

        return response 
********************************************
********************************************
def init_inventory_well(well, headers=None, org_name=None,container_json=None):
    """Initialize well (set volume etc) for Transcriptic"""
    
    initialize_config()
        
    headers = headers if headers else TSC_HEADERS
    org_name = org_name if org_name else ORG_NAME    
    
    def _container_url(container_id):
        return 'https://secure.transcriptic.com/{}/samples/{}.json'.format(org_name, container_id)

    #only initialize containers that have already been made
    if not well.container.id:
        well.volume = ul(0)
        return

    if container_json:
        container = container_json
    else:
        response = requests.get(_container_url(well.container.id), headers=headers)
        response.raise_for_status()
    
        container = response.json()

    well_data = list(filter(lambda w: w['well_idx'] == well.index,container['aliquots']))
    
    #correct the cover status on the container
    
    #they don't return info on empty wells
    if not well_data:
        well.volume = ul(0)
        return
    
    well_data = well_data[0]
    well.name = "{}".format(well_data['name']) if well_data['name'] is not None else container["label"]
    well.properties = well_data['properties']
    if well_data.get('resource'):
        well.properties['Resource'] = well_data['resource']['name']
    well.volume = Unit(well_data['volume_ul'], 'microliter')

    if 'ERROR' in well.properties:
        raise ValueError("Well {} has ERROR property: {}".format(well, well.properties["ERROR"]))
    #if well.volume < Unit(20, "microliter"):
    #    logging.warn("Low volume for well {} : {}".format(well.name, well.volume))

    return True 
********************************************
********************************************
def getcookies(user, passwd):
    # 获取验证码
    sign = random.random()
    url = "https://captcha.weibo.com/api/pattern/get?ver=daf139fb2696a4540b298756bd06266a&source=ssologin&usrname=" + user + "&line=160&side=100&radius=30&_rnd=" + str(
        sign) + "&callback=pl_cb"
    r = requests.get(url)
    imgdata = json.loads(r.text.replace("pl_cb(", '').replace(")", ''))['path_enc']
    id = json.loads(r.text.replace("pl_cb(", '').replace(")", ''))['id']
    recombinePattern(imgdata)
    data_enc = pathdataEncode(path_generate(patterntohash()))
    path_enc = pathEncode(patterntohash(), id)

    url2 = "https://captcha.weibo.com/api/pattern/verify?ver=daf139fb2696a4540b298756bd06266a&id=" + id + "&usrname=" + user + "&source=ssologin&path_enc=" + path_enc + "&data_enc=" + data_enc + "&callback=pl_cb"
    url3 = 'https://passport.weibo.cn/sso/login'
    # 必要的等待时间
    time.sleep(1)
    # 验证验证码
    session = requests.Session()
    r2 = session.get(url2)
    # print r2.headers
    print json.loads(r2.text.replace("pl_cb(", '').replace(")", ''))['msg']
    # print id

    formdata = {'username': user,
                'password': passwd,
                'savestate': '1',
                'ec': '0',
                'entry': 'mweibo',
                'mainpageflag': '1',
                'vid': id,
                'wentry': '',
                'loginfrom': '',
                'client_id': '',
                'code:qq': '',
                'r': '',
                'pagerefer': '',
                'hff': '',
                'hfp': ''}

    # print formdata['vid']
    # 登录
    r3 = session.post(url3, data=formdata, headers=headers3)
    cookies_url = r3.headers['Set-Cookie']
    print json.loads(r3.content)['msg']
    return {k.split('=')[0]: k.split('=')[1] for k in cookies_url.split(';')}

    # r4 = requests.get('https://m.weibo.cn/')
    # print r4.headers['Set-Cookie'] 
********************************************
********************************************
def sync_dns_from_my_ip(self, dns_type='A'):
        """
        Sync dns from my public ip address.
        It will not do update if ip address in dns record is already same as
        current public ip address.
        :param dns_type:
        :return:
        """
        ip_address = ''
        for finder in self.public_ip_finder:
            try:
                result = requests.get(finder)
            except requests.RequestException:
                continue
            if result.status_code == 200:
                try:
                    socket.inet_aton(result.text)
                    ip_address = result.text
                    break
                except socket.error:
                    try:
                        socket.inet_aton(result.json().get('ip'))
                        ip_address = result.json()['ip']
                        break
                    except socket.error:
                        continue

        if ip_address == '':
            print('None of public ip finder is working. Please try later')
            sys.exit(1)

        try:
            record = self.get_record(dns_type, self.domain) \
                if len(self.domain.split('.')) == 3 \
                else self.get_record(dns_type, self.domain)
        except RecordNotFound:
            self.create_record(dns_type, self.domain, ip_address, proxied=self.proxied)
            print('Successfully created new record with IP address {new_ip}'
                  .format(new_ip=ip_address))
        else:
            if record['content'] != ip_address:
                self.update_record(dns_type, self.domain, ip_address, proxied=record['proxied'])
                print('Successfully updated IP address from {old_ip} to {new_ip}'
                      .format(old_ip=record['content'], new_ip=ip_address))
            else:
                print('IP address on CloudFlare is same as your current address') 
********************************************
********************************************
def FlightInfo(ident, username, apiKey, verbose=0, results=10):
	try:
		fxmlUrl = "https://flightxml.flightaware.com/json/FlightXML3/"
		ident = ident.strip()
		payload = {'ident':ident, 'howMany':results}
		response = requests.get(fxmlUrl + "FlightInfoStatus", params=payload, auth=(username, apiKey))
		output = dict()
		if response.status_code == 402:
			print(response.text)
			return False
		if response.status_code == 200:
			decodedResponse = response.json()
			print(decodedResponse)
			if 'FlightInfoStatusResult' not in decodedResponse:
				return False
			for flight in decodedResponse['FlightInfoStatusResult']['flights']:
				if 'status' not in flight:
					continue
				if flight['status'].startswith('On') or flight['status'].startswith('En') or flight['status'].startswith('In'):
					output = {
						"orig_name":flight['origin']['airport_name'],
						"orig_city":flight['origin']['city'],
						"orig_alt":flight['origin']['alternate_ident'],
						"orig_code":flight['origin']['code'],
						"dest_name":flight['destination']['airport_name'],
						"dest_city":flight['destination']['city'],
						"dest_alt":flight['destination']['alternate_ident'],
						"dest_code":flight['destination']['code']
					}
					break
			if verbose:
				return decodedResponse
			else:
				return output
		else:
			print("FA API status code: {}".format(response.status_code))
			print(response.text)
			return False
	except Exception:
		print("exception in fa_api.FlightInfo():")
		traceback.print_exc()
		return False 
********************************************
********************************************
def PS1catalog(ra,dec,magmin,magmax):

    queryurl = 'https://archive.stsci.edu/panstarrs/search.php?'
    queryurl += 'RA='+str(ra)
    queryurl += '&DEC='+str(dec)
    queryurl += '&SR=0.083&selectedColumnsCsv=ndetections,raMean,decMean,'
    queryurl += 'gMeanPSFMag,rMeanPSFMag,iMeanPSFMag,zMeanPSFMag,yMeanPSFMag,iMeanKronMag'
    queryurl += '&ordercolumn1=ndetections&descending1=on&max_records=200'

    print('\nQuerying PS1 for reference stars via MAST...\n')

    query = requests.get(queryurl)

    results = query.text

    entries = results.split('DATA')[2][11:][:-19].split('</TD>\n</TR>\n<TR>\n<TD>')

    data = []

    for i in entries:
        data.append(np.array(i.split('</TD><TD>')).T)

    if len(data) > 1:

        data = np.array(data).astype(float)

        # Get rid of n_det column
        data = data[:,1:][data[:,0]>3]

        # Get rid of non-detections:
        data = data[data[:,2]>-999]
        data = data[data[:,3]>-999]
        data = data[data[:,4]>-999]
        data = data[data[:,5]>-999]
        data = data[data[:,6]>-999]

        # Get rid of very faint stars
        data = data[data[:,2]<magmin]
        data = data[data[:,3]<magmin]
        data = data[data[:,4]<magmin]
        data = data[data[:,5]<magmin]
        data = data[data[:,6]<magmin]

        # Get rid of stars likely to saturate
        data = data[data[:,2]>magmax]
        data = data[data[:,3]>magmax]
        data = data[data[:,4]>magmax]
        data = data[data[:,5]>magmax]
        data = data[data[:,6]>magmax]


        # Star-galaxy separation
        data = data[:,:-1][data[:,4]-data[:,-1]<0.05]

        np.savetxt('PS1_seq.txt',data,fmt='%.8f\t%.8f\t%.2f\t%.2f\t%.2f\t%.2f\t%.2f',header='Ra\tDec\tg\tr\ti\tz\ty\n',comments='')

        print('Success! Sequence star file created: PS1_seq.txt')

    else:
        sys.exit('Field not in PS1! Exiting') 
********************************************
********************************************
def PS1cutouts(ra,dec,filt):

    print('\nSearching for PS1 images of field...\n')

    ps1_url = 'http://ps1images.stsci.edu/cgi-bin/ps1filenames.py?'

    ps1_url += '&ra='+str(ra)
    ps1_url += '&dec='+str(dec)
    ps1_url += '&filters='+filt

    ps1_im = requests.get(ps1_url)

    try:
        image_name = ps1_im.text.split()[16]

        print('Image found: ' + image_name + '\n')

        cutout_url = 'http://ps1images.stsci.edu/cgi-bin/fitscut.cgi?&filetypes=stack&size=2500'

        cutout_url += '&ra='+str(ra)
        cutout_url += '&dec='+str(dec)
        cutout_url += '&filters='+filt
        cutout_url += '&format=fits'
        cutout_url += '&red='+image_name

        dest_file = filt + '_template.fits'

        cmd = 'wget -O %s "%s"' % (dest_file, cutout_url)

        os.system(cmd)

        print('Template downloaded as ' + dest_file + '\n')

    except:
        print('\nPS1 template search failed!\n')


##################################




# Try to match header keyword to a known filter automatically: 
********************************************
********************************************
def PS1catalog(ra,dec,magmin,magmax):

    queryurl = 'https://archive.stsci.edu/panstarrs/search.php?'
    queryurl += 'RA='+str(ra)
    queryurl += '&DEC='+str(dec)
    queryurl += '&SR=0.083&selectedColumnsCsv=ndetections,raMean,decMean,'
    queryurl += 'gMeanPSFMag,rMeanPSFMag,iMeanPSFMag,zMeanPSFMag,yMeanPSFMag,iMeanKronMag'
    queryurl += '&ordercolumn1=ndetections&descending1=on&max_records=200'

    print '\nQuerying PS1 for reference stars via MAST...\n'

    query = requests.get(queryurl)

    results = query.text

    entries = results.split('DATA')[2][11:][:-19].split('</TD>\n</TR>\n<TR>\n<TD>')

    data = []

    for i in entries:
        data.append(np.array(i.split('</TD><TD>')).T)

    if len(data) > 1:

        data = np.array(data).astype(float)

        # Get rid of n_det column
        data = data[:,1:][data[:,0]>3]

        # Get rid of non-detections:
        data = data[data[:,2]>-999]
        data = data[data[:,3]>-999]
        data = data[data[:,4]>-999]
        data = data[data[:,5]>-999]
        data = data[data[:,6]>-999]

        # Get rid of very faint stars
        data = data[data[:,2]<magmin]
        data = data[data[:,3]<magmin]
        data = data[data[:,4]<magmin]
        data = data[data[:,5]<magmin]
        data = data[data[:,6]<magmin]

        # Get rid of stars likely to saturate
        data = data[data[:,2]>magmax]
        data = data[data[:,3]>magmax]
        data = data[data[:,4]>magmax]
        data = data[data[:,5]>magmax]
        data = data[data[:,6]>magmax]


        # Star-galaxy separation
        data = data[:,:-1][data[:,4]-data[:,-1]<0.05]

        np.savetxt('PS1_seq.txt',data,fmt='%.8f\t%.8f\t%.2f\t%.2f\t%.2f\t%.2f\t%.2f',header='Ra\tDec\tg\tr\ti\tz\ty\n',comments='')

        print 'Success! Sequence star file created: PS1_seq.txt'

    else:
        sys.exit('Field not in PS1! Exiting') 
********************************************
********************************************
def connect(self, user, password):
        '''
        Description: Connection method, stablish a connection to the ATD server and populates all
                 self variables of the constructor
        Input:       User and password
        Output:      Two possible values:
                 (0, error_info): Unsucessful connection, error_info contain the cause of the error
                 (1, 'Connection sucessful): Sucessful connection
        '''

        #

        authheader = {
            'Accept': 'application/vnd.ve.v1.0+json',
            'Content-Type': 'application/json',
            'VE-API-Version': self.apiver,
            'VE-SDK-API': '%s' % self.b64(user, password)
        }

        url = 'https://%s/php/session.php' % self.atdserver

        try:
            r = requests.get(url, headers=authheader, verify=False)
        except Exception as e:
            error_info = 'Error connecting to ATD:\n %s' % e
            return (0, error_info)

        if r.status_code == 200:
            server_info = json.loads(r.content)
            if server_info['success'] is True:
                self.session = server_info['results']['session']
                self.userId = server_info['results']['userId']
                self.matdver = server_info['results']['matdVersion']
                self.sessionhdr = {
                    'Accept': 'application/vnd.ve.v1.0+json',
                    'Content-Type': 'application/json',
                    'VE-API-Version': self.apiver,
                    'VE-SDK-API': '%s' % self.b64(self.session, self.userId)
                }
            else:
                error_info = 'Connection unsucessful'
                return (0, error_info)
        else:
            if r.status_code == 401:
                error_info = 'Error conecting to ATD, Username / Password combination not accepted.'
                return(0, error_info)
            else:
                error_info = 'Error conecting to ATD, Status Code: %d' % r.status_code
                return(0, error_info)

        return(1, 'Connection sucessful') 
********************************************
********************************************
def getTicket():
    # put the ip address or dns of your apic-em controller in this url
    url = "https://" + controller + "/api/v1/ticket"

    #the username and password to access the APIC-EM Controller
    payload = {"username":"usernae","password":"password"}

    #Content type must be included in the header
    header = {"content-type": "application/json"}

    #Performs a POST on the specified url to get the service ticket
    response= requests.post(url,data=json.dumps(payload), headers=header, verify=False)

    #convert response to json format
    r_json=response.json()

    #parse the json to get the service ticket
    ticket = r_json["response"]["serviceTicket"]

    return ticket 
********************************************
********************************************
def bindiff_export(self, sample, is_64_bit = True, timeout = None):
        """
        Load a sample into IDA Pro, perform autoanalysis and export a BinDiff database.
        :param sample: The sample's path
        :param is_64_bit: If the sample needs to be analyzed by the 64 bit version of IDA
        :param timeout: Timeout for the analysis in seconds
        :return: The file name of the exported bindiff database. The file needs
        to be deleted by the caller. Returns None on error.
        """

        data_to_send = {
            "timeout": timeout,
            "is_64_bit": is_64_bit}
        url = "%s/binexport" % next(self._urls)
        log.debug("curl -XPOST --data '%s' '%s'", json.dumps(data_to_send), url)
        response = requests.post(url, data = data_to_send, files = {os.path.basename(sample): open(sample, "rb")})
        if response.status_code == 200:
            handle, output = tempfile.mkstemp(suffix = ".BinExport")
            with os.fdopen(handle, "wb") as f:
                map(f.write, response.iter_content(1024))
            return output
        else:
            log.error("Bindiff server responded with status code %d: %s", response.status_code, response.content)
            return None 
********************************************
********************************************
def answer_ponies(query_id, results, timed_out):
    result = requests.post(
        "https://api.telegram.org/bot{}/answerInlineQuery".format(settings.TELEGRAM_TOKEN),
        headers={"Content-Type": "application/json"},
        json={
            'inline_query_id': query_id,
            'cache_time': settings.CACHE_TIME if timed_out == 0 else settings.PARTIAL_RESULT_CACHE_TIME,
            'is_personal': False,
            'results': [
                {
                    'type': 'mpeg4_gif',
                    'id': url,
                    'mpeg4_url': url,
                    'thumb_url': thumb,
                    'mpeg4_width': dimensions[0],
                    'mpeg4_height': dimensions[1],
                } for url, thumb, id_number, dimensions in results
            ]
        })
    print(result.content)
    result.raise_for_status() 
********************************************
********************************************
def sendNotification(url, msg):
    """Send a notification using a Slack webhook URL. See https://api.slack.com/incoming-webhooks

    Arguments:
        url (string): Slack incoming webhook URL for sending the message.
        msg (string): The message to be sent (can use markdown for formatting)
    """
    try:
        res = requests.post(url, data=json.dumps(
            {"text": msg, "mrkdwn": True}))
        if res.status_code != 200:
            print(f'Falied to send notification "{msg}" to "{url}"')
            print('Response', res.content)
            return False
    except Exception as e:
        print(f'Falied to send notification "{msg}" to "{url}"')
        print(e)
        return False
    return True 
********************************************
********************************************
def processfile(inputfile, serverurl):
    headers = {'content-type': 'application/json'}
    try:
        with open(inputfile) as json_file:

            file_data = json_file.read()
            file_data = file_data.replace("\n", "")

            print(json.dumps(file_data, sort_keys=True, indent=4, separators=(',', ': ')))
            try:
                r = requests.post(serverurl, data=file_data, headers=headers, timeout=5)
            except Exception as err:

                print("COMMUNICATION ERROR : " + format(err))
                sys.exit(2)
    except Exception as err:
        print("File ERROR : " + format(err))

        return False

    print (inputfile + " sent to " + serverurl + ". Status code: " + str(r.status_code) + ".")

    return True 
********************************************
********************************************
def fj_login(name=USERNAME, password=PASSWORD, email_mode=False, _print=False):
    ''' ([str, str, bool, bool]) -> dict
    name: your username. If email_mode == True, your email address
    password: your password
    email_mode: whether name is your email or username
    _print: debug

    returns authentication information'''
    
    if email_mode:
        url = 'https://account.freejamgames.com/api/authenticate/email/web'
        body_json = {'EmailAddress':name, 'Password':password}
        response = requests.post(url, json=body_json)
    else:
        url = 'https://account.freejamgames.com/api/authenticate/displayname/web'
        body_json = {'DisplayName':name, 'Password':password}
        response = requests.post(url, json=body_json)
    if response.status_code != 200:
        if _print:
            print('FJ Auth returned error', response.status_code)
    return response.json() 
********************************************
********************************************
def pickle_export(self, sample, is_64_bit = True, timeout = None):
        """
        Load a sample into IDA Pro, perform autoanalysis and export a pickle file. 
        :param sample: The sample's path
        :param is_64_bit: If the sample needs to be analyzed by the 64 bit version of IDA
        :param timeout: Timeout for the analysis in seconds
        :return: The file name of the exported pickle database. The file needs
        to be deleted by the caller. Returns None on error.
        """

        data_to_send = {
            "timeout": timeout,
            "is_64_bit": is_64_bit}
        url = "%s/pickle" % next(self._urls)
        log.debug("curl -XPOST --data '%s' '%s'", json.dumps(data_to_send), url)
        response = requests.post(url, data = data_to_send, files = {os.path.basename(sample): open(sample, "rb")})
        if response.status_code == 200:
            handle, output = tempfile.mkstemp(suffix = ".pickle")
            with os.fdopen(handle, "wb") as f:
                map(f.write, response.iter_content(1024))
            return output
        else:
            log.error("Bindiff server responded with status code %d: %s", response.status_code, response.content)
            return None 
********************************************
********************************************
def compare(self, primary, secondary, timeout = None):
        """
        Run BinDiff on the two BinDiff databases.
        :param primary: The first BinExport database
        :param secondary: The second BinExport database
        :param timeout: Timeout for the command in seconds
        :returns: The directory name of the directory with the generated data on the shared volume
        """

        url = "%s/compare" % next(self._urls)
        log.debug("curl -XPOST --form 'timeout=%s' --form '[email protected]%s' --form '[email protected]%s' '%s'", str(timeout), primary, secondary, url)
        response = requests.post(url, data = {"timeout": timeout}, \
                files = {"primary": open(primary, "rb"), "secondary": open(secondary, "rb")})

        if response.status_code == 200:
            handle, path = tempfile.mkstemp(suffix = ".bindiff.sqlite3")
            with os.fdopen(handle, "wb") as f:
                map(f.write, response.iter_content(1024))
            return path
        else:
            log.error("Bindiff server responded with status code %d: %s", response.status_code, response.content)
            return None 
********************************************
********************************************
def apply_configuration():
    applyConfigurationResponse = requests.post(applyConfigurationURI, 
        headers={
            'Authorization': iotHubSasToken,
            'Content-Type': 'application/json'
        },
        data = get_config_file_contents()
    )

    print(applyConfigurationURI)
    print(applyConfigurationResponse.status_code)
    print(applyConfigurationResponse.text)

    if applyConfigurationResponse.status_code == 204:
        print("Configuration successfully applied.  Please run `docker logs edgeAgent -f` to see the change applied.")
    else:
        print("There was an error applying the configuration. You should see an error message above that indicates the issue.") 
********************************************
********************************************
def convert_to_mp4(url):
    cached = check_pony_cache(url)
    if cached is not None:
        return cached
    print("Converting {} to mp4...".format(url))
    result = requests.post("https://api.imgur.com/3/image", {
        "image": url,
        "type": "URL"
    }, headers={
        'Authorization': 'Client-ID {}'.format(settings.IMGUR_TOKEN),
        'Content-Type': 'application/x-www-form-urlencoded',
    })
    try:
        result.raise_for_status()
    except requests.HTTPError as e:
        print(e.response.content)
        return None
    mp4_url = result.json()['data'].get('mp4', None)
    if mp4_url is not None:
        cache_pony(url, mp4_url)
    return mp4_url 
********************************************
********************************************
def sendToCityIO(data, endpoint=-1, token=None):
    config = get_config()
    if endpoint == -1 or endpoint == None:
        post_address = config['CITY_SCOPE']['TABLE_URL_RESULT_POST'] # default endpoint
    else:
        post_address = json.loads(config['CITY_SCOPE']['TABLE_URL_RESULT_POST_LIST'])[endpoint] # user endpoint

    if token is None:
        r = requests.post(post_address, json=data, headers={'Content-Type': 'application/json'})
    else: # with authentication
        r = requests.post(post_address, json=data, headers={'Content-Type': 'application/json', 'Authorization': 'Bearer '+token})
    print(r)
    if not r.status_code == 200:
        print("could not post result to cityIO", post_address)
        print("Error code", r.status_code)
    else:
        print("Successfully posted to cityIO", post_address, r.status_code)

# checks for updates on the cityIO grid
# If the grid changes the city-scope parser is called to create a new buildings.json
# The noise calculation is triggered 
********************************************
********************************************
def __card_login(self):
        card_url = 'https://pass.neu.edu.cn/tpass/login?service=http://ecard.neu.edu.cn/selflogin/login.aspx'
        response = requests.get(card_url, cookies=self.pass_cookies, proxies=proxies['index'])

        data = {
            'username': re.findall("<input type=hidden name='username' id='username' value='(.*?)'/>", response.text)[
                0],
            'timestamp':
                re.findall("<input type=hidden name='timestamp' id='timestamp' value='(.*?)'/>", response.text)[0],
            'auid': re.findall("<input type=hidden name='auid' id='auid' value='(.*?)'/>", response.text)[0]
        }

        get_session = requests.post('http://ecard.neu.edu.cn/selfsearch/SSOLogin.aspx',
                                    cookies=response.cookies,
                                    data=data,
                                    headers=self.__headers,
                                    proxies=proxies['card'])

        self.card_cookies = {
            '.ASPXAUTSSM': get_session.history[0].cookies['.ASPXAUTSSM'],
            'ASP.NET_SessionId': response.cookies['ASP.NET_SessionId']
        }

    # 通过一网通办登陆教务处，私有方法 
********************************************
********************************************
def borrow_info(self):
        post_url = 'https://portal.neu.edu.cn/tp_up/up/subgroup/getLibraryInfo'
        my_headers = {
            'Referer': 'https://portal.neu.edu.cn/tp_up/view?m=up',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36',
            'Origin': 'https://portal.neu.edu.cn',
            'Content-Type': 'application/json;charset=UTF-8'
        }
        response = requests.post(post_url,
                                headers=my_headers,
                                cookies=self.index_cookies,
                                data='{}',
                                proxies=proxies['index'])
        return response.json()


    # 卡是否挂失 
********************************************
********************************************
def _get_song_lyrics(self, song):
        lyric_url = "https://music.163.com/weapi/song/lyric"
        raw_data = {
            "csrf_token": "",
            "id": song.id,
            "lv": -1,
            "tv": -1
        }
        post_data = get_encrypted_post_data(raw_data)
        r = requests.post(lyric_url, data=post_data, headers=self.headers)
        json_data = r.json()

        lyric = ""
        if "lrc" in json_data and "lyric" in json_data["lrc"]:
            lyric = json_data["lrc"]["lyric"]

        return lyric 
********************************************
********************************************
def new_post(self, text):

        header = {
            "Content-Type": "application/json",
            "User-Agent" : self.UA,
            "X-Line-Mid" : self.mid,
            "x-lct" : self.channel_access_token,
        }

        payload = {
            "postInfo" : { "readPermission" : { "type" : "ALL" } },
            "sourceType" : "TIMELINE",
            "contents" : { "text" : text }
        }

        r = requests.post(
            "http://" + self.host + "/mh/api/v24/post/create.json",
            headers = header,
            data = json.dumps(payload)
        )

        return r.json() 
********************************************
********************************************
def postPhoto(self,text,path):
        header = {
            "Content-Type": "application/json",
            "User-Agent" : self.UA,
            "X-Line-Mid" : self.mid,
            "x-lct" : self.channel_access_token,
        }

        payload = {
            "postInfo" : { "readPermission" : { "type" : "ALL" } },
            "sourceType" : "TIMELINE",
            "contents" : { "text" : text ,"media" :  [{u'objectId': u'F57144CF9ECC4AD2E162E68554D1A8BD1a1ab0t04ff07f6'}]}
        }
        r = requests.post(
            "http://" + self.host + "/mh/api/v24/post/create.json",
            headers = header,
            data = json.dumps(payload)
        )

        return r.json() 
********************************************
********************************************
def like(self, mid, postid, likeType=1001):

        header = {
            "Content-Type" : "application/json",
            "X-Line-Mid" : self.mid,
            "x-lct" : self.channel_access_token,
        }

        payload = {
            "likeType" : likeType,
            "activityExternalId" : postid,
            "actorId" : mid
        }

        r = requests.post(
            "http://" + self.host + "/mh/api/v23/like/create.json?homeId=" + mid,
            headers = header,
            data = json.dumps(payload)
        )

        return r.json() 
********************************************
********************************************
def comment(self, mid, postid, text):
        header = {
            "Content-Type" : "application/json",
            "X-Line-Mid" : self.mid,
            "x-lct" : self.channel_access_token,
        }

        payload = {
            "commentText" : text,
            "activityExternalId" : postid,
            "actorId" : mid
        }

        r = requests.post(
            "http://" + self.host + "/mh/api/v23/comment/create.json?homeId=" + mid,
            headers = header,
            data = json.dumps(payload)
        )

        return r.json() 
********************************************
********************************************
def createAlbum(self,gid,name):
        header = {
                    "Content-Type": "application/json",
                    "User-Agent" : self.UA,
                    "X-Line-Mid" : self.mid,
                    "x-lct" : self.channel_access_token,
        }
        payload = {
                "type" : "image",
                "title" : name
        }
        r = requests.post(
            "http://" + self.host + "/mh/album/v3/album?count=1&auto=0&homeId=" + gid,
            headers = header,
            data = json.dumps(payload)
        )
        return r.json() 
********************************************
********************************************
def querylanguage(auth):
    """Query user's language that's available on v2c."""
    default = 'en'

    r = requests.post(
        url=api_url.replace('index.php', 'api.php'),
        data={
            'action': 'query',
            'format': 'json',
            'meta': 'userinfo',
            'uiprop': 'options'
        },
        auth=auth
    )

    try:
        language = r.json()['query']['userinfo']['options']['language']
    except (NameError, KeyError):
        return default

    if not language:
        return default

    return language 
********************************************
********************************************
def configAuthenticate(username, password):
	FACTORIOPATH = getFactorioPath()

	url = "https://auth.factorio.com/api-login"
	params = {'username': username, 'password': password, 'apiVersion': 2}


	if not os.path.isfile("%s/bin/x64/factorio" % (FACTORIOPATH) ):
		print("Could not find factorio at %s" % (FACTORIOPATH))
		sys.exit(1)


	print("Fetching token for %s" %  (username))
	myResponse = requests.post(url,data=params, verify=True)
	if(myResponse.ok):

	    jData = json.loads(myResponse.text)
	    print("Writing %s to settings.json" % (jData[0]))
	    
	else:
	  # If response code is not ok (200), print the resulting http error code with description
	    myResponse.raise_for_status()
	    sys.exit(1)
	

	try:
		with codecs.open(getSettingsFile(), 'r', encoding='utf-8') as settings_file:
			settingsJson = json.load(settings_file)
			settingsJson['token'] = jData[0]
			settingsJson['username'] = username
				


		with codecs.open("%s/config/settings.json" % (FACTORIOPATH), 'w', encoding='utf-8') as settings_file:
			json.dump(settingsJson, settings_file, indent=4)
	except Exception as e:
		print(e)
		print("Help! Can't deal with the settings file!") 
********************************************
********************************************
def get_melting_temp(dna_sequence, oligo_concentration_uM,
                     pcr_settings='q5'):
    global IDT_COOKIE
    
    #@TODO: update this to use http://tmcalculator.neb.com/#!/ since its probably accounting for the salt concentration in the buffer
    
    if not IDT_COOKIE:
        r = requests.get('http://www.idtdna.com/calc/analyzer',allow_redirects=False)
        IDT_COOKIE = r.cookies['ASP.NET_SessionId']       
        
    headers = {"content-type":'application/json','Cookie':'ASP.NET_SessionId=%s'%IDT_COOKIE}
    
    idt_response = requests.post('https://www.idtdna.com/calc/analyzer/home/analyze',json={
        "settings": {
            "Sequence": dna_sequence,
            "NaConc": 0,
            "MgConc": 2,
            "DNTPsConc": 5, 
            "OligoConc": oligo_concentration_uM,
            "NucleotideType": "DNA"
        }
    }, verify=False
    ,headers=headers)    
    
    idt_response.raise_for_status()
    
    return idt_response.json()['MeltTemp'] 
********************************************
********************************************
def post(self, *args, **kwargs):
        kwargs['method'] = 'post'
        return self.do(*args, **kwargs) 
********************************************
********************************************
def _get_token(self):
        response = requests.post(self.AUTH_URL, data={'grant_type': 'client_credentials'}, auth=(self.id, self.secret))
        response.raise_for_status()
        return response.json()['access_token'] 
********************************************
********************************************
def nli(self, text, cusid=None):
        response = requests.post(self.URL, params=self._gen_parameters('nli', text, cusid))
        response.raise_for_status()
        response_json = response.json()
        if response_json['status'] != 'ok':
            raise NliStatusError("NLI responded status != 'ok': {}".format(response_json['status']))
        else:
            nli_obj = response_json['data']['nli'][0]
            return self.intent_detection(nli_obj) 
********************************************
********************************************
def _isAsthamaPump(self, imageWidth, imageHeight, imageString):
        result = {}
        coordinates = {}
        metadata = {}
        isPresent = False

        try :
            self._printLogs("Sending Image To DL Server...", "NORMAL")

            url = DL_SERVER_URL
            payload = {
                        "imageWidth"   : imageWidth,
                        "imageHeight"  : imageHeight,
                        "image_string" : base64.b64encode(imageString),
                        "imageID"      : self.imageNo2d
                        }
            headers = {'content-type': 'application/json'}

            res = requests.post(url, data=json.dumps(payload), headers=headers)
            result = res.json()
            self._printLogs("[*] Sent to  : " + str(url), "OKBLUE")
            self._printLogs("[*] Response : " + str(result), "OKBLUE")

        except Exception, err:
            self._printLogs("Error Found on connecting to server : " + str(err), "FAIL")
            self._printLogs("+", "LINE") 
********************************************
********************************************
def get_token( client_id, secret ) :
    """Gets Authorizaton token to use in other requests."""
    auth_url  = 'https://auth-api.lexisnexis.com/oauth/v2/token'
    payload   = ( 'grant_type=client_credentials&scope=http%3a%2f%2f' 'oauth.lexisnexis.com%2fall' )
    headers   = { 'Content-Type': 'application/x-www-form-urlencoded' }
    r         = requests.post( auth_url, auth=HTTPBasicAuth( client_id, secret ), headers=headers, data=payload )
    json_data = r.json()
    return json_data[ 'access_token' ] 
********************************************
********************************************
def get_token( client_id, secret ) :
    """Gets Authorizaton token to use in other requests."""
    auth_url  = 'https://auth-api.lexisnexis.com/oauth/v2/token'
    payload   = ( 'grant_type=client_credentials&scope=http%3a%2f%2f' 'oauth.lexisnexis.com%2fall' )
    headers   = { 'Content-Type': 'application/x-www-form-urlencoded' }
    r         = requests.post( auth_url, auth=HTTPBasicAuth( client_id, secret ), headers=headers, data=payload )
    json_data = r.json()
    return json_data[ 'access_token' ] 
********************************************
********************************************
def execute(self, document, variable_values=None, timeout=None):
        query_str = print_ast(document)
        payload = {
            'query': query_str,
            'variables': variable_values or {}
        }

        data_key = 'json' if self.use_json else 'data'
        post_args = {
            'headers': self.headers,
            'auth': self.auth,
            'cookies': self.cookies,
            'timeout': timeout or self.default_timeout,
            data_key: payload
        }
        request = requests.post(self.url, **post_args)
        request.raise_for_status()

        result = request.json()
        assert 'errors' in result or 'data' in result, 'Received non-compatible response "{}"'.format(result)
        return ExecutionResult(
            errors=result.get('errors'),
            data=result.get('data')
        ) 
********************************************
********************************************
def main():
    module = AnsibleModule(
      argument_spec = dict(
        host = dict(required=True),
        username = dict(required=True),
        password = dict(required=True)
      )
    )
    
    device = module.params.get('host')
    username = module.params.get('username')
    password = module.params.get('password')

    url='http://' + host + '/ins'
    switchuser=username
    switchpassword=password

    myheaders={'content-type':'application/json-rpc'}
    
    payload=[
      {
        "jsonrpc": "2.0",
        "method": "cli",
        "params": {
          "cmd": "show version",
          "version": 1.2
        },
        "id": 1
      }
    ]
    response = requests.post(url,data=json.dumps(payload), headers=myheaders,auth=(switchuser,switchpassword)).json()

    version = response['result']['body']['sys_ver_str']
    data = json.dumps({"version": version})
    module.exit_json(changed=False, msg=str(data)) 
********************************************
********************************************
def add(self, db, **kwargs):
        db_data = db.data.copy()
        db_data["functions"] = _filter_functions(db.data["functions"], **kwargs)
        data = pickle.dumps(db_data)
        result = requests.post("{:s}/function".format(self.url), files = {"file": BytesIO(data)})
        if result.status_code != 200:
            raise RuntimeError("Request failed with status code {:d}".format(result.status_code)) 
********************************************
********************************************
def find_raw(self, db, **kwargs):
        db_data = db.data.copy()
        db_data["functions"] = _filter_functions(db.data["functions"], **kwargs)
        data = pickle.dumps(db_data)
        result = requests.post("{:s}/function/find/raw".format(self.url), files = {"file": BytesIO(data)})
        if result.status_code == 200:
            return True
        elif result.status_code == 404:
            return False
        else:
            raise RuntimeError("Request failed with status code {:d}".format(result.status_code)) 
********************************************
********************************************
def find_mnem(self, db, **kwargs):
        db_data = db.data.copy()
        db_data["functions"] = _filter_functions(db.data["functions"], **kwargs)
        data = pickle.dumps(db_data)
        result = requests.post("{:s}/function/find/mnem".format(self.url), files = {"file": BytesIO(data)})
        if result.status_code == 200:
            return True
        elif result.status_code == 404:
            return False
        else:
            raise RuntimeError("Request failed with status code {:d}".format(result.status_code)) 
********************************************
********************************************
def main(args, env):
    response = requests.post("{:s}/whitelist".format(args.url), files = {"file": open(args.sample, "rb")})
    if response.status_code != 200:
        print("Server returned error {:d}: {:s}".format(response.status_code, response.content)) 
********************************************
********************************************
def add_sample(self, paths):
        if isinstance(paths, str):
            paths = [paths]
        reply = requests.post("%s/job/%d/add_sample" % (self.url, self.id), files = [(path, open(path, "rb")) for path in paths[0].split(",")])
        if reply.status_code != 200:
            try:
                message = reply.json()["message"]
            except ValueError:
                message = reply.content
            raise RuntimeError("Server returned error code %d: %s" % (reply.status_code, message)) 
********************************************
********************************************
def submit(self):
        reply = requests.post("%s/job/%d/submit" % (self.url, self.id))
        if reply.status_code != 200:
            try:
                message = reply.json()["message"]
            except ValueError:
                message = reply.content
            raise RuntimeError("Server returned error code %d: %s" % (reply.status_code, message)) 
********************************************
********************************************
def create_job(self):
        reply = requests.post("%s/job" % self.url)
        if reply.status_code == 200:
            return Job(self.url, reply.json()["job"])
        else:
            try:
                message = reply.json()["message"]
            except ValueError:
                message = reply.content
            raise RuntimeError("Server returned error code %d: %s" % (reply.status_code, message)) 
********************************************
********************************************
def find_station(self, search_term):

        LOG.debug("pre-alias search_term: " + search_term);
        search_term = self.apply_aliases(search_term)
        LOG.debug("aliased search_term: " + search_term);

        payload = { "query" : search_term }
        # get the response from the TuneIn API
        res = requests.post(base_url, data=payload, headers=headers)
        dom = parseString(res.text)
        # results are each in their own <outline> tag as defined by OPML (https://en.wikipedia.org/wiki/OPML)
        entries = dom.getElementsByTagName("outline")

        # Loop through outlines in the lists
        for entry in entries:
            # Only look at outlines that are of type=audio and item=station
            if (entry.getAttribute("type") == "audio") and (entry.getAttribute("item") == "station"):
                if (entry.getAttribute("key") != "unavailable"):
                    # stop the current stream if we have one running
                    if (self.audio_state == "playing"):
                        self.stop()
                    # Ignore entries that are marked as unavailable
                    self.mpeg_url = entry.getAttribute("URL")
                    self.station_name = entry.getAttribute("text")
                    # this URL will return audio/x-mpegurl data. This is just a list of URLs to the real streams
                    self.stream_url = self.get_stream_url(self.mpeg_url)
                    self.audio_state = "playing"
                    self.speak_dialog("now.playing", {"station": self.station_name} )
                    wait_while_speaking()
                    LOG.debug("Found stream URL: " + self.stream_url)
                    self.process = play_mp3(self.stream_url)
                    return

        # We didn't find any playable stations
        self.speak_dialog("not.found")
        wait_while_speaking()
        LOG.debug("Could not find a station with the query term: " + search_term) 
********************************************
********************************************
def callback(request):
    body = json.loads(request.body)
    text = body['message']['text'].split(' ')
    token = None
    if len(text) > 1:
        token = text[1]

    bot_key = os.environ.get('TELEGRAM_API_KEY')
    chat_id = body['message']['chat']['id']

    try:
        notification = Notification.objects.get(channel='telegram', connect_token=token)
        notification.channel_id = chat_id
        notification.save()

        text = "Welcome to the MuN"
        send_message_url = f'https://api.telegram.org/bot{bot_key}/sendMessage?chat_id={chat_id}&text={text}'
        requests.post(send_message_url)

        return HttpResponse()
    except Notification.DoesNotExist:
        text = "Sorry, seems like the MuN is too far..."
        send_message_url = f'https://api.telegram.org/bot{bot_key}/sendMessage?chat_id={chat_id}&text={text}'
        requests.post(send_message_url)

        return HttpResponse() 
********************************************
********************************************
def _client_wrapper(self, rpc_name, rpc_signature):
        def _rpc_call(*args, **kargs):
            for k,v in zip(rpc_signature.parameters, args):
                 kargs[k] = v
            res = requests.post("http://%s:%s/%s" % (self.ip, self.port, rpc_name), params=kargs)
            return res.json() if res.status_code == 200 else None
        return _rpc_call 
********************************************
********************************************
def get_token_from_code(auth_code, redirect_uri):
    # Build the post form for the token request
    post_data = {'grant_type': 'authorization_code',
                 'code': auth_code,
                 'redirect_uri': redirect_uri,
                 'scope': ' '.join(str(i) for i in scopes),
                 'client_id': client_id,
                 'client_secret': client_secret}

    r = requests.post(token_url, data=post_data)
    try:
        return r.json()
    except:
        return 'Error retrieving token: {0} - {1}'.format(r.status_code, r.text) 
********************************************
********************************************
def jenkins_post(url, config_xml):

    try:

        log.info('Posting data to jenkins: %s' % url)
        headers = {'Content-Type': 'text/xml'}
        auth = HTTPBasicAuth(jenkins_user, jenkins_pass)
        r = requests.post(url, verify=False, headers=headers, auth=auth, data=config_xml)
    
        if r.status_code == requests.codes.ok:
            log.info('Success: %s' % r.status_code)
            return r
        else:
            msg = 'There was an error posting to Jenkins: http_status_code={0}s,reason={1},request={2}'.format(r.status_code, r.reason, url)
            log.error(msg)
            raise Exception(msg)

    except Exception, e:
        msg = 'Failed to create jenkins conf job: {0}'.format(e)
        log.error(msg)
        raise Exception(msg) 
********************************************
********************************************
def send(self, msg):
        payload = msg
        r = requests.post(self.url, data=json.dumps(payload))

        if (r.status_code == requests.codes.ok):
            print "FlowMod Succeeded - "+str(r.status_code)
        else:
            print "FlowMod Failed - "+str(r.status_code) 
********************************************
********************************************
def demo__custom_identity_verify(identity_dict):
    """
    For CC98 identity verify

    :type identity_dict: dict
    """
    import hashlib
    import requests
    import config

    if 'cc98_username' not in identity_dict or 'cc98_password' not in identity_dict:
        return False

    try:
        pass_md5 = hashlib.md5()
        pass_md5.update(identity_dict['cc98_password'].encode())
        pass_md5 = pass_md5.hexdigest()
        if config.is_use_proxy:
            proxy = config.requests_proxies
        else:
            proxy = None
        r = requests.post('http://www.cc98.org/sign.asp', data={
            'a': 'i',
            'u': identity_dict['cc98_username'],
            'p': pass_md5,
            'userhidden': 2
        }, proxies=proxy)
        if r.text == '9898':
            return True
        else:
            return False
    except:
        return False


# Demo for Twitter 
********************************************
********************************************
def test_decision_tree(self):
        from requests import post
        from os import system
        from pandas import DataFrame, read_json
        from sklearn.datasets import load_iris
        from sklearn.tree import DecisionTreeClassifier

        iris = load_iris()
        input_df = DataFrame(data=iris['data'], columns=iris['feature_names'])
        clf = DecisionTreeClassifier(max_depth=2)
        clf.fit(input_df.values, iris['target'])

        # convert classifier to Docker container
        from sklearn2docker.constructor import Sklearn2Docker
        s2d = Sklearn2Docker(
            classifier=clf,
            feature_names=iris['feature_names'],
            class_names=iris['target_names'].tolist(),
        )
        s2d.save(
            name="classifier",
            tag="iris",
        )

        # run your Docker container as a detached process
        system("docker run -d -p {}:5000 --name {} classifier:iris && sleep 5".format(self.port, self.container_name))

        # send your training data as a json string
        request = post("http://localhost:{}/predict/split".format(self.port), json=input_df.to_json(orient="split"))
        result = read_json(request.content.decode(), orient="split")
        self.assertEqual(list(result), ['prediction'])
        self.assertGreater(len(result), 0)

        request = post("http://localhost:{}/predict_proba/split".format(self.port), json=input_df.to_json(orient="split"))
        result = read_json(request.content.decode(), orient="split")
        self.assertEqual(list(result), ['setosa', 'versicolor', 'virginica'])
        self.assertGreater(len(result), 0) 
********************************************
********************************************
def test_barebones_keras(self):
        from sklearn.datasets import load_iris
        from pandas import DataFrame
        from numpy import array
        from os import system
        from pandas import read_json
        from requests import post

        iris = load_iris()
        input_df = DataFrame(data=iris['data'], columns=iris['feature_names'])
        model = self.create_categorical_classification_model()
        X, Y = input_df.values, array(iris['target'])
        model.fit(X, Y)

        # convert classifier to Docker container
        from sklearn2docker.constructor import Sklearn2Docker
        s2d = Sklearn2Docker(
            classifier=model,
            feature_names=list(input_df),
            class_names=iris['target_names'].tolist(),
        )
        s2d.save(
            name="classifier",
            tag="keras",
        )

        # # run your Docker container as a detached process
        system("docker run -d -p {}:5000 --name {} classifier:keras && sleep 5".format(self.port, self.container_name))

        # send your training data as a json string
        request = post("http://localhost:{}/predict/split".format(self.port), json=input_df.to_json(orient="split"))
        result = read_json(request.content.decode(), orient="split")
        self.assertEqual(len(list(result)), 1)
        self.assertEqual(len(result), len(input_df))

        request = post("http://localhost:{}/predict_proba/split".format(self.port), json=input_df.to_json(orient="split"))
        result = read_json(request.content.decode(), orient="split")
        self.assertEqual(len(list(result)), 3)
        self.assertEqual(len(result), len(input_df)) 
********************************************
********************************************
def _post(self):
        if self.data_type == "json":
            return requests.post(self.config['url'], json=self.data, headers=self.headers, timeout=10, proxies=self.proxy)
        elif self.data_type == "urlencoded":
            return requests.post(self.config['url'], data=self.data, headers=self.headers, timeout=10, proxies=self.proxy) 
********************************************
********************************************
def track(self, page):
        url = 'https://ssl.google-analytics.com/collect'
        payload = {
            'v': 1,
            'tid': 'UA-23742434-4',
            'cid': self._get_visitorid(),
            't': 'screenview',
            'an': 'Lynda.com Kodi Addon',
            'av': self.version,
            'cd': page
        }

        headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:11.0) Gecko/20100101 Firefox/11.0'}
        r = requests.post(url, data=payload, headers=headers) 
********************************************
********************************************
def __index_login(self):
        login_page = requests.get('https://pass.neu.edu.cn/tpass/login', proxies=proxies['index'])
        # 生成登录参数
        lt = re.findall("input type=\"hidden\" id=\"lt\" name=\"lt\" value=\"(.*?)\" />", login_page.text)[0]
        execution = re.findall("input type=\"hidden\" name=\"execution\" value=\"(.*?)\" />", login_page.text)[0]
        rsa = self.id + self.password + lt
        ul = len(self.id)
        pl = len(self.password)

        # JESESSIONID是一个必不可少的cookie
        self.pass_cookies['jsessionid_tpass'] = login_page.cookies['jsessionid_tpass']

        post_data = {
            'rsa': rsa,
            'ul': ul,
            'pl': pl,
            'lt': lt,
            'execution': execution,
            '_eventId': 'submit'
        }

        login_post = requests.post('https://pass.neu.edu.cn/tpass/login',
                                   headers=self.__headers,
                                   cookies=login_page.cookies,
                                   proxies=proxies['index'],
                                   data=post_data)
        for i in login_post.history:
            if 'CASTGC' in i.cookies:
                self.pass_cookies['CASTGC'] = i.cookies['CASTGC']
            if 'tp_up' in i.cookies:
                self.index_cookies = i.cookies
                self.success = True

    # 通过一网通办登录图书馆，私有方法 
********************************************
********************************************
def card_info(self):
        res = requests.post('https://portal.neu.edu.cn/tp_up/up/subgroup/getCardMoney',
                            cookies=self.index_cookies,
                            headers=self.__headers,
                            data='{}',
                            proxies=proxies['index'])
        return res.json()

    # 校园网使用情况 
********************************************
********************************************
def net_info(self):
        res = requests.post('https://portal.neu.edu.cn/tp_up/up/subgroup/getWlzzList',
                            cookies=self.index_cookies,
                            headers=self.__headers,
                            data='{}',
                            proxies=proxies['index'])
        return res.json()

    # 学生邮箱情况 
********************************************
********************************************
def email_info(self):
        res = requests.post('https://portal.neu.edu.cn/tp_up/up/subgroup/getBindEmailInfo',
                            cookies=self.index_cookies,
                            headers=self.__headers,
                            data='{}',
                            proxies=proxies['index'])
        return res.json()

    # 获取报销情况 
********************************************
********************************************
def mobile_info(self):
        post_url = 'https://portal.neu.edu.cn/tp_up/sys/uacm/profile/getMobileEmail'
        my_headers = {
            'Referer': 'https://portal.neu.edu.cn/tp_up/view?m=up',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36',
            'Origin': 'https://portal.neu.edu.cn',
            'Content-Type': 'application/json;charset=UTF-8'
        }
        response = requests.post(post_url,
                                 headers=my_headers,
                                 cookies=self.index_cookies,
                                 data='{}',
                                 proxies=proxies['index'])
        return response.json() 
********************************************
********************************************
def get_course(self, school_year, semester):
        data = requests.post("https://portal.neu.edu.cn/tp_up/up/widgets/getClassbyUserInfo",
                            cookies=self.index_cookies,
                            headers={
                                'Accept':'application/json, text/javascript, */*; q=0.01',
                                'Content-Type': 'application/json;charset=UTF-8'
                            },
                            proxies=proxies['index'],
                            data='{"schoolYear":"%s","semester":"%s","learnWeek":"1"}'%(school_year,semester)).json()
        courses = []
        for i in range(len(data)):
            if data[i]['KKXND']!=school_year or data[i]['KKXQM']!=semester:
                continue
            not_have = True
            for j in courses:
                if j['name'] == data[i]['KCMC']:
                    is_have = False
                    if data[i]['JSXM'] not in j['teachers']:
                        j['teachers'].append(data[i]['JSXM'])
                    j['schedules'].append({
                        "weeks": self.__week_num(data[i]['SKZC']),
                        "day": int(data[i]['SKXQ']),
                        "section": [ section for section in range(int(data[i]['KKXQM']),  1+int(data[i]['KKXQM'])+int(data[i]['SKJC']))],
                        "classroom": data[i]['JXDD']
                    })
                    continue
            if not_have:
                courses.append({
                    "name": data[i]['KCMC'],
                    "teachers": [data[i]['JSXM']],
                    "schedules": [{
                        "weeks": self.__week_num(data[i]['SKZC']),
                        "day": int(data[i]['SKXQ']),
                        "section": [ section for section in range(int(data[i]['KKXQM']),  1+int(data[i]['KKXQM'])+int(data[i]['SKJC']))],
                        "classroom": data[i]['JXDD']
                    }]
                })
        return courses

    # 通过教务处获取课表以及课程信息，如果教务处限制外网访问将无法在外网使用，建议使用get_course 
********************************************
********************************************
def lost_card(self, card_pwd, human_id):
        lost_page = requests.get('http://ecard.neu.edu.cn/selfsearch/User/UserLoss.aspx',
                                 cookies=self.card_cookies, headers=self.__headers)
        data = {
            '__VIEWSTATE': re.findall('id="__VIEWSTATE" value="(.*?)"', lost_page.text)[0],
            '__EVENTVALIDATION': re.findall('id="__EVENTVALIDATION" value="(.*?)"', lost_page.text)[0],
            'ctl00$ContentPlaceHolder1$txtPwd': card_pwd,
            'ctl00$ContentPlaceHolder1$txtIDcardNo': human_id,
            'ctl00$ContentPlaceHolder1$btnLoss': '挂  失'
        }
        response = requests.post('http://ecard.neu.edu.cn/selfsearch/User/UserLoss.aspx',
                                 data=data,
                                 cookies=self.card_cookies,
                                 headers=self.__headers,
                                 proxies=proxies['card'])
        return re.findall("showMsg\('(.*?)'\)", response.text)[0]

    # 登录校园网关，如果登录成功，返回ip 
********************************************
********************************************
def get_songs(self, query, offset=0, count=DEFAULT_QUERY_NUM):
        if not query or len(query) == 0:
            return [], 0
        search_url = "https://music.163.com/weapi/cloudsearch/get/web"
        raw_data = {
            "csrf_token": "",
            "hlposttag": "</span>",
            "hlpretag": "<span class=\"s-fc7\">",
            # 最大为100
            "limit": "%s" % (count if count else NeteaseSongSpider.DEFAULT_QUERY_NUM),
            "offset": "%s" % offset,
            "s": query,
            "total": "true",
            "type": "1"
        }
        post_data = get_encrypted_post_data(raw_data)

        resp = requests.post(search_url, data=post_data, headers=self.headers)
        json = resp.json()
        result = []
        total_count = 0

        if "result" not in json or not json["result"] or "songs" not in json["result"]:
            return result, total_count

        for json_song in json["result"]["songs"]:
            result.append(parse_song(json_song))

        if "songCount" in json["result"]:
            total_count = json["result"]["songCount"]

        return result, total_count 
********************************************
********************************************
def _get_song_url(self, song):
        url = "https://music.163.com/weapi/song/enhance/player/url"
        raw_data = {
            # "br": 128000,
            "br": 320000,   # 比特率
            "csrf_token": "",
            "ids": "[%s]" % song.id
        }
        post_data = get_encrypted_post_data(raw_data)
        r = requests.post(url, data=post_data, headers=self.headers)
        json_data = r.json()
        song_url = json_data["data"][0]["url"]

        return song_url 
********************************************
********************************************
def sendSignedPostRequest(self, uri, data):
        r = requests.post(self.endpoint + uri, json=data, auth=self.auth)
        return r.json() 
********************************************
********************************************
def token(self):
        """Cached authentication token"""
        if not self._token:
            self._token = requests.post(self.url,
                json.dumps({'id': 1, 'method': 'Authenticate', 
                    'params': {'API': 1, 'Password': self.password}, 
                    'jsonrpc': '2.0'}),
                verify=False).json()["result"]["Token"]
        return self._token 
********************************************
********************************************
def request(self, method, params):
        """Execute authenticated request"""
        return requests.post(self.url, 
            json.dumps({'id': 1, 'method': method, 'params': params, 
                'jsonrpc': '2.0', 'Token': self.token}),
            verify=False
        ).json() 
********************************************
********************************************
def data_Crawling(from_h,to_h,date_h,i_orgcity, i_dstCity):
    url = 'http://www.ceair.com/otabooking/flight-search!doFlightSearch.shtml?rand=0.9983412510479111'
    data ='searchCond={"tripType":"OW","adtCount":1,"chdCount":0,"infCount":0,"currency":"CNY","sortType":"a","segmentList":[{"deptCd":"'+from_h+'","arrCd":"'+to_h+'","deptDt":"'+date_h+'","deptCdTxt":"'+i_orgcity+'","arrCdTxt":"'+i_dstCity+'","deptCityCode":"'+from_h+'","arrCityCode":"'+to_h+'"}],"sortExec":"a","page":"0"}'
    try:
        resp1 = requests.post(url,data = data,headers = headers,timeout = 15)
        return resp1.text,resp1.status_code
    except requests.exceptions.ReadTimeout:
        return -1,-1
    except requests.exceptions.ConnectionError:
        return -2,-2
# 解析数据 
********************************************
********************************************
def data_Crawling(from_h,to_h,date_h,i_orgcity, i_dstCity):
    url = 'http://www.ceair.com/otabooking/flight-search!doFlightSearch.shtml?rand=0.9983412510479111'
    data ='searchCond={"tripType":"OW","adtCount":1,"chdCount":0,"infCount":0,"currency":"CNY","sortType":"a","segmentList":[{"deptCd":"'+from_h+'","arrCd":"'+to_h+'","deptDt":"'+date_h+'","deptCdTxt":"'+i_orgcity+'","arrCdTxt":"'+i_dstCity+'","deptCityCode":"'+from_h+'","arrCityCode":"'+to_h+'"}],"sortExec":"a","page":"0"}'
    try:
        resp1 = requests.post(url,data = data,headers = headers,timeout = 15)
        return resp1.text,resp1.status_code
    except requests.exceptions.ReadTimeout:
        return -1,-1
    except requests.exceptions.ConnectionError:
        return -2,-2
# 解析数据 
********************************************
********************************************
def post_annotation(annotation, api_key):
    ''' Takes annotation dict and api_key string'''
    base_url = 'https://api.circonus.com/v2'
    anootate_post_endpoint = '/annotation'
    resp = requests.post(base_url + anootate_post_endpoint,
                         headers=build_headers(api_key), data=json.dumps(annotation))
    resp.raise_for_status()
    return resp 
********************************************
********************************************
def import_file(xapi, module, ip_address, file_, category):
    xapi.keygen()

    params = {
        'type': 'import',
        'category': category,
        'key': xapi.api_key
    }

    filename = os.path.basename(file_)

    mef = requests_toolbelt.MultipartEncoder(
        fields={
            'file': (filename, open(file_, 'rb'), 'application/octet-stream')
        }
    )

    r = requests.post(
        'https://' + ip_address + '/api/',
        verify=False,
        params=params,
        headers={'Content-Type': mef.content_type},
        data=mef
    )

    # if something goes wrong just raise an exception
    r.raise_for_status()

    resp = xml.etree.ElementTree.fromstring(r.content)

    if resp.attrib['status'] == 'error':
        module.fail_json(msg=r.content)

    return True, filename 
********************************************
********************************************
def getGlucoseDex():
	# Get most recent glucose from Dexcom Share
	# Code adapted from the Share to Nightscout bridge, via @bewest and @shanselman
	# https://github.com/bewest/share2nightscout-bridge
	# Login and get a Dexcom Share session ID
	# ... need to only do this once and then refresh the sessionID as necessary
	dexLoginURL = "https://share1.dexcom.com/ShareWebServices/Services/General/LoginPublisherAccountByName"
	dexLoginPayload = {
            "User-Agent": "Dexcom Share/3.0.2.11 CFNetwork/711.2.23 Darwin/14.0.0",
            "applicationId": "d89443d2-327c-4a6f-89e5-496bbb0317db",
            "accountName": dexUsername,
            "password": dexPassword,
        }
	dexLoginHeaders = {
	    'content-type': "application/json",
	    'accept': "application/json",
	    }
	dexLoginResponse = requests.post(dexLoginURL, json=dexLoginPayload, headers=dexLoginHeaders)
	sessionID = dexLoginResponse.json()
	# Use the session ID to retrieve the latest glucose record
	dexGlucoseURL = "https://share1.dexcom.com/ShareWebServices/Services/Publisher/ReadPublisherLatestGlucoseValues"
	dexGlucoseQueryString = {"sessionID":sessionID,"minutes":"1440","maxCount":"1"}
	dexGlucoseHeaders = {
	    'content-type': "application/json",
	    'accept': "application/json",
	    }
	dexGlucoseResponse = requests.post(dexGlucoseURL, headers=dexGlucoseHeaders, params=dexGlucoseQueryString)
	dexGlucoseResponseJSON = dexGlucoseResponse.json()
	dexGlucose = dexGlucoseResponseJSON[0]["Value"]
	dexGlucoseEpochString = dexGlucoseResponseJSON[0]["ST"]
	dexGlucoseEpoch = int(re.match('/Date\((\d+)\)/', dexGlucoseEpochString).group(1))/1e3
	dexGlucoseTimestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(dexGlucoseEpoch))
	print("Current Glucose (Share):      " + str(dexGlucose) + " " + glucoseUnit + " at " + time.strftime("%-I:%M:%S %p on %A, %B %d, %Y",time.localtime(dexGlucoseEpoch)))
	return dexGlucose 
********************************************
********************************************
def factory_list(token, body = dict(misc.CRF_BODY)):
    '''(str [, dict]) ->  list of dict
    token: "Token" value in the .auth.fj_login(username, password) dict
    body: search parameters in the format of .misc.CRF_BODY

    returns a list of robots per the request parameters'''

    url = r'https://factory.robocraftgame.com/api/roboShopItems/list'
    headers = misc.make_headers(token)
    response = requests.post(url, json=body, headers=headers)
    return response.json()['response']['roboShopItems'] 
********************************************
********************************************
def getNote(self,gid, commentLimit, likeLimit):
        header = {
            "Content-Type" : "application/json",
            "X-Line-Mid" : self.mid,
            "x-lct": self.channel_access_token,

        }
        r = requests.get(
            "http://" + self.host + "/mh/api/v27/post/list.json?homeId=" + gid + "&commentLimit=" + commentLimit + "&sourceType=TALKROOM&likeLimit=" + likeLimit,
            headers = header
        )
        return r.json() 
********************************************
********************************************
def getHome(self,mid):
        header = {
                    "Content-Type": "application/json",
                    "User-Agent" : self.UA,
                    "X-Line-Mid" : self.mid,
                    "x-lct" : self.channel_access_token,
        }

        r = requests.get(
        "http://" + self.host + "/mh/api/v27/post/list.json?homeId=" + mid + "&commentLimit=2&sourceType=LINE_PROFILE_COVER&likeLimit=6",
        headers = header
        )
        return r.json() 
********************************************
********************************************
def main():
   # credentials
   dfcreds = get_credentials(keyfile) # get the authentication information from the keyfile
   headers = {'content-type': 'application/json', 'HLIAMKey': dfcreds['key']}
   
   # Viewable Communities
   rViewableCommunities = requests.get(repo_path + 'api/v2.0/Communities/GetViewableCommunities', headers=headers)
   dfViewableCommunities = pd.read_json(rViewableCommunities.content)

   # Community Members
   payload = {
           "CommunityKey": 'd06df790-8ca4-4e54-91a0-244af0228ddc',
           "StartRecord": 1,
           "EndRecord": 1500
           }
   rCommunityMembers = requests.post(repo_path + 'api/v2.0/Communities/GetCommunityMembers', headers=headers, json=payload)
   dfCommunityMembers = pd.read_json(rCommunityMembers.content)
   # add a timestamp to the data
   dfCommunityMembers['cmtimestamp'] = dt.datetime.now()
   dfCommunityMembers.index.names = ['rowUID']
   dfCommunityMembers.drop('Community', 1, inplace=True) #remove the nested dictionary of community information
   dfCommunityMembers.to_csv(file_path + 'techsup_hl_communitymembers.csv', index=False, date_format="%Y-%m-%d")

   # Discussion Posts
   rDiscussionPosts = requests.get(repo_path + 'api/v2.0/Discussions/GetDiscussionPosts?maxToRetrieve=5000', headers=headers)
   dfDiscussionPosts = pd.read_json(rDiscussionPosts.content)
   # add a timestamp to the data
   dfDiscussionPosts['dptimestamp'] = dt.datetime.now()
   dfDiscussionPosts.to_csv(file_path + 'techsup_hl_discussionposts.csv', index=False, date_format="%Y-%m-%d") 
********************************************
********************************************
def send_to_rapidpro(msg = {}):
    '''sends a given message to the RapidPro server'''

    try:
        #if there is a keyword in the message, just remove it before forwarding to RapidPro
        keyword = KANNEL_SERVERS[msg['host']]['keyword']
        logger.debug("The server %s use the keyword %s", msg['host'], keyword)

        if keyword is not None:
            text = msg['text'].split()
            if text[0].upper() == keyword.upper(): #match using the same case
                logger.debug("Removing keyword %s from the message %s", keyword, msg['text'])
                msg['text'] = " ".join(text[1:]) #remove the keyword from the message and reconstruct the SMS

        url = RAPIDPRO_URLS['RECEIVED']

        data = { #the data to be sent in the body of the request
                'from'  : msg['from'],
                'text'  : msg['text'],
        }
        r = post(url=url, data = data)
        logger.debug("Sending request to RapidPro server at %s", r.url)
        logger.debug("Data inside request to RapidPro server is %s", r.request.body)
        logger.debug("The response we got from RapidPro is %s", r.text)

        r.raise_for_status() #Will raise an exception with the HTTP code ONLY IF the HTTP status was NOT 200
        return True
    except Exception as e:
        logger.debug("Exception %s occurred", e)
        raise e 
********************************************
********************************************
def upload_file(self, filetosubmit, vmprofile=1):
        '''
        Description: Upload procedure, uploads a file to the ATD server for inspection
        Input:
                 filetosubmit:  Path to the file to be submitted
                 vmProfileList: ATD Profile ID for inspection

        Output:      Two possible values:

                 (0, error_info): Unsucessful procedure
                 (1, {'jobID':'xxx','taskId':'xxx','file':'xxx','md5':'xxx','size':'xxx':'mimeType':'xxx'}): Sucessful upload
        '''

        url = 'https://%s/php/fileupload.php' % self.atdserver

        payload = {"data": {"vmProfileList": vmprofile, "submitType": 0}, "amas_filename": self.get_filename(filetosubmit)}

        data = json.dumps(payload)

        try:
            files = {'amas_filename': (self.get_filename(filetosubmit), open(filetosubmit, 'rb'))}
        except Exception as e:
            error_info = 'Upload method: Error opening file: %s' % e
            return(0, error_info)

        custom_header = {
            'Accept': 'application/vnd.ve.v1.0+json',
            'VE-SDK-API': '%s' % self.b64(self.session, self.userId),
            'accept-encoding': 'gzip;q=0,deflate,sdch'
        }

        try:
            r = requests.post(url, headers=custom_header, files=files, data={'data': data}, verify=False)

        except Exception as e:
            error_info = 'Error submitting file to ATD:\n%s' % e
            return(0, error_info)

        if r.status_code == 200:
            server_info = json.loads(r.content)
            if server_info['success'] is True:
                info = {
                    'jobId': server_info['subId'],
                    'taskId': server_info['results'][0]['taskId'],
                    'file': server_info['results'][0]['file'],
                    'md5': server_info['results'][0]['md5'],
                    'size': server_info['results'][0]['size'],
                    'mimeType': server_info['mimeType']
                }
                return (1, info)
            else:
                error_info = 'Upload operation did not return a success value'
                return (0, error_info)
        else:
            error_info = 'Error uploading file, bad credentials or header - status code: %d' % r.status_code
            return (0, error_info) 
********************************************
********************************************
def _get_data(self):
        conf = get_config()
        try:
            res = requests.post('https://www.yr.no/place/%s/%s/%s/forecast.xml'
                                % (conf['api']['yrno']['country'], conf['api']['yrno']['province'], conf['api']['yrno']['city']))
        except RequestException or HTTPError:  # retry on error
            logger.error("Caught RequestException")
            res = requests.post('https://www.yr.no/place/%s/%s/%s/forecast.xml'
                                % (conf['api']['yrno']['country'], conf['api']['yrno']['province'], conf['api']['yrno']['city']))
        res.raise_for_status()
        root = ET.fromstring(res.text)

        # filter data and parse to weather dict
        legal_xml = root.find('credit').find('link')
        location_xml = root.find('location')
        sun_xml = root.find('sun')
        weather_data = {
            'credit': {
                "text": legal_xml.get('text'),
                "url": legal_xml.get('url')
            },
            'city': location_xml.find('name').text,
            'country': location_xml.find('country').text,
            'lastUpdate': time.strptime(root.find('meta').find('lastupdate').text, time_format_str),
            'sun': {
                "rise": time.strptime(sun_xml.get('rise'), time_format_str),
                "set": time.strptime(sun_xml.get('set'), time_format_str)
            },
            "forecast": []
        }
        tabular_xml = root.find('forecast').find('tabular')
        for time_xml in tabular_xml.findall('time'):
            symbol_xml = time_xml.find('symbol')
            wind_xml = time_xml.find('windSpeed')
            weather_data['forecast'].append({
                'time': {
                    "from": time.strptime(time_xml.get('from'), time_format_str),
                    "to": time.strptime(time_xml.get('to'), time_format_str)
                },
                'symbol': {
                    "id": symbol_xml.get('number'),
                    "description": symbol_xml.get('name')
                },
                'precipitation': time_xml.find('precipitation').get('value'),
                'wind': {
                    "direction": time_xml.find('windDirection').get('code'),
                    "mps": wind_xml.get('mps'),
                    "description": wind_xml.get('name')
                },
                "celsius": time_xml.find('temperature').get('value')
            })

        logger.info("retrieved data: %s" % weather_data)
        self.data = weather_data 
********************************************
********************************************
def upload_file(body, fileName, contentType, contentLength):

    # 1. GET FILE STORAGE URI
    fileUploadPartsResponse = requests.post(fileUploadRequestURI, 
        headers={
            'Authorization': iotHubSasToken,
            'Content-Type': 'application/json'
        },
        data = '{ "blobName": "%s"}' % (fileName)
    )

    print(fileUploadRequestURI)
    print(fileUploadPartsResponse.status_code)
    print(fileUploadPartsResponse.text)

    if fileUploadPartsResponse.status_code == 200:
 
        fileUploadParts = fileUploadPartsResponse.json()
        fileUploadURI = fileUploadURITemplate % (fileUploadParts["hostName"], fileUploadParts["containerName"], fileUploadParts["blobName"], fileUploadParts["sasToken"])
        
        # 2. UPLOAD FILE TO BLOB STORAGE
        uploadResponse = requests.put(fileUploadURI, 
            headers={
                'Content-Type': contentType,
                'Content-Length': contentLength,
                'x-ms-blob-type': 'BlockBlob',
                
            },
            data = body
        )

        print(fileUploadURI)
        print(uploadResponse.status_code)
        print(uploadResponse.text)
        
        if uploadResponse.status_code == 201:

            # 3. GET UPLOAD FILE NOTIFICATION
            notificationResponse = requests.post(notificationURI, 
                headers={
                    'Authorization': iotHubSasToken,
                    'Content-Type': 'application/json'
                },
                data = '{"correlationId": "%s" }' % (fileUploadParts["correlationId"])
            )
    
            print(notificationURI)
            print(notificationResponse.status_code)
            print(notificationResponse.text) 
********************************************
********************************************
def make_api_call(method, url, token, payload = None, parameters = None):
    # Send these headers with all API calls
    headers = { 'User-Agent' : 'django-tutorial/1.0',
                'Authorization' : 'Bearer {0}'.format(token),
                'Accept' : 'application/json'}

    # Use these headers to instrument calls. Makes it easier
    # to correlate requests and responses in case of problems
    # and is a recommended best practice.
    request_id = str(uuid.uuid4())
    instrumentation = { 'client-request-id' : request_id,
                        'return-client-request-id' : 'true' }

    headers.update(instrumentation)

    response = None 

    payload = {
              "Subject": "Discuss the Calendar REST API",
              "Body": {
                "ContentType": "HTML",
                "Content": "I think it will meet our requirements!"
              },
              "Start": {
                  "DateTime": "2014-04-04T18:00:00",
                  "TimeZone": "Pacific Standard Time"
              },
              "End": {
                  "DateTime": "2014-04-04T19:00:00",
                  "TimeZone": "Pacific Standard Time"
              },
              "Attendees": [
                {
                  "EmailAddress": {
                    "Address": "[email protected]",
                    "Name": "Janet Schorr"
                  },
                  "Type": "Required"
                }
              ]
            }

    if (method.upper() == 'GET'):
        response = requests.get(url, headers = headers, params = parameters)
    elif (method.upper() == 'DELETE'):
        response = requests.delete(url, headers = headers, params = parameters)
    elif (method.upper() == 'PATCH'):
        headers.update({ 'Content-Type' : 'application/json' })
        response = requests.patch(url, headers = headers, data = json.dumps(payload), params = parameters)
    elif (method.upper() == 'POST'):
        headers.update({ 'Content-Type' : 'application/json' })
        response = requests.post(url, headers = headers, data = json.dumps(payload), params = parameters)

    return response 
********************************************
********************************************
def test_binary_classifier(self):
        from keras.wrappers.scikit_learn import KerasClassifier
        from sklearn.pipeline import Pipeline
        from sklearn.datasets import load_breast_cancer
        from sklearn import preprocessing
        from pandas import DataFrame
        from numpy import array
        from os import system
        from pandas import read_json
        from requests import post

        breast_cancer = load_breast_cancer()
        input_df = DataFrame(data=breast_cancer['data'], columns=breast_cancer['feature_names'])
        model = Pipeline([
            ('rescale', preprocessing.StandardScaler()),
            ('min_max', preprocessing.MinMaxScaler((-1, 1,))),
            ('nn', KerasClassifier(build_fn=self.create_binary_classification_model, epochs=1, verbose=1)),
        ])
        X, Y = input_df.values, array(breast_cancer['target'])
        model.fit(X, Y)

        # convert classifier to Docker container
        from sklearn2docker.constructor import Sklearn2Docker
        s2d = Sklearn2Docker(
            classifier=model,
            feature_names=list(input_df),
            class_names=breast_cancer['target_names'].tolist(),
        )
        s2d.save(
            name="classifier",
            tag="keras",
        )

        # # run your Docker container as a detached process
        system("docker run -d -p {}:5000 --name {} classifier:keras && sleep 5".format(self.port, self.container_name))

        # send your training data as a json string
        request = post("http://localhost:{}/predict/split".format(self.port), json=input_df.to_json(orient="split"))
        result = read_json(request.content.decode(), orient="split")
        self.assertEqual(len(list(result)), 1)
        self.assertEqual(len(result), len(input_df))

        request = post("http://localhost:{}/predict_proba/split".format(self.port), json=input_df.to_json(orient="split"))
        result = read_json(request.content.decode(), orient="split")
        self.assertEqual(len(list(result)), 1)
        self.assertEqual(len(result), len(input_df)) 
********************************************
********************************************
def test_categorical_classifier(self):
        from keras.wrappers.scikit_learn import KerasClassifier
        from sklearn.pipeline import Pipeline
        from sklearn.datasets import load_iris
        from sklearn import preprocessing
        from pandas import DataFrame
        from numpy import array
        from os import system
        from pandas import read_json
        from requests import post

        iris = load_iris()
        input_df = DataFrame(data=iris['data'], columns=iris['feature_names'])
        model = Pipeline([
            ('rescale', preprocessing.StandardScaler()),
            ('min_max', preprocessing.MinMaxScaler((-1, 1,))),
            ('nn', KerasClassifier(build_fn=self.create_categorical_classification_model, epochs=1, verbose=1)),
        ])
        X, Y = input_df.values, array(iris['target'])
        model.fit(X, Y)

        # convert classifier to Docker container
        from sklearn2docker.constructor import Sklearn2Docker
        s2d = Sklearn2Docker(
            classifier=model,
            feature_names=list(input_df),
            class_names=iris['target_names'].tolist(),
        )
        s2d.save(
            name="classifier",
            tag="keras",
        )

        # # run your Docker container as a detached process
        system("docker run -d -p {}:5000 --name {} classifier:keras && sleep 5".format(self.port, self.container_name))

        # send your training data as a json string
        request = post("http://localhost:{}/predict/split".format(self.port), json=input_df.to_json(orient="split"))
        result = read_json(request.content.decode(), orient="split")
        self.assertEqual(len(list(result)), 1)
        self.assertEqual(len(result), len(input_df))

        request = post("http://localhost:{}/predict_proba/split".format(self.port), json=input_df.to_json(orient="split"))
        result = read_json(request.content.decode(), orient="split")
        self.assertEqual(len(list(result)), 3)
        self.assertEqual(len(result), len(input_df)) 
********************************************
********************************************
def get_course_by_aao(self, semester):
        # 如果还没有登录新教务处则登录
        if self.aao_cookies == None:
            self.__aao_login()
        post_data = {
            'ignoreHead': '1',
            'showPrintAndExport': '1',
            'setting.kind': 'std',
            'startWeek': '',
            'project.id': '1',
            'semester.id': semester,
            'ids': '46600'
        }

        course_data = requests.post("http://219.216.96.4/eams/courseTableForStd!courseTable.action",
                                    data=post_data,
                                    headers=self.__headers,
                                    cookies=self.aao_cookies,
                                    proxies=proxies['aao'])
        teachers = re.findall('var teachers = \[{id:.*?,name:"(.*?)",lab:false}\];', course_data.text)
        course_names = re.findall('actTeacherName.join\(\',\'\),".*?","(.*?)"', course_data.text)
        course_classroom = re.findall('actTeacherName.join\(\',\'\),".*?",".*?",".*?","(.*?)"', course_data.text)
        course_weeks_str = re.findall('actTeacherName.join\(\',\'\),".*?",".*?",".*?",".*?","(.*?)"', course_data.text)
        course_class = re.findall('activity = new TaskActivity([\s\S]*?)var teachers', course_data.text)
        course_class = course_class + re.findall('activity = new TaskActivity([\s\S]*?)table0.marshalTable',
                                                 course_data.text)
        course_info = re.findall(
            '<td>(.*?)</td><td>(.*?)</td><td>(.*?)</td><td>[\s\S]*?<a href=".*?" onclick=".*?" title="点击显示单个教学任务具体安排">.*?</a>[\s\S]*?</td><td>(.*?)</td><td>',
            course_data.text)

        for count in range(0, len(course_class)):
            day = re.findall('index =(\d+)\*unitCount\+\d+', course_class[count])
            class_num = re.findall('index =\d+\*unitCount\+(\d+)', course_class[count])
            this_course = []
            for cour in range(0, len(day)):
                this_course.append({'day': int(day[cour]), 'class_num': int(class_num[cour])})
            course_class[count] = this_course

        res = [
            {
                'teacher': teachers[i],
                'name': course_names[i],
                'classroom': course_classroom[i],
                'weeks': self.__week_num(course_weeks_str[i]),
                'time': course_class[i]
            }
            for i in range(0, len(teachers))
        ]
        info = [
            {
                'course_code': i[0],
                'course_name': i[1],
                'course_score': i[2],
                'course_teacher': i[3]
            }
            for i in course_info
        ]

        return {'table': res, 'info': info}

    # 获取校园卡消费记录 
********************************************
********************************************
def get_card_trade(self, start, end):
        # 如果还没有通过一网通办获取到一卡通的cookies，则获取
        if self.card_cookies == False:
            self.__card_login()
        trade_url = 'http://ecard.neu.edu.cn/selfsearch/User/ConsumeInfo.aspx'
        page = 1
        res = []
        query_page = requests.get('http://ecard.neu.edu.cn/selfsearch/User/ConsumeInfo.aspx',
                                  cookies=self.card_cookies,
                                  proxies=proxies['card'])
        while True:
            data = {
                '__EVENTTARGET': 'ctl00$ContentPlaceHolder1$AspNetPager1',
                '__EVENTARGUMENT': str(page),
                '__VIEWSTATE':
                    re.findall('<input type="hidden" name="__VIEWSTATE" id="__VIEWSTATE"[\s\S]*?value="(.*?)"',
                               query_page.text)[0],
                '__EVENTVALIDATION':
                    re.findall('<input type="hidden" name="__EVENTVALIDATION" id="__EVENTVALIDATION" value="(.*?)" />',
                               query_page.text)[0],
                'ctl00$ContentPlaceHolder1$rbtnType': '0',
                'ctl00$ContentPlaceHolder1$txtStartDate': start,
                'ctl00$ContentPlaceHolder1$txtEndDate': end,
            }

            if page == 1:
                data['ctl00$ContentPlaceHolder1$btnSearch'] = '查  询'
                data['__EVENTARGUMENT'] = ''

            query_page = requests.post(trade_url, data=data,
                                       cookies=self.card_cookies,
                                       headers=self.__headers,
                                       proxies=proxies['card'])
            page_num = re.findall('style="margin-right:5px;">(\d+)</a>', query_page.text)
            trade_time = re.findall('<span id="Content.*?">(.*?)</span>', query_page.text)
            trades = re.findall(
                '</td><td>(.*?)</td><td align="right">(.*?)</td><td align="right">(.*?)</td><td>(.*?)</td><td>(.*?)</td><td>(.*?)</td>',
                query_page.text)
            for i in range(0, len(trade_time)):
                if (trade_time[i] == ''):
                    break
                append_content = {
                    'trade_time': trade_time[i],
                    'trade_type': trades[i][0],
                    'trade_cost': trades[i][1],
                    'remaind': trades[i][2],
                    'salesman': trades[i][3],
                    'place': trades[i][4],
                    'terminal': trades[i][5]
                }
                res.append(append_content)
            if 1 - (str(page + 1) in page_num):
                break
            page += 1
        return res

    # 门禁记录，start与end格式为都如2019-05-18这样 
********************************************
********************************************
def sendSignedPostRequest(self, uri, postData):
        signature = hmac.new(self.secret, msg=uri+postData, digestmod=hashlib.sha512).digest().encode("hex")
        r = requests.post(self.endpoint + uri, headers={"X-Signature": signature}, data=postData)
        #print r, r.text, r.json()
        return r.json() 
********************************************
********************************************
def createAlbum2(self,gid,name,path,oid):
        header = {
                    "Content-Type": "application/json",
                    "User-Agent" : self.UA,
                    "X-Line-Mid" : self.mid,
                    "x-lct" : self.channel_access_token,
        }
        payload = {
                "type" : "image",
                "title" : name
        }
        r = requests.post(
            "http://" + self.host + "/mh/album/v3/album?count=1&auto=0&homeId=" + gid,
            headers = header,
            data = json.dumps(payload)
        )
        #albumId = r.json()["result"]["items"][0]["id"]


        #h = {
        #            "Content-Type": "application/x-www-form-urlencoded",
        #            "User-Agent" : self.UA,
        #            "X-Line-Mid" : gid,
        #            "X-Line-Album" : albumId,
        #            "x-lct" : self.channel_access_token,
                    #"x-obs-host" : "obs-jp.line-apps.com:443",

        #}
        #print r.json()
        #files = {
        #    'file': open(path, 'rb'),
        #}
        #p = {
        #    "userid" : gid,
        #    "type" : "image",
        #    "oid" : oid,
        #    "ver" : "1.0"
        #}
        #data = {
        #    'params': json.dumps(p)
        #}
        #r = requests.post(
        #"http://obs-jp.line-apps.com/oa/album/a/object_info.nhn:443",
        #headers = h,
        #data = data,
        #files = files
        #)
        return r.json()
        #cl.createAlbum("cea9d61ba824e937aaf91637991ac934b","ss3ai","kawamuki.png") 
********************************************
********************************************
def requests_retry_session(
    retries=3,
    backoff_factor=0.3,
    status_forcelist=(500, 502, 504),
    session=None,
):
    session = session or requests.Session()
    retry = Retry(
        total=retries,
        read=retries,
        connect=retries,
        backoff_factor=backoff_factor,
        status_forcelist=status_forcelist,
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session 
********************************************
********************************************
def data_Crawling(from_h,to_h,date_h):
    s = requests.Session()
    dates = date_h.split('-')
    year = dates[0]
    month =  dates[1]
    day =  dates[2]
    try:
        resp =s.get('http://et.airchina.com.cn/InternetBooking/AirLowFareSearchExternal.do?&tripType=OW&searchType=FARE&flexibleSearch=false&directFlightsOnly=false&fareOptions=1.FAR.X&outboundOption.originLocationCode='+from_h+'&outboundOption.destinationLocationCode='+to_h+'&outboundOption.departureDay='+day+'&outboundOption.departureMonth='+month+'&outboundOption.departureYear='+year+'&outboundOption.departureTime=NA&guestTypes%5B0%5D.type=ADT&guestTypes%5B0%5D.amount=1&guestTypes%5B1%5D.type=CNN&guestTypes%5B1%5D.amount=0&pos=AIRCHINA_CN&lang=zh_CN&guestTypes%5B2%5D.type=INF&guestTypes%5B2%5D.amount=0',headers=h1,timeout=60)
        resp2 = s.post('http://et.airchina.com.cn/InternetBooking/AirLowFareSearchExt.do?ajaxAction=true',headers=h1,timeout=60)
        resp1 = s.get('http://et.airchina.com.cn/InternetBooking/AirFareFamiliesFlexibleForward.do',headers=h1,timeout=60)
        return resp1.text,resp1.status_code
    except requests.exceptions.ReadTimeout:
        return -1,-1
    except requests.exceptions.ConnectionError:
        return -2,-2

# 解析数据 
********************************************
********************************************
def __init__(self, url, mutual_auth, cert=None, verify='true', **kwargs):

        self._logger = logging.getLogger("SPOT.INGEST.HDFS_client")
        session = Session()

        if verify == 'true':
            self._logger.info('SSL verification enabled')
            session.verify = True
            if cert is not None:
                self._logger.info('SSL Cert: ' + cert)
                if ',' in cert:
                    session.cert = [path.strip() for path in cert.split(',')]
                else:
                    session.cert = cert
        elif verify == 'false':
            session.verify = False

        super(SecureKerberosClient, self).__init__(url, mutual_auth, session=session, **kwargs) 
********************************************
********************************************
def __init__(self, url, mutual_auth, cert=None, verify='true', **kwargs):

        self._logger = logging.getLogger("SPOT.INGEST.HDFS_client")
        session = Session()

        if verify == 'true':
            self._logger.info('SSL verification enabled')
            session.verify = True
            if cert is not None:
                self._logger.info('SSL Cert: ' + cert)
                if ',' in cert:
                    session.cert = [path.strip() for path in cert.split(',')]
                else:
                    session.cert = cert
        elif verify == 'false':
            session.verify = False

        super(SecureKerberosClient, self).__init__(url, mutual_auth, session=session, **kwargs) 
********************************************
********************************************
def __init__(self, user_agent, token_url_template, platform):
        self.user_agent = user_agent
        self.token_url_template = token_url_template
        self.platform = platform

        self.session = requests.Session()
        self.session.cookies = http.cookiejar.LWPCookieJar()
        if not os.path.exists(COOKIE_FILE):
            self.session.cookies.save(COOKIE_FILE)
        self.session.cookies.load(COOKIE_FILE, ignore_discard=True)
        self.session.headers = {"User-agent": user_agent}
        if os.path.exists(SESSION_FILE):
            self.load()
        else:
            self._state = {
                'api_key': None,
                'client_api_key': None,
                'token': None,
                'access_token': None,
                'access_token_expiry': None
            }
        self.login() 
********************************************
********************************************
def __init__(self, **kwargs):
        super(XueQiuClient, self).__init__()
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:32.0) Gecko/20100101 Firefox/32.0',
            'Host': 'xueqiu.com',
            'Pragma': 'no-cache',
            'Connection': 'keep-alive',
            'Accept': '*/*',
            'Accept-Encoding': 'gzip,deflate,sdch',
            'Cache-Control': 'no-cache',
            'Referer': 'http://xueqiu.com/P/ZH003694',
            'X-Requested-With': 'XMLHttpRequest',
            'Accept-Language': 'zh-CN,zh;q=0.8'
        }
        self.session = requests.Session()
        self.session.headers.update(headers)
        self.account_config = None
        self.config.update({
            "create_cubes_url": "https://xueqiu.com/cubes/create.json",
            "get_token_url": "https://xueqiu.com/service/csrf",
            "get_cubes_list": "https://xueqiu.com/v4/stock/portfolio/stocks.json",
            "get_cubes_detail": "https://xueqiu.com/cubes/quote.json",
        }) 
********************************************
********************************************
def __init__(self, scheme='http', marker=None, timeout=None):
        self.logger = logging.getLogger(__name__)
        self.__session = requests.Session()

        self.set_auth()

        self.marker = marker
        self.__session.headers.update({'X-Context-Marker': marker})

        self.prom_url = self._get_prom_url()
        self.port = self.prom_url.port
        self.host = self.prom_url.hostname
        self.scheme = scheme

        if self.port:
            self.base_url = "%s://%s:%s/api/" % (self.scheme, self.host,
                                                 self.port)
        else:
            # assume default port for scheme
            self.base_url = "%s://%s/api/" % (self.scheme, self.host)

        self.default_timeout = self._calc_timeout_tuple((20, 30), timeout) 
********************************************
********************************************
def test_port_mapping_single_port(self):
        env_name = self._env_name()
        proj_name, py_version = 'test-proj', '3.7'

        proj_dir = self._create_project_dir(proj_name)
        self._commander.run(
            f'create --name={env_name} --version={py_version} {str(proj_dir)}')
        with self._commander.active_env(env_name) as env:
            os.chdir(proj_dir)

            port = 8000
            out = self._commander.run(
                f'run -d -p {port} -- python -m http.server {port}', env=env)
            self.assertCommandOk(out)
            self.assertPortMapperExists(env_name, port)

            s = requests.Session()
            s.mount('http://', HTTPAdapter(
                max_retries=Retry(connect=3, backoff_factor=1)))
            r = s.get(f'http://localhost:{port}')

            self.assertEqual(r.status_code, 200, msg=r.content) 
********************************************
********************************************
def test_port_mapping_multi_ports(self):
        env_name = self._env_name()
        proj_name, py_version = 'test-proj', '3.7'

        proj_dir = self._create_project_dir(proj_name)
        self._commander.run(
            f'create --name={env_name} --version={py_version} {str(proj_dir)}')
        with self._commander.active_env(env_name) as env:
            os.chdir(proj_dir)

            for port in range(8000, 8003):
                out = self._commander.run(
                    f'run -d -p {port} -- python -m http.server {port}',
                    env=env
                )
                self.assertCommandOk(out)
                self.assertPortMapperExists(env_name, port)

                s = requests.Session()
                s.mount('http://', HTTPAdapter(
                    max_retries=Retry(connect=3, backoff_factor=1)))
                r = s.get(f'http://localhost:{port}')

                self.assertEqual(r.status_code, 200, msg=r.content) 
********************************************
********************************************
def requests_retry_session(
    retries=3,
    backoff_factor=0.3,
    status_forcelist=(500, 502, 504),
    session=None,
):
    session = session or requests.Session()
    retry = Retry(
        total=retries,
        read=retries,
        connect=retries,
        backoff_factor=backoff_factor,
        status_forcelist=status_forcelist,
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session 
********************************************
********************************************
def get_raw(stocks) -> dict:
    req = requests.Session()
    req.get(SESSION_URL, proxies=get_proxies())

    r = req.get(
        STOCKINFO_URL.format(
            stock_id=_join_stock_id(stocks),
            time=int(time.time()) * 1000))

    if sys.version_info < (3, 5):
        try:
            return r.json()
        except ValueError:
            return {'rtmessage': 'json decode error', 'rtcode': '5000'}
    else:
        try:
            return r.json()
        except json.decoder.JSONDecodeError:
            return {'rtmessage': 'json decode error', 'rtcode': '5000'} 
********************************************
********************************************
def data_Crawling(from_h,to_h,date_h):
    s = requests.Session()
    dates = date_h.split('-')
    year = dates[0]
    month =  dates[1]
    day =  dates[2]
    try:
        resp =s.get('http://et.airchina.com.cn/InternetBooking/AirLowFareSearchExternal.do?&tripType=OW&searchType=FARE&flexibleSearch=false&directFlightsOnly=false&fareOptions=1.FAR.X&outboundOption.originLocationCode='+from_h+'&outboundOption.destinationLocationCode='+to_h+'&outboundOption.departureDay='+day+'&outboundOption.departureMonth='+month+'&outboundOption.departureYear='+year+'&outboundOption.departureTime=NA&guestTypes%5B0%5D.type=ADT&guestTypes%5B0%5D.amount=1&guestTypes%5B1%5D.type=CNN&guestTypes%5B1%5D.amount=0&pos=AIRCHINA_CN&lang=zh_CN&guestTypes%5B2%5D.type=INF&guestTypes%5B2%5D.amount=0',headers=h1,timeout=60)
        resp2 = s.post('http://et.airchina.com.cn/InternetBooking/AirLowFareSearchExt.do?ajaxAction=true',headers=h1,timeout=60)
        resp1 = s.get('http://et.airchina.com.cn/InternetBooking/AirFareFamiliesFlexibleForward.do',headers=h1,timeout=60)
        return resp1.text,resp1.status_code
    except requests.exceptions.ReadTimeout:
        return -1,-1
    except requests.exceptions.ConnectionError:
        return -2,-2

# 解析数据 
********************************************
********************************************
def get_object(self, name, is_private=False):
        method = 'get'
        appid = self.option['Appid']
        SecretID = self.option['SecretID']
        SecretKey = self.option['SecretKey']
        region = self.option['region']
        bucket = self.option['bucket']
        if name[0] != '/':
            name = '/' + name
        objectName = name
        url = "http://%s-%s.%s.myqcloud.com%s" % (bucket, appid, region, objectName)
        s = requests.Session()
        if is_private:
            auth = Auth(appid, SecretID, SecretKey, bucket, region, method, objectName)
            Authorization = {'Authorization': auth.get_authorization()}
            s.headers.update(Authorization)

        r = s.get(url)
        return r 
********************************************
********************************************
def put_object(self, name, content):
        method = 'put'
        appid = self.option['Appid']
        SecretID = self.option['SecretID']
        SecretKey = self.option['SecretKey']
        region = self.option['region']
        bucket = self.option['bucket']
        if name[0] != '/':
            name = '/' + name
        objectName = name
        url = "http://%s-%s.%s.myqcloud.com%s" % (bucket, appid, region, objectName)

        s = requests.Session()
        auth = Auth(appid, SecretID, SecretKey, bucket, region, method, objectName)
        Authorization = {'Authorization': auth.get_authorization()}
        s.headers.update(Authorization)

        r = s.put(url, data=content)
        if r.status_code == 200:
            return r
        else:
            LOGGER.info(r.content) 
********************************************
********************************************
def head_object(self, name, is_private=False):
        method = 'head'
        appid = self.option['Appid']
        SecretID = self.option['SecretID']
        SecretKey = self.option['SecretKey']
        region = self.option['region']
        bucket = self.option['bucket']
        if name[0] != '/':
            name = '/' + name
        objectName = name
        url = "http://%s-%s.%s.myqcloud.com%s" % (bucket, appid, region, objectName)
        s = requests.Session()
        if is_private:
            auth = Auth(appid, SecretID, SecretKey, bucket, region, method, objectName)
            Authorization = {'Authorization': auth.get_authorization()}
            s.headers.update(Authorization)

        r = s.head(url)
        return r 
********************************************
********************************************
def delete_object(self, name):
        method = 'delete'
        appid = self.option['Appid']
        SecretID = self.option['SecretID']
        SecretKey = self.option['SecretKey']
        region = self.option['region']
        bucket = self.option['bucket']
        if name[0] != '/':
            name = '/' + name
        objectName = name
        url = "http://%s-%s.%s.myqcloud.com%s" % (bucket, appid, region, objectName)

        s = requests.Session()
        auth = Auth(appid, SecretID, SecretKey, bucket, region, method, objectName)
        Authorization = {'Authorization': auth.get_authorization()}
        s.headers.update(Authorization)

        r = s.delete(url)
        if r.status_code == 204:
            return True 
********************************************
********************************************
def _init_session(self):
        """_init_session() is a private method to perfom command
        session initializaton actions.
        As this creates a session without a logged in user, only
        public profile content is available, until a login is
        performed on this session.
        """

        s = self._session = requests.Session()

        s.headers['Accept-charset'] = "utf-8"

        if self._format == "json":
            pass
        elif self._format == 'xmlfm':
            s.headers['Accept'] = "application/xml"
            s.headers['Accept'] = "text/xml"
        else:
            raise ValueError("Invalid format: " + repr(self._format)) 
********************************************
********************************************
def setup_connector(app, name='default', **options):
	if not hasattr(app, 'extensions'):
		app.extensions = {}

	if 'connectors' not in app.extensions:
		app.extensions['connectors'] = {}
	session = Session()

	if 'auth' in options:
		session.auth = options['auth']
	headers = options.get('headers', {})

	if 'Content-Type' not in headers:
		headers['Content-Type'] = 'application/json'
	session.headers.update(headers)

	app.extensions['connectors'][name] = session
	return session 
********************************************
********************************************
def setup_connector(app, name='default', **options):
	if not hasattr(app, 'extensions'):
		app.extensions = {}

	if 'connectors' not in app.extensions:
		app.extensions['connectors'] = {}
	session = Session()

	if 'auth' in options:
		session.auth = options['auth']
	headers = options.get('headers', {})

	if 'Content-Type' not in headers:
		headers['Content-Type'] = 'application/json'
	session.headers.update(headers)

	app.extensions['connectors'][name] = session
	return session 
********************************************
********************************************
def paging(endpoint, request_json) -> tuple:
    """Split input list into pages"""
    result_pages = {}
    session = requests.Session()
    success = True
    while True:
        r_json = utils.vmaas_post_request(endpoint, request_json, session)
        if r_json is None:
            LOGGER.info("Downloading ERROR.")
            success = False
            break
        response_to_list = [(k, v) for k, v in r_json.items()]
        data_index = response_to_list[0][0]
        data = response_to_list[0][1]
        result_pages.setdefault(data_index, {}).update(data)
        LOGGER.info("Downloading CVE/REPOs metadata (page: %s, page_size: %s, pages: %s)",
                    request_json['page'], r_json["page_size"], r_json['pages'])
        if request_json['page'] >= r_json['pages']:
            break
        request_json['page'] += 1
    session.close()

    if success:
        result_pages.update({"page": r_json["page"], "page_size": r_json["page_size"], "pages": r_json["pages"]})
    return (success, result_pages) 
********************************************
********************************************
def __init__(self):
        """init"""
        self.session = requests.Session()
        self.eln_session_id = None
        self.headers = { 
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36'
                        ' (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36',
        'X-Requested-With': 'XMLHttpRequest',
        } 
********************************************
********************************************
def __init__(self, base_url, apikey):
        # The URL in the config should end in /MAAS/, but the api is behind /MAAS/api/2.0/
        self.base_url = base_url + "/api/2.0/"
        self.apikey = apikey

        self.signer = MaasOauth(apikey)
        self.http_session = requests.Session()

        # TODO(sh8121att) Get logger name from config
        self.logger = logging.getLogger('drydock') 
********************************************
********************************************
def __init__(self,
                 host,
                 port=None,
                 scheme='http',
                 auth_gen=None,
                 marker=None,
                 end_user=None,
                 timeout=None):
        self.logger = logging.getLogger(__name__)
        self.__session = requests.Session()
        self.auth_gen = auth_gen

        self.set_auth()

        self.marker = marker
        self.end_user = end_user
        self.__session.headers.update({'X-Context-Marker': marker})

        if end_user:
            self.__session.headers.update({'X-End-User': end_user})

        self.host = host
        self.scheme = scheme

        if port:
            self.port = port
            self.base_url = "%s://%s:%s/api/" % (self.scheme, self.host,
                                                 self.port)
        else:
            # assume default port for scheme
            self.base_url = "%s://%s/api/" % (self.scheme, self.host)

        self.default_timeout = self._calc_timeout_tuple((20, 30), timeout) 
********************************************
********************************************
def get_ks_session(**kwargs):
        # Establishes a keystone session
        if 'token' in kwargs:
            auth = v3.TokenMethod(token=kwargs.get('token'))
        else:
            auth = v3.Password(**kwargs)
        return session.Session(auth=auth) 
********************************************
********************************************
def __init__(self, url, user, password):
        """
            Create a session with the server.
            :param user: User name
            :param password: Password
            :except: Raises RuntimeError if server replies with an error code.
        """
        self.url = url
        self.session = Session()
        self._login(user, password) 
********************************************
********************************************
def requests_retry_session(
    retries=3,
    backoff_factor=5,
    session=None,
):
    session = requests.Session()
    retry = Retry(
        total=retries,
        read=retries,
        connect=retries,
        backoff_factor=backoff_factor,
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    #session.mount('https://', adapter)
    return session

########### Declare Type of station: AWS,ARG or AGRO ########
#station = 'aws' 
********************************************
********************************************
def do_network_work(args) -> None:
        network_resource, *_ = args
        Session = network_resource
        with Session() as s:
            adapter = HTTPAdapter(max_retries=3)
            s.mount('http://', adapter)
            s.get('http://localhost:8080/') 
********************************************
********************************************
def get_session(domain):
    """
    获取一个此域名的 keep-alive 的session
    :param domain: 域名
    :type domain: str
    :rtype: requests.Session
    """
    if domain not in pool:
        pool[domain] = []

    if not hasattr(locked_session, "session"):
        # 这个变量用于存储本线程中被锁定的session
        # 当一个session被拿出来使用时, 会从 pool 中被移除, 加入到下面这个变量中
        # 当线程结束后, 需要调用 release_lock() 来释放被锁定的session
        #    此时被锁定的session会重新进入session池
        locked_session.session = []

    if not pool[domain]:
        # 线程池空, 新建一个 session
        session = {
            "domain": domain,
            "session": requests.Session(),
        }
    else:
        # 从线程池中取出最近的一个
        session = pool[domain].pop()

    session["active"] = time()

    locked_session.session.append(session)

    return session["session"] 
********************************************
********************************************
def __init__(self, logger, user_name, password):
        this_func_name = sys._getframe().f_code.co_name

        self.logger = logger
        self.user_name = user_name
        self.password = password

        session = requests.Session()
        session.headers = {
                'User-Agent': random.choice(pc_browser_ua)
                }
        self.session = session

        self.logger.debug("%s(): user_name: %s\tpassword: %s" % (this_func_name, user_name, password))
        self.logger.debug("%s(): start ..." % this_func_name) 
********************************************
********************************************
def __init__(self, host, username, password, ssl, verify_ssl=True):
        self.host = host
        self.session = requests.Session()
        self.session.auth = (username, password)
        self.ssl = ssl
        self.verify_ssl = verify_ssl 
********************************************
********************************************
def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(
            {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; \
             Linux x86_64; rv:28.0) Gecko/20100101 Firefox/28.0'}) 
********************************************
********************************************
def __init__(self, cookiejar=None):
        # Initialise the session
        self._s = requests.Session()
        self._user_cached = None
        self.logged_in = False

        if cookiejar:
            self._s.cookies = cookiejar

            # Check if the given cookies log the user in
            user = self.user()
            if user:
                self.logged_in = True 
********************************************
********************************************
def refresh_login(self):
        """Deletes persisted cookies which forces a login attempt with current credentials"""

        s = requests.Session()
        empty_cookie_jar = s.cookies
        if util.save_data(addon, self.COOKIE_FILE_NAME, empty_cookie_jar):
            xbmcgui.Dialog().ok(addonname, "Cleared cookies. Please exit the addon and open it again.")
        else:
            xbmcgui.Dialog().ok(addonname, "Could not refresh lynda session cookies") 
********************************************
********************************************
def __init__(
        self, url: str, auth: Auth = None, verify: Verify = True
    ) -> None:
        self.url = url
        self.session = requests.Session()
        if auth is not None:
            self.session.auth = auth
        self.session.verify = verify 
********************************************
********************************************
def list_sessions(self) -> List[Session]:
        """List all the active sessions in Livy."""
        data = self._client.get("/sessions")
        return [Session.from_json(item) for item in data["sessions"]] 
********************************************
********************************************
def get_session(self, session_id: int) -> Optional[Session]:
        """Get information about a session.

        :param session_id: The ID of the session.
        """
        try:
            data = self._client.get(f"/sessions/{session_id}")
        except requests.HTTPError as e:
            if e.response.status_code == 404:
                return None
            else:
                raise
        return Session.from_json(data) 
********************************************
********************************************
def change_params_and_get_the_session():
    global params, datas, login_url, post_url, s, dl_session
    tag = 'Change_Params_And_Get_The_Session'
    print_with_tag(tag, 'Changing Request params..')
    datas['pixiv_id'] = pixiv_user_name
    datas['password'] = pixiv_user_pass
    print_with_tag(tag, 'Post data params changed.')
    s = requests.Session()
    s.headers = params
    # dl_session = requests.Session()
    print_with_tag(tag, 'Session started.') 
********************************************
********************************************
def run_job(id):
    s = requests.Session()

    r = s.post("http://127.0.0.1:5000/api/v1/auth/",
               data={"email": "[email protected]", "password": "123456", "method": "login"})

    s.get("http://127.0.0.1:5000/test_run/auto/%s" % id) 
********************************************
********************************************
def check_version():
    f = codecs.open('version.txt', 'r')
    version = f.readline()
    s = requests.Session()
    r_version = s.get("https://gitee.com/lym51/AutoLine/raw/master/version.txt").text
    if version != r_version:
        print("*" * 25)
        print("本地版本：v%s" % version)
        print("github版本: v%s" % r_version)
        print("AutoLine开源平台代码已有更新，请到下面的地址更新代码:")
        print("https://github.com/small99/AutoLine")
        print("*" * 25)
        exit(0)
    f.close() 
********************************************
********************************************
def get_chaohua_list(cookie, sinceId):
    url = "https://m.weibo.cn/api/container/getIndex?containerid=100803_-_page_my_follow_super"
    if sinceId != '':
        url = url + "&since_id=%s" % sinceId
    headers = {
        'Cookie': cookie
    }
    session = requests.Session()
    response = session.get(url, headers=headers)
    response = response.json()
    return response 
********************************************
********************************************
def data_Crawling(from_h,to_h,date_h):
    s = requests.Session()
    start_url = 'http://b2c.csair.com/B2C40/modules/bookingnew/main/flightSelectDirect.html?t=S&c1='+from_h+'&c2='+to_h+'&d1='+date_h+'&at=1&ct=0&it=0&preUrl=360BUY'
    h2 = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
          'Accept-Encoding':'gzip, deflate, sdch',
          'Accept-Language':'zh-CN,zh;q=0.8',
          'X-Requested-With':'XMLHttpRequest',
          #Cookie:Webtrends=111.160.254.58.1464136223732083; BIGipServerpool_sc_122.119.122.51=830109562.20480.0000; JSESSIONID=0000M2suWXhIhzAP_5BzKDkE78S:1a5jgnqlj; OZ_1U_671=vid=v744f3c10a3ea8.0&ctime=1464140663&ltime=1464140662; OZ_1Y_671=erefer=-&eurl=http%3A//sc.travelsky.com/scet/queryAv.do%3Flan%3Dcn%26countrytype%3D0%26travelType%3D0%26cityNameOrg%3D%25E5%258C%2597%25E4%25BA%25AC%26cityCodeOrg%3DPEK%26cityNameDes%3D%25E5%258E%25A6%25E9%2597%25A8%26cityCodeDes%3DXMN%26takeoffDate%3D2016-05-28%26returnDate%3D2016-05-28%26cabinStage%3D0%26adultNum%3D1%26childNum%3D0&etime=1464139715&ctime=1464140663&ltime=1464140662&compid=671
          'Referer':start_url,
          'Origin':'http://b2c.csair.com',
          'Host':'b2c.csair.com',
          'Proxy-Connection':'keep-alive',
          'Upgrade-Insecure-Requests':1,
          'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.155 Safari/537.36'
          }
    date_h = date_h.replace('-','')
    data = '{"depcity":"'+from_h+'", "arrcity":"'+to_h+'", "flightdate":"'+date_h+'", "adultnum":"1", "childnum":"0", "infantnum":"0", "cabinorder":"0","airline":"1", "flytype":"0", "international":"0", "action":"0", "segtype":"1", "cache":"0", "preUrl":"360BUY", "isMember":""}'
    data = urllib.quote(data)
    res_url ='http://b2c.csair.com/B2C40/query/jaxb/direct/query.ao?json='+data
    try:
        re =s.get(start_url,headers = h1,timeout = 15)
        resp1 = s.post(res_url,headers = h2,timeout = 15)
        return resp1.text,resp1.status_code
    except requests.exceptions.ReadTimeout:
        return -1,-1
    except requests.exceptions.ConnectionError:
        return -2,-2
    except AttributeError:
        #print resp.text
        return -1,-1
# 解析数据 
********************************************
********************************************
def data_Crawling(from_h,to_h,date_h):
    s = requests.Session()
    start_url = 'http://b2c.csair.com/B2C40/modules/bookingnew/main/flightSelectDirect.html?t=S&c1='+from_h+'&c2='+to_h+'&d1='+date_h+'&at=1&ct=0&it=0&preUrl=360BUY'
    h2 = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
          'Accept-Encoding':'gzip, deflate, sdch',
          'Accept-Language':'zh-CN,zh;q=0.8',
          'X-Requested-With':'XMLHttpRequest',
          #Cookie:Webtrends=111.160.254.58.1464136223732083; BIGipServerpool_sc_122.119.122.51=830109562.20480.0000; JSESSIONID=0000M2suWXhIhzAP_5BzKDkE78S:1a5jgnqlj; OZ_1U_671=vid=v744f3c10a3ea8.0&ctime=1464140663&ltime=1464140662; OZ_1Y_671=erefer=-&eurl=http%3A//sc.travelsky.com/scet/queryAv.do%3Flan%3Dcn%26countrytype%3D0%26travelType%3D0%26cityNameOrg%3D%25E5%258C%2597%25E4%25BA%25AC%26cityCodeOrg%3DPEK%26cityNameDes%3D%25E5%258E%25A6%25E9%2597%25A8%26cityCodeDes%3DXMN%26takeoffDate%3D2016-05-28%26returnDate%3D2016-05-28%26cabinStage%3D0%26adultNum%3D1%26childNum%3D0&etime=1464139715&ctime=1464140663&ltime=1464140662&compid=671
          'Referer':start_url,
          'Origin':'http://b2c.csair.com',
          'Host':'b2c.csair.com',
          'Proxy-Connection':'keep-alive',
          'Upgrade-Insecure-Requests':1,
          'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.155 Safari/537.36'
          }
    date_h = date_h.replace('-','')
    data = '{"depcity":"'+from_h+'", "arrcity":"'+to_h+'", "flightdate":"'+date_h+'", "adultnum":"1", "childnum":"0", "infantnum":"0", "cabinorder":"0","airline":"1", "flytype":"0", "international":"0", "action":"0", "segtype":"1", "cache":"0", "preUrl":"360BUY", "isMember":""}'
    data = urllib.quote(data)
    res_url ='http://b2c.csair.com/B2C40/query/jaxb/direct/query.ao?json='+data
    try:
        re =s.get(start_url,headers = h1,timeout = 15)
        resp1 = s.post(res_url,headers = h2,timeout = 15)
        return resp1.text,resp1.status_code
    except requests.exceptions.ReadTimeout:
        return -1,-1
    except requests.exceptions.ConnectionError:
        return -2,-2
    except AttributeError:
        #print resp.text
        return -1,-1
# 解析数据 
********************************************
********************************************
def data_Crawling(from_h,to_h,date_h,SC):
    s = requests.Session()
    try:
        resp1 = s.get('http://new.hnair.com/hainanair/ibe/deeplink/ancillary.do?DD1='+date_h+'&DD2=&TA=1&TC=0&TI=&ORI='+from_h+'&DES='+to_h+'&SC='+SC+'&ICS=F&PT=F&PT=&FLC=1&NOR=&PACK=T',headers=h1,timeout=15)

        resp2 = s.post('http://new.hnair.com/hainanair/ibe/deeplink/ancillary.do?DD1='+date_h+'&DD2=&TA=1&TC=0&TI=&ORI='+from_h+'&DES='+to_h+'&SC='+SC+'&ICS=F&PT=F&PT=&FLC=1&NOR=&PACK=T&redirected=true',headers=h2,timeout=15)

        resp3 = s.get('http://new.hnair.com/hainanair/ibe/deeplink/entryPointRedirect.do?QUERY=ancillaryIBESearch',headers=h3,timeout=15)

        #resp4 = s.post('http://new.hnair.com/hainanair/ibe/deeplink/entryPointRedirect.do?redirected=true')

        resp5 = s.get('http://new.hnair.com/hainanair/ibe/common/processSearchEntry.do?fromEntryPoint=true',headers=h5,timeout=15)

        resp6 = s.get('http://new.hnair.com/hainanair/ibe/common/spinner.do',headers=h6,timeout=15)

        #respwait = s.get('http://www.hnair.com/qt/dkggpic/wait/')

        resp7 = s.post('http://new.hnair.com/hainanair/ibe/common/processSearch.do',headers=h7,timeout=15)

        resp8 = s.get('http://new.hnair.com/hainanair/ibe/air/processSearch.do',headers=h8,timeout=15)

        resp = s.get('http://new.hnair.com/hainanair/ibe/air/searchResults.do',headers=h9,timeout=15)
        return resp.text,resp.status_code
    except requests.exceptions.ReadTimeout:
        return -1,-1
    except requests.exceptions.ConnectionError:
        return -2,-2
# 解析数据 
********************************************
********************************************
def data_Crawling(from_h,to_h,date_h):
    s = requests.Session()
    try:
        #resp1 = s.post('http://sc.travelsky.com/scet/queryAv.do?lan=cn&countrytype=0&travelType=0&cityNameOrg=%E5%8C%97%E4%BA%AC&cityCodeOrg=PEK&cityNameDes=%E5%8E%A6%E9%97%A8&cityCodeDes=XMN&takeoffDate=2016-05-28&returnDate=2016-05-28&cabinStage=0&adultNum=1&childNum=0',headers=h1,timeout=15)
        url = 'http://sc.travelsky.com/scet/queryAv.do?lan=cn&countrytype=0&travelType=0&cityNameOrg=&cityCodeOrg='+from_h+'&cityNameDes=&cityCodeDes='+to_h+'&takeoffDate='+date_h+'&returnDate=&cabinStage=0&adultNum=1&childNum=0'
        resp1 = s.post(url,headers = h1,timeout=15)
        h2 = {
                'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Encoding':'gzip, deflate',
                'Accept-Language':'zh-CN,zh;q=0.8',
                'Cache-Control':'max-age=0',
                #'Content-Length':234
                'Content-Type':'application/x-www-form-urlencoded',
               # Cookie:Webtrends=111.160.254.58.1464136223732083; BIGipServerpool_sc_122.119.122.51=830109562.20480.0000; JSESSIONID=0000M2suWXhIhzAP_5BzKDkE78S:1a5jgnqlj; OZ_1U_671=vid=v744f3c10a3ea8.0&ctime=1464140688&ltime=1464140663; OZ_1Y_671=erefer=-&eurl=http%3A//sc.travelsky.com/scet/queryAv.do%3Flan%3Dcn%26countrytype%3D0%26travelType%3D0%26cityNameOrg%3D%25E5%258C%2597%25E4%25BA%25AC%26cityCodeOrg%3DPEK%26cityNameDes%3D%25E5%258E%25A6%25E9%2597%25A8%26cityCodeDes%3DXMN%26takeoffDate%3D2016-05-28%26returnDate%3D2016-05-28%26cabinStage%3D0%26adultNum%3D1%26childNum%3D0&etime=1464139715&ctime=1464140688&ltime=1464140663&compid=671
                'Host':'sc.travelsky.com',
                'Origin':'http://sc.travelsky.com',
                'Proxy-Connection':'keep-alive',
                'Referer':url,
                'Upgrade-Insecure-Requests':1,
                'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.155 Safari/537.36'
        }
        id = re.search(r'<input type="hidden" name ="airAvailId" value="(.*?)" />',resp1.text)
        resp= s.post('http://sc.travelsky.com/scet/airAvail.do?airAvailId='+id.group(1)+'&cityCodeOrg='+from_h+'&cityCodeDes='+to_h+'&cityNameOrg=&cityNameDes=&takeoffDate='+date_h+'&returnDate=&travelType=0&countrytype=0&needRT=0&cabinStage=0&adultNum=1&childNum=0',headers = h2,timeout=15)
        return resp.text,resp.status_code
    except requests.exceptions.ReadTimeout:
        return -1,-1
    except requests.exceptions.ConnectionError:
        return -2,-2
# 解析数据 
********************************************
********************************************
def data_Crawling(from_h,to_h,date_h):
    s = requests.Session()
    try:
        #resp1 = s.post('http://sc.travelsky.com/scet/queryAv.do?lan=cn&countrytype=0&travelType=0&cityNameOrg=%E5%8C%97%E4%BA%AC&cityCodeOrg=PEK&cityNameDes=%E5%8E%A6%E9%97%A8&cityCodeDes=XMN&takeoffDate=2016-05-28&returnDate=2016-05-28&cabinStage=0&adultNum=1&childNum=0',headers=h1,timeout=15)
        url = 'http://sc.travelsky.com/scet/queryAv.do?lan=cn&countrytype=0&travelType=0&cityNameOrg=&cityCodeOrg='+from_h+'&cityNameDes=&cityCodeDes='+to_h+'&takeoffDate='+date_h+'&returnDate=&cabinStage=0&adultNum=1&childNum=0'
        resp1 = s.post(url,headers = h1,timeout=15)
        h2 = {
                'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Encoding':'gzip, deflate',
                'Accept-Language':'zh-CN,zh;q=0.8',
                'Cache-Control':'max-age=0',
                #'Content-Length':234
                'Content-Type':'application/x-www-form-urlencoded',
               # Cookie:Webtrends=111.160.254.58.1464136223732083; BIGipServerpool_sc_122.119.122.51=830109562.20480.0000; JSESSIONID=0000M2suWXhIhzAP_5BzKDkE78S:1a5jgnqlj; OZ_1U_671=vid=v744f3c10a3ea8.0&ctime=1464140688&ltime=1464140663; OZ_1Y_671=erefer=-&eurl=http%3A//sc.travelsky.com/scet/queryAv.do%3Flan%3Dcn%26countrytype%3D0%26travelType%3D0%26cityNameOrg%3D%25E5%258C%2597%25E4%25BA%25AC%26cityCodeOrg%3DPEK%26cityNameDes%3D%25E5%258E%25A6%25E9%2597%25A8%26cityCodeDes%3DXMN%26takeoffDate%3D2016-05-28%26returnDate%3D2016-05-28%26cabinStage%3D0%26adultNum%3D1%26childNum%3D0&etime=1464139715&ctime=1464140688&ltime=1464140663&compid=671
                'Host':'sc.travelsky.com',
                'Origin':'http://sc.travelsky.com',
                'Proxy-Connection':'keep-alive',
                'Referer':url,
                'Upgrade-Insecure-Requests':1,
                'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.155 Safari/537.36'
        }
        id = re.search(r'<input type="hidden" name ="airAvailId" value="(.*?)" />',resp1.text)
        time.sleep(2)
        resp= s.post('http://sc.travelsky.com/scet/airAvail.do?airAvailId='+id.group(1)+'&cityCodeOrg='+from_h+'&cityCodeDes='+to_h+'&cityNameOrg=&cityNameDes=&takeoffDate='+date_h+'&returnDate=&travelType=0&countrytype=0&needRT=0&cabinStage=0&adultNum=1&childNum=0',headers = h2,timeout=15)
        return resp.text,resp.status_code
    except requests.exceptions.ReadTimeout:
        return -1,-1
    except requests.exceptions.ConnectionError:
        return -2,-2
    except AttributeError:
        #print resp.text
        return -1,-1
# 解析数据 
********************************************
********************************************
def data_Crawling(from_h,to_h,date_h,i_orgcity, i_dstCity):
    s = requests.Session()
    data = '{"AirlineType":"Single","IsFixedCabin":false,"RouteList":[{"RouteIndex":1,"RouteName":"单    程","OrgCity":"'+from_h+'","DesCity":"'+to_h+'","OrgCityName":"'+i_orgcity+'","DesCityName":"'+i_dstCity+'","FlightDate":"'+date_h+'"}],"AVType":0}'
    #data = urllib.urlencode(data)
    data = urllib.quote(data)
    data = 'http://www.scal.com.cn/Web/ETicket/AirlineList?AirlineParamJSON='+data
    try:
        resp = s.post(data,timeout=15)
        url = re.search(r'arrPageValue.AirlineParamJSON = (.*?);',resp.text)
        #print url.group(1)
        data = json.loads( url.group(1))
        #print(data)
        data1 = re.search(r'arrPageValue.AirlineParamJSON = .*\[(.*?)\].*;',resp.text).group(1).replace("}","")
        #data1 = str(data['RouteList'][0]).replace("}","")
        data11 = data['AirlineType']
        data1 += ',\"AirlineType\":\"'+data11+'\"'
        data11 = data['AVType']
        data1 += ',\"AVType\":'+str(data11)
        data1 += ',\"CardFlag\":null'
        data11 = data['Flag']
        data1 += ',\"Flag\":null'
        data11 = data['BuyerType']
        data1 += ',\"BuyerType\":'+str(data11)
        data11 = data['IsFixedCabin']
        data1 += ',\"IsFixedCabin\":'+str(data11).lower()
        data11 = data['PassKey']
        data1 += ',\"PassKey\":\"'+data11+'\"}'
        data1 = json.loads(data1)
        resp1 = s.post('http://www.scal.com.cn/Web/ETicket/GetSingleChina',json=data1)
        return resp1.text,resp1.status_code
    except requests.exceptions.ReadTimeout:
        return -1,-1
    except requests.exceptions.ConnectionError:
        return -2,-2
    except AttributeError:
        #print resp.text
        return -1,-1
# 解析数据 
********************************************
********************************************
def data_Crawling(from_h,to_h,date_h,i_orgcity, i_dstCity):
    s = requests.Session()
    data = '{"AirlineType":"Single","IsFixedCabin":false,"RouteList":[{"RouteIndex":1,"RouteName":"单    程","OrgCity":"'+from_h+'","DesCity":"'+to_h+'","OrgCityName":"'+i_orgcity+'","DesCityName":"'+i_dstCity+'","FlightDate":"'+date_h+'"}],"AVType":0}'
    #data = urllib.urlencode(data)
    data = urllib.quote(data)
    data = 'http://www.scal.com.cn/Web/ETicket/AirlineList?AirlineParamJSON='+data
    try:
        resp = s.post(data,timeout=15)
        url = re.search(r'arrPageValue.AirlineParamJSON = (.*?);',resp.text)
        #print url.group(1)
        data = json.loads( url.group(1))
        #print(data)
        data1 = re.search(r'arrPageValue.AirlineParamJSON = .*\[(.*?)\].*;',resp.text).group(1).replace("}","")
        #data1 = str(data['RouteList'][0]).replace("}","")
        data11 = data['AirlineType']
        data1 += ',\"AirlineType\":\"'+data11+'\"'
        data11 = data['AVType']
        data1 += ',\"AVType\":'+str(data11)
        data1 += ',\"CardFlag\":null'
        data11 = data['Flag']
        data1 += ',\"Flag\":null'
        data11 = data['BuyerType']
        data1 += ',\"BuyerType\":'+str(data11)
        data11 = data['IsFixedCabin']
        data1 += ',\"IsFixedCabin\":'+str(data11).lower()
        data11 = data['PassKey']
        data1 += ',\"PassKey\":\"'+data11+'\"}'
        data1 = json.loads(data1)
        resp1 = s.post('http://www.scal.com.cn/Web/ETicket/GetSingleChina',json=data1)
        return resp1.text,resp1.status_code
    except requests.exceptions.ReadTimeout:
        return -1,-1
    except requests.exceptions.ConnectionError:
        return -2,-2
    except AttributeError:
        #print resp.text
        return -1,-1
# 解析数据 
********************************************
********************************************
def _set_user_agent(clc):
        if hasattr(clc, 'SetRequestsSession'):
            agent_string = "ClcAnsibleModule/" + __version__
            ses = requests.Session()
            ses.headers.update({"Api-Client": agent_string})
            ses.headers['User-Agent'] += " " + agent_string
            clc.SetRequestsSession(ses) 
********************************************
********************************************
def _set_user_agent(clc):
        if hasattr(clc, 'SetRequestsSession'):
            agent_string = "ClcAnsibleModule/" + __version__
            ses = requests.Session()
            ses.headers.update({"Api-Client": agent_string})
            ses.headers['User-Agent'] += " " + agent_string
            clc.SetRequestsSession(ses) 
********************************************
********************************************
def _set_user_agent(clc):
        if hasattr(clc, 'SetRequestsSession'):
            agent_string = "ClcAnsibleModule/" + __version__
            ses = requests.Session()
            ses.headers.update({"Api-Client": agent_string})
            ses.headers['User-Agent'] += " " + agent_string
            clc.SetRequestsSession(ses) 
********************************************
********************************************
def _set_user_agent(clc):
        if hasattr(clc, 'SetRequestsSession'):
            agent_string = "ClcAnsibleModule/" + __version__
            ses = requests.Session()
            ses.headers.update({"Api-Client": agent_string})
            ses.headers['User-Agent'] += " " + agent_string
            clc.SetRequestsSession(ses) 
********************************************
********************************************
def _set_user_agent(clc):
        if hasattr(clc, 'SetRequestsSession'):
            agent_string = "ClcAnsibleModule/" + __version__
            ses = requests.Session()
            ses.headers.update({"Api-Client": agent_string})
            ses.headers['User-Agent'] += " " + agent_string
            clc.SetRequestsSession(ses) 
********************************************
********************************************
def _set_user_agent(clc):
        if hasattr(clc, 'SetRequestsSession'):
            agent_string = "ClcAnsibleModule/" + __version__
            ses = requests.Session()
            ses.headers.update({"Api-Client": agent_string})
            ses.headers['User-Agent'] += " " + agent_string
            clc.SetRequestsSession(ses) 
********************************************
********************************************
def _set_user_agent(clc):
        if hasattr(clc, 'SetRequestsSession'):
            agent_string = "ClcAnsibleModule/" + __version__
            ses = requests.Session()
            ses.headers.update({"Api-Client": agent_string})
            ses.headers['User-Agent'] += " " + agent_string
            clc.SetRequestsSession(ses) 
********************************************
********************************************
def _set_user_agent(clc):
        if hasattr(clc, 'SetRequestsSession'):
            agent_string = "ClcAnsibleModule/" + __version__
            ses = requests.Session()
            ses.headers.update({"Api-Client": agent_string})
            ses.headers['User-Agent'] += " " + agent_string
            clc.SetRequestsSession(ses) 
********************************************
********************************************
def _set_user_agent(clc):
        if hasattr(clc, 'SetRequestsSession'):
            agent_string = "ClcAnsibleModule/" + __version__
            ses = requests.Session()
            ses.headers.update({"Api-Client": agent_string})
            ses.headers['User-Agent'] += " " + agent_string
            clc.SetRequestsSession(ses) 
********************************************
********************************************
def __init__(
        self,
        proxy=None,
        session=None,
        timeout=80,
        verify_ssl=True,
        **kwargs
    ):
        # type: (str, str, str or dict, _requests.Session, int, bool) -> HTTPClient
        self._proxy = None
        self._session = session
        self._timeout = timeout
        self._verify_ssl = verify_ssl
        self._kwargs = _copy.deepcopy(kwargs)

        if proxy:

            if isinstance(proxy, str):
                proxy = { "http": proxy, "https": proxy }

            if not isinstance(proxy, dict):
                raise ValueError(
                    """
                    The `proxy` parameter must either be `None`, a string
                    representing the URL of a proxy for both HTTP + HTTPS,
                    or a dictionary with a different URL for `"http"` and
                    `"https"`.
                    """
                )

            self._proxy = _copy.deepcopy(proxy)

        # NOTE: This make HTTPClient and any class containing it as an attribute
        # impossible to pickle. Implemented custom pickling to avoid this.
        self._local_thread = _threading.local() 
********************************************
********************************************
def _get_session(self):
        # type: None -> _threading.local
        """
        Return or establish the session associated with the current thread.
        """
        # Make sure the local thread storage has been instantiated
        if getattr(self, "_local_thread", None) is None:
            self._local_thread = _threading.local()

        # Store whatever session we use in the local thread storage
        if getattr(self._local_thread, "session", None) is None:
            self._local_thread.session = self._session or _requests.Session()

        return self._local_thread.session 
********************************************
********************************************
def close(self):
        # type: None -> None
        session = self._get_session()
        if session:
            session.close()
            s = _requests.Session() 
********************************************
********************************************
def __init__(self, path, *args, **kwargs):
        super(Session, self).__init__(*args, **kwargs)
        self._path = path
        self['headers'] = {}
        self['cookies'] = {}
        self['auth'] = {
            'type': None,
            'username': None,
            'password': None
        } 
********************************************
********************************************
def __init__(self, endpoint: str, *args: Any, **kwargs: Any) -> None:
        """
        Args:
            endpoint: The server address.
        """
        super().__init__(*args, **kwargs)
        self.endpoint = endpoint
        # Make use of Requests' sessions feature
        self.session = Session()
        self.session.headers.update(self.DEFAULT_HEADERS) 
********************************************
********************************************
def __init__(self, server, ssl_verify=True, token=None, ignore_system_proxy=False,
                 use_https_proxy=None, ssl_verify_hostname=True, use_http_proxy=None):
        """ Requires:
                server -    URL to the Carbon Black server.  Usually the same as
                            the web GUI.
                ssl_verify - verify server SSL certificate
                token - this is for CLI API interface
        """

        if not server.startswith("http"):
            raise TypeError("Server must be URL: e.g, http://cb.example.com")

        if token is None:
            raise TypeError("Missing required authentication token.")

        self.server = server.rstrip("/")
        self.ssl_verify = ssl_verify
        self.token = token
        self.token_header = {'X-Auth-Token': self.token}
        self.session = requests.Session()

        if not ssl_verify_hostname:
            self.session.mount("https://", HostNameIgnoringAdapter())

        self.proxies = {}
        if ignore_system_proxy:  # see https://github.com/kennethreitz/requests/issues/879
            self.proxies = {
                'no': 'pass'
            }
        else:
            if use_http_proxy:
                self.proxies['http'] = use_http_proxy
            if use_https_proxy:
                self.proxies['https'] = use_https_proxy 
********************************************
********************************************
def __init__(self, api_key):
        self.session = Session()
        self.session.params = {'key': api_key} 
********************************************
********************************************
def __init__(self, account, server ='https://bugzilla.mozilla.org'):
		self.account = account
		self.server = server
		self.session = requests.Session() 
********************************************
********************************************
def post(str):
    session=requests.Session()
    #url="https://www.cnblogs.com/mvc/PostComment/Add.aspx"
    url='https://account.cnblogs.com/signin?returnUrl=https%3A%2F%2Fhome.cnblogs.com%2Fblog%2F'
    headers={
        "user-agent":"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 SE 2.X MetaSr 1.0"
    }
    res=session.get(url,headers=headers)
    token=re.search('name=__RequestVerificationToken type=hidden value=(.*?)>',res.text)[1]
    print(token)
    key=re.search('name=PublicKey value=(.*?)>',res.text)[1]
    print(key)
    data1={
        'LoginName':'zy123451',
        'Password':'zy415320.',
        '__RequestVerificationToken':token,
        'PublicKey':key
    }
    session.post(url,headers=headers,data=data1)

    data={
    'blogApp':"xiaoxiaotank",
    'body':str,
    'parentCommentId':'0',
    'postId':'11078571'
    }
    cookie={
    "cookie":"_ga=GA1.2.158148253.1558313715; __gads=ID=2043acc179f4021b:T=1558313717:S=ALNI_MYgVO4530KXKAD9xjm_j-KTRR2Wlg; _gid=GA1.2.589494181.1562637111; .Cnblogs.AspNetCore.Cookies=CfDJ8D8Q4oM3DPZMgpKI1MnYlrlW4lTc59EvgWOGRw7qH00UJpXPH0E-YVENWsf0ULemiPSzMMhfz04A8ndbfiUBTt5DOAiaTKilh7AgZe0PIw45E_pOEim9YOy2V-kV64U18T-QGkcqWVGCtAGQFC3SgnCtYqbYqsvrJaUCfkkHeKkgEVIbNHWX3mExbVM7XfbUyMHjyr4nPSLAh8cXkhHFM6S_JX8wnoQIIXAV0g3E24YGgTJVPDUtItrhadprWB9vx2APQRCd_JRogM1ZqVOwHaM7AfGRh-TeyGxHX4Dkj07xcLx1KJIykA--lnlul0-1h1ysW8IRSiGER2YJdWjsd_Hqgpl_g9eubSfgRqeeh4-n5k6JGZRyQyCY5ZEEoNln_Pn31irPsrppNsavByLarFbH_RLboie_SGWzoLovriu814kATjX6M39jc_e76keJ3A; .CNBlogsCookie=41BDBE3C536D3FB0AD9CDB403AE27910026F844DD834B6A31EF2154CE046DB784D0ECF43E797C955DA44B8B5707AD0366C80E7092FAA258AD761E874CD52AF80A42620426B53861431CF4C3F165FA033731352F6; _gat=1"
    }
    aaurl = "https://www.cnblogs.com/mvc/PostComment/Add.aspx"
    a=requests.post(aaurl,data=data,cookies=cookie,headers=headers)
    print(a.json()) 
********************************************
********************************************
def test_nonexistent_collection(self):

        logger = logging.getLogger(__name__)
        requests_cache.install_cache(cachefile, backend='sqlite')
        session = requests.Session()

        aic = ArchiveItCollection(2, session=session, logger=logger)

        metadata = aic.return_collection_metadata_dict()

        self.assertFalse(metadata["exists"])
        self.assertEqual(metadata["id"], '2')

        self.assertFalse(aic.does_exist()) 
********************************************
********************************************
def test_private_collection_12(self):

        logger = logging.getLogger(__name__)
        requests_cache.install_cache(cachefile, backend='sqlite')
        session = requests.Session()

        aic = ArchiveItCollection(12, session=session, logger=logger)

        metadata = aic.return_collection_metadata_dict()
        
        self.assertEqual(metadata["name"], "state minnesota sites")
        self.assertEqual(metadata["uri"], "https://archive-it.org/collections/12")
        self.assertEqual(metadata["collected_by"], "Demo Account #1")
        self.assertEqual(metadata["collected_by_uri"], "https://archive-it.org/organizations/3")
        self.assertEqual(metadata["description"], "No description.")

        self.assertEqual(metadata["subject"],
            [
                "historical"
            ]
        )

        self.assertEqual(metadata["archived_since"], "Sep, 2005")

        metadata_fields = aic.list_optional_metadata_fields()

        self.assertTrue("collector" in metadata_fields)

        self.assertEqual(aic.get_optional_metadata("collector"), [ "Demo Account #1" ])

        self.assertTrue(aic.is_private())
        self.assertTrue(aic.does_exist())

        seed_metadata = aic.return_seed_metadata_dict()

        self.assertEqual(seed_metadata["seed_metadata"], {}) 
********************************************
********************************************
def test_public_collection_6820(self):

        logger = logging.getLogger(__name__)
        requests_cache.install_cache(cachefile, backend='sqlite')
        session = requests.Session()

        aic = ArchiveItCollection(6820, session=session, logger=logger)

        metadata = aic.return_collection_metadata_dict()

        self.assertTrue(metadata["exists"])
        self.assertEqual(metadata["id"], '6820')

        self.assertEqual(metadata["name"], "Congressional Budget Office")
        self.assertEqual(metadata["uri"], "https://archive-it.org/collections/6820")
        self.assertEqual(metadata["collected_by"], "Federal Depository Library Program Web Archive")
        self.assertEqual(metadata["collected_by_uri"], "https://archive-it.org/organizations/593")
        self.assertEqual(metadata["description"],
            """Since 1975, CBO has produced independent analyses of budgetary and economic issues to support the Congressional budget process. Each year, the agency’s economists and budget analysts produce dozens of reports and hundreds of cost estimates for proposed legislation. CBO is strictly nonpartisan; conducts objective, impartial analysis; and hires its employees solely on the basis of professional competence without regard to political affiliation."""
            )

        self.assertEqual(metadata["subject"],
            [
                "Government - US Federal",
                "American History & Political Science",
                "Economics & Finance"
            ]
        )
        self.assertEqual(metadata["archived_since"], "Jan, 2016")

        metadata_fields = aic.list_optional_metadata_fields()

        self.assertTrue("language" in metadata_fields)
        self.assertTrue("collector" in metadata_fields)

        self.assertEqual(aic.get_optional_metadata("language"), [ "English" ])
        self.assertEqual(aic.get_optional_metadata("collector"), [ "U.S. Government Publishing Office" ])

        self.assertFalse(aic.is_private())
        self.assertTrue(aic.does_exist()) 
********************************************
********************************************
def __init__(self, collection_id, session=requests.Session(), logger=None):

        self.collection_id = str(collection_id)
        self.session = session
        self.metadata_loaded = False
        self.seed_metadata_loaded = False
        self.metadata = {}
        self.seed_metadata = {}
        self.private = None
        self.exists = None

        self.collection_uri = "{}/{}".format(collection_uri_prefix, collection_id)
        self.firstpage_response = self.session.get(self.collection_uri)        

        self.logger = logger or logging.getLogger(__name__) 
********************************************
********************************************
def __init__(self, token, url, version):
        self.version = version
        self.header = {
            "Authorization": "Bearer {}".format(token)
        }
        self.baseUrl = url
        self.session = requests.Session()
        self.session.headers.update(self.header)
        self.set_user_agent("?") 
********************************************
********************************************
def getcookies(user, passwd):
    # 获取验证码
    sign = random.random()
    url = "https://captcha.weibo.com/api/pattern/get?ver=daf139fb2696a4540b298756bd06266a&source=ssologin&usrname=" + user + "&line=160&side=100&radius=30&_rnd=" + str(
        sign) + "&callback=pl_cb"
    r = requests.get(url)
    imgdata = json.loads(r.text.replace("pl_cb(", '').replace(")", ''))['path_enc']
    id = json.loads(r.text.replace("pl_cb(", '').replace(")", ''))['id']
    recombinePattern(imgdata)
    data_enc = pathdataEncode(path_generate(patterntohash()))
    path_enc = pathEncode(patterntohash(), id)

    url2 = "https://captcha.weibo.com/api/pattern/verify?ver=daf139fb2696a4540b298756bd06266a&id=" + id + "&usrname=" + user + "&source=ssologin&path_enc=" + path_enc + "&data_enc=" + data_enc + "&callback=pl_cb"
    url3 = 'https://passport.weibo.cn/sso/login'
    # 必要的等待时间
    time.sleep(1)
    # 验证验证码
    session = requests.Session()
    r2 = session.get(url2)
    # print r2.headers
    print json.loads(r2.text.replace("pl_cb(", '').replace(")", ''))['msg']
    # print id

    formdata = {'username': user,
                'password': passwd,
                'savestate': '1',
                'ec': '0',
                'entry': 'mweibo',
                'mainpageflag': '1',
                'vid': id,
                'wentry': '',
                'loginfrom': '',
                'client_id': '',
                'code:qq': '',
                'r': '',
                'pagerefer': '',
                'hff': '',
                'hfp': ''}

    # print formdata['vid']
    # 登录
    r3 = session.post(url3, data=formdata, headers=headers3)
    cookies_url = r3.headers['Set-Cookie']
    print json.loads(r3.content)['msg']
    return {k.split('=')[0]: k.split('=')[1] for k in cookies_url.split(';')}

    # r4 = requests.get('https://m.weibo.cn/')
    # print r4.headers['Set-Cookie'] 
********************************************
********************************************
def main(argv):
    try:
        old_config = None
        while True:
            params = {
                'vhosts': {},
            }

            s = requests.Session()
            apps = json.loads(s.get('http://master.mesos:8080/v2/apps').text)
            for app in apps['apps']:
                try:
                    vhost = app['labels']['VIRTUAL_HOST']
                except KeyError:
                    continue
                tasks = json.loads(s.get('http://master.mesos:8080/v2/apps%s/tasks' % app['id'],
                                         headers={'Accept': 'application/json'}).text)
                backends = []
                for task in tasks['tasks']:
                    try:
                        ip = socket.gethostbyname(task['host'])
                    except socket.gaierror:
                        print "Can't look up host %s" % task['host']
                        continue
                    backends.append('%s:%s' % (ip, task['ports'][0]))
                if backends:
                    params['vhosts'][vhost] = {
                        'backends': backends,
                    }

            template = Template(TEMPLATE)
            new_config = template.render(params)
            if new_config != old_config:
                with file('/etc/nginx/sites-available/default', 'w') as fh:
                    fh.write(new_config)
                test = subprocess.Popen(['/usr/sbin/nginx', '-t'], stderr=subprocess.PIPE)
                output = test.communicate()
                if test.returncode != 0:
                    if old_config:
                        print 'Error generating new NGINX configuration, not reloading'
                        return
                    else:
                        raise RuntimeError('Error generating NGINX configuration')
                subprocess.call(['/usr/sbin/nginx', '-s', 'reload'])
                old_config = new_config
            time.sleep(10)
    except KeyboardInterrupt:
        return 1 
********************************************
********************************************
def login(self, username, password):
        """
        # 新浪微博登录
        :param username: 微博手机号
        :param password: 微博密码
        :return:
        """
        if username == "" or password == "":
            raise StockMatchError("用户名或密码不能为空")
        post_data = {
            "entry": "finance",
            "gateway": "1",
            "from": None,
            "savestate": "30",
            "qrcode_flag": True,
            "useticket": "0",
            "pagerefer": "http://jiaoyi.sina.com.cn/jy/index.php",
            "vsnf": "1",
            "su": base64.b64encode(username.encode("utf-8")).decode("utf-8"),
            "service": "sso",
            "servertime": get_unix_timestamp(False),
            "nonce": "RA12UM",
            # "pwencode": "rsa2",  # 取消掉使用rsa2加密密码
            "sp": password,
            "sr": "1280*800",
            "encoding": "UTF-8",
            "cdult": "3",
            "domain": "sina.com.cn",
            "prelt": "56",
            "returntype": "TEXT",
        }
        session = requests.Session()
        session.headers.update(conf.SINA_CONFIG["request_headers"])
        res = session.post(conf.SINA_CONFIG["login_url"], data=post_data, params={
            "client": "ssologin.js(v1.4.19)",
            "_": get_unix_timestamp(),
        })
        res.encoding = "gb2312"
        info = json.loads(res.content)
        if info["retcode"] != "0":
            logger.error(info["reason"])
            raise LoginFailedError(info["reason"])
        logger.info("用户%s登录成功" % username)
        return session, info['uid'] 
********************************************
********************************************
def __init__(self,
                 issuer=None,
                 provider_metadata=None,
                 userinfo_http_method='GET',
                 client_registration_info=None,
                 client_metadata=None,
                 auth_request_params=None,
                 session_refresh_interval_seconds=None,
                 requests_session=None):
        """
        Args:
            issuer (str): OP Issuer Identifier. If this is specified discovery will be used to fetch the provider
                metadata, otherwise `provider_metadata` must be specified.
            provider_metadata (ProviderMetadata): OP metadata,
            userinfo_http_method (Optional[str]): HTTP method (GET or POST) to use when sending the UserInfo Request.
                If `none` is specified, no userinfo request will be sent.
            client_registration_info (ClientRegistrationInfo): Client metadata to register your app
                dynamically with the provider. Either this or `registered_client_metadata` must be specified.
            client_metadata (ClientMetadata): Client metadata if your app is statically
                registered with the provider. Either this or `client_registration_info` must be specified.
            auth_request_params (dict): Extra parameters that should be included in the authentication request.
            session_refresh_interval_seconds (int): Length of interval (in seconds) between attempted user data
                refreshes.
            requests_session (requests.Session): custom requests object to allow for example retry handling, etc.
        """

        if not issuer and not provider_metadata:
            raise ValueError("Specify either 'issuer' or 'provider_metadata'.")

        if not client_registration_info and not client_metadata:
            raise ValueError("Specify either 'client_registration_info' or 'client_metadata'.")

        self._issuer = issuer
        self._provider_metadata = provider_metadata

        self._client_registration_info = client_registration_info
        self._client_metadata = client_metadata

        self.userinfo_endpoint_method = userinfo_http_method
        self.auth_request_params = auth_request_params or {}
        self.session_refresh_interval_seconds = session_refresh_interval_seconds

        self.requests_session = requests_session or requests.Session() 
********************************************
********************************************
def send_request(url, method='GET', headers=None, param_get=None, data=None):
    """实际发送请求到目标服务器, 对于重定向, 原样返回给用户
    被request_remote_site_and_parse()调用"""
    final_hostname = urlsplit(url).netloc
    dbgprint('FinalRequestUrl', url, 'FinalHostname', final_hostname)
    # Only external in-zone domains are allowed (SSRF check layer 2)
    if final_hostname not in allowed_domains_set and not developer_temporary_disable_ssrf_prevention:
        raise ConnectionAbortedError('Trying to access an OUT-OF-ZONE domain(SSRF Layer 2):', final_hostname)

    # set zero data to None instead of b''
    if not data:
        data = None

    prepped_req = requests.Request(
        method,
        url,
        headers=headers,
        params=param_get,
        data=data,
    ).prepare()

    # get session
    if enable_connection_keep_alive:
        _session = connection_pool.get_session(final_hostname)
    else:
        _session = requests.Session()

    # Send real requests
    parse.time["req_start_time"] = time()
    r = _session.send(
        prepped_req,
        proxies=requests_proxies,
        allow_redirects=False,
        stream=enable_stream_content_transfer,
        verify=not developer_do_not_verify_ssl,
    )
    # remote request time
    parse.time["req_time_header"] = time() - parse.time["req_start_time"]
    dbgprint('RequestTime:', parse.time["req_time_header"], v=4)

    # Some debug output
    # print(r.request.headers, r.headers)
    if verbose_level >= 3:
        dbgprint(r.request.method, "FinalSentToRemoteRequestUrl:", r.url, "\nRem Resp Stat: ", r.status_code)
        dbgprint("RemoteRequestHeaders: ", r.request.headers)
        if data:
            dbgprint('RemoteRequestRawData: ', r.request.body)
        dbgprint("RemoteResponseHeaders: ", r.headers)

    return r 
********************************************
********************************************
def data_Crawling(from_h,to_h,date_h):
    #创建回话，用于重定向，用于三次请求在一个会话中
    s = requests.Session()
    #超时异常处理，若是超时将等待6分钟
    try:
        #第一次请求
        #response0 = s.get("http://data.cn.coremetrics.com/cm?ci=90409626&st=1466388314288&vn1=4.15.18&ec=utf-8&pi=new-step1_flightChoose&ul=http%3A%2F%2Fwww.shenzhenair.com&cjen=1&cjuid=57398592835414660835879&cjsid=1466388133&cjvf=1&tid=8&ti=1466388338458&hr=%2Fservlet%2FSearchEngineServlet%3Fpc%3D"+from_h+"%26pd%3D"+to_h+"%26pe%3D"+date_h+"%26oriPage%3Duiue",headers=headers)
        response = s.get("http://www.shenzhenair.com/servlet/SearchEngineServlet?pc="+from_h+"&pd="+to_h+"&pe="+date_h+"&oriPage=uiue",headers=headers1,timeout=15)

        #第二次请求
        url = 'http://www.shenzhenair.com/uiue/flightSearch.do?waf_search_token=szair&operate=flightSearch&originalPage=loading'
        data = {'waf_search_token': 'szair',
                'originalPage': 'loading',
                'operate': 'flightSearch'}
        response1 = s.get('http://www.shenzhenair.com/uiue/flightSearch.do?operate=toLoading&originalPage=login'
                        ,headers=headers1,timeout=15)
        #print response1.status_code
        time.sleep(1)
        #response3 = s.get( 'http://data.cn.coremetrics.com/cm?ci=90409626&st=1463492988375&vn1=4.15.18&ec=utf-8&vn2=e4.0&pi=new-bookFlightLoading&ul=http%3A%2F%2Fwww.shenzhenair.com%2Fuiue%2FflightSearch.do%3Foperate%3DtoLoading%26originalPage%3Dlogin&cjen=1&cjuid=84891504478414626697084&cjsid=1463488973&cjvf=1&tid=1&cg=Flights&rnd=1463499547693',headers=headers,timeout=30)
        #time.sleep(3)
        #第三次请求
        data = re.search(r'\'cookie\' : \"(.*?)\",',response1.text)
        #print data.group(1)
        #print s.cookies
        headers4 = {
        'Accept': 'text/html, application/xhtml+xml, */*',
        'Referer': 'http://www.shenzhenair.com/uiue/flightSearch.do?operate=toLoading&originalPage=login',
        'Accept-Language': 'zh-CN',
        'User-Agent': 'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',
        'Cookie':'90409626_clogin=l=1466669827&v=1&e=1466671633403; RadwareP=BQQAeDLICwr6TbMa8P3sBQ$$; verynginx_sign_cookie=21edd0522f8b10155285c7a9f3e98a51; userid_hash=CgtvTFdrPOxfU3YZBTeCAg==; route=ab8f2b4b0aac13fa8231e67aa418e142; JSESSIONID=00006SQbEuUUW_3KCzGcW5sjluw:-1; sessionid_hash=b8b55026eab01d0fa676d7d9109c6646; cmTPSet=Y; verynginx_sign_javascript='+data.group(1),
        'Content-Type': 'application/x-www-form-urlencoded',
        'Accept-Encoding': 'gzip, deflate',
        'Host': 'www.shenzhenair.com',
        #'Content-Length': 64,
        'Connection': 'Keep-Alive',
        'Pragma': 'no-cache'
        }
        response2 = s.post(url,headers=headers4,timeout=15)
        
        response2 = s.post(url,headers=headers4,timeout=15)
        #response2 = s.post(url,timeout=15)
        #print response2.status_code
        return response2.text,response2.cookies
    except requests.exceptions.ReadTimeout:
        return -1,-1
    except requests.exceptions.ConnectionError:
        return -2,-2

# 解析数据 
********************************************
********************************************
def send_to_kannel( msg = {}, preferred_kannel_server = None):
    '''sends a given messages to the _RIGHT_ kannel server'''
    server = None
    if preferred_kannel_server is not None:
        try:
            server = KANNEL_SERVERS[preferred_kannel_server.lower()] #locate using ip
        except KeyError as ke:
            server = KANNEL_SERVERS['DEFAULT_KANNEL_SERVER']
    else: #no preferred server was given, select the proper one based on the recipient number
    
        for s,s_info in KANNEL_SERVERS.items():
            prefixes = s_info['series']
            logger.debug("Server %s supports all numbers with the prefixes %s", s, prefixes)
            for p in prefixes:
                recipient = msg['to'].strip('+')
                logger.debug("Trying to match %s with prefix %s ", recipient, p)
                if recipient.startswith(p): #this is our number series
                    server = s_info 
                    logger.debug("Selected server %s with prefix (%s) matching with recipient number %s", server, prefixes,recipient)
                    break;

            if server is not None: #we have found our server!
                break;

    if server is None:
        logger.error("Could not select any server for forwarding message! Check logs.")
        return (False, 500, '')

    try:
        #compose the complete Request with URL and data for sending sms
        session = Session()

        request = session.prepare_request(compose_request_for_kannel(msg, server))
        logger.debug("Calling %s with data %s", request.url, request.body)
        response = session.send(request)

        print response.status_code
        print response.text
        logger.debug("Received response code %s with text %s", response.status_code, response.text)
        logger.debug("Result is %s %s ", response.status_code, response.text)
        exc_info = sys.exc_info()

        return (True, response.status_code, response.text)
    except requests.ConnectionError as ce:
        exc_info = sys.exc_info()
        logger.critical("Problem while connecting to the server!")
        logger.exception(ce)

        return (False, response.status_code, response.text)
    finally:
        exc_info = sys.exc_info()
        traceback.print_exception(*exc_info)
        del exc_info 
********************************************
********************************************
def get_response(session_name, requests_kwargs, config_dir, args,
                 read_only=False):
    """Like `client.get_response`, but applies permanent
    aspects of the session to the request.

    """
    if os.path.sep in session_name:
        path = os.path.expanduser(session_name)
    else:
        hostname = (
            requests_kwargs['headers'].get('Host', None)
            or urlsplit(requests_kwargs['url']).netloc.split('@')[-1]
        )

        assert re.match('^[a-zA-Z0-9_.:-]+$', hostname)

        # host:port => host_port
        hostname = hostname.replace(':', '_')
        path = os.path.join(config_dir,
                            SESSIONS_DIR_NAME,
                            hostname,
                            session_name + '.json')

    session = Session(path)
    session.load()

    request_headers = requests_kwargs.get('headers', {})
    requests_kwargs['headers'] = dict(session.headers, **request_headers)
    session.update_headers(request_headers)

    if args.auth:
        session.auth = {
            'type': args.auth_type,
            'username': args.auth.key,
            'password': args.auth.value,
        }
    elif session.auth:
        requests_kwargs['auth'] = session.auth

    requests_session = requests.Session()
    requests_session.cookies = session.cookies

    try:
        response = requests_session.request(**requests_kwargs)
    except Exception:
        raise
    else:
        # Existing sessions with `read_only=True` don't get updated.
        if session.is_new or not read_only:
            session.cookies = requests_session.cookies
            session.save()
        return response 
********************************************
********************************************
def emomeLogin(username, password):
    url = "http://websms1.emome.net/sms/sendsms/new.jsp?msg="
    authUrl = "https://member.cht.com.tw/HiReg/multiauthentication"
    confirmUrl = "https://member.cht.com.tw/HiReg/redirect?m=logininfo"

    s = requests.Session()
    r = s.get(url)

    if r.history:
        content = r.text
        match = re.search(".*checksum.*value=\"(?P<checksum>[a-f0-9]+)\"/>", content)
        if match:
            checksum = match.group("checksum")
            postfield = {
                          "version": "1.0",
                          "curl": "http://auth.emome.net/emome/membersvc/AuthServlet",
                          "siteid": "76",
                          "sessionid": "",
                          "channelurl": "http://auth.emome.net/emome/",
                          "others": "5235",
                          "checksum": checksum,
                          "cp_reg_info": "",
                          "reg_url": "",
                          "service_type": "",
                          "finish_channelurl": "",
                          "formtype": "",
                          "sso": "yes",
                          "uid": username,
                          "pw": password
                        }
            r = s.post(authUrl, data = postfield)
            r = s.get(confirmUrl)
            r = s.get(url)
            if r.text.find(u"訊息內容") > 0:
                print "Login Successfully! We can send SMS now"
                return s
            else:
                print "Unable to retrieve Emome WebSMS interface"
                return False
        else:
            print "Something wrong with Emome authentication!"
            return False
    else:
        print "Unable to open Emome website"
        return False 
********************************************
********************************************
def download_live(target_room_id):
    if not os.path.exists(os.path.join(ptts.dl_path, ptts.tt_target_user, 'broadcasts')):
        os.makedirs(os.path.join(ptts.dl_path, ptts.tt_target_user, 'broadcasts'))

    download_path = os.path.join(ptts.dl_path, ptts.tt_target_user, 'broadcasts')
    logger.separator()
    logger.info("Checking for ongoing livestreams.")
    logger.separator()
    s = requests.Session()
    s.headers.update({
        'User-Agent': 'Mozilla/5.0 (X11; Linux i686; rv:60.0) Gecko/20100101 Firefox/60.0',
    })
    r = s.get(Constants.LIVE_WEB_URL.format(target_room_id))
    r.raise_for_status()
    live = r.text
    live_room_id = re.search(r'(stream-)(.*)(?=\/playlist)', live)
    if live_room_id:
        live_hls_url = Constants.LIVE_HLS_ENDP.format(live_room_id[2])
        logger.info("HLS url: {:s}".format(live_hls_url))
        logger.separator()
        logger.info("HLS url retrieved. Calling youtube-dl.")
        helpers.call_ytdl(live_hls_url, os.path.join(download_path, str(live_room_id[2]) + "_" + ptts.epochtime))
    else:
        logger.info("There is no available livestream for this user.")
        logger.separator()




    ### NEEDS LOGIN BUT IS BROKEN ###

    # if not os.path.exists(os.path.join(ptts.dl_path, ptts.tt_target_user, 'broadcasts')):
    #     os.makedirs(os.path.join(ptts.dl_path, ptts.tt_target_user, 'broadcasts'))
    #
    # download_path = os.path.join(ptts.dl_path, ptts.tt_target_user, 'broadcasts')
    # logger.separator()
    # logger.info("Checking for ongoing livestreams.")
    # logger.separator()
    # json_data = api.user_post_feed(user_id=target_user_id, max_cursor=0)
    # if  json_data.get("aweme_list"):
    #     live_room_id = json_data.get("aweme_list")[0].get('author', None).get('room_id', None)
    #     if live_room_id:
    #         logger.info("Livestream available, getting information and beginning download.")
    #         logger.separator()
    #         live_json = api.get_live_feed(live_room_id)
    #         live_hls_url = Constants.LIVE_HLS_ENDP.format(live_json.get('room').get('stream_url').get('sid'))
    #         logger.info("HLS url retrieved. Calling youtube-dl.")
    #         helpers.call_ytdl(live_hls_url, os.path.join(download_path, str(live_room_id) + "_" + ptts.epochtime))
    #     else:
    #         logger.info("There is no available livestream for this user.")
    #         logger.separator()
    # else:
    #     logger.info("There is no available livestream for this user.")
    #     logger.separator() 
********************************************
********************************************
def _send_raw_http_request(self, method, url, data=None):
        self.__logger.debug('%s %s' % (method, url))
        if method in ['POST', 'PUT', 'PATCH']:
            self.__logger.log(TINTRI_LOG_LEVEL_DATA, 'Data: %s' % data)

        headers = {'content-type': 'application/json'}
        if self.__session_id:
            headers['cookie'] = 'JSESSIONID=%s' % self.__session_id

        if method in ['GET', 'POST', 'PUT', 'PATCH', 'DELETE']:
            if method == 'GET': httpresp = requests.get(url, headers=headers, verify=False)
            elif method == 'POST': httpresp = requests.post(url, data, headers=headers, verify=False)
            elif method == 'PUT': httpresp = requests.put(url, data, headers=headers, verify=False)
            elif method == 'PATCH': httpresp = requests.patch(url, data, headers=headers, verify=False)
            elif method == 'DELETE': httpresp = requests.delete(url, headers=headers, verify=False)
            self._httpresp = httpresp # self._httpresp is for debugging only, not thread-safe
            return httpresp
        else:
            raise TintriError(None, message='Invalid HTTP method: ' + method) # This should never happen 
********************************************
********************************************
def put_well_data(container_id, well_index, data_obj, headers=None, org_name=None,container_json=None):
    """Update a well with new data"""
    
    initialize_config()
        
    headers = headers if headers else TSC_HEADERS
    org_name = org_name if org_name else ORG_NAME    
    
    def _well_url(container_id, well_index):
        return 'https://secure.transcriptic.com/{}/inventory/samples/{}/{}'.format(org_name, container_id, well_index)

    headers['content-type'] = 'application/json'
   
    response = requests.put(_well_url(container_id, well_index), headers=headers,
                            data=json.dumps(data_obj),
                            verify=False
                            )
    
    response.raise_for_status() 
********************************************
********************************************
def notify_party_and_respondent_account_locked(respondent_id, email_address, status=None):
    logger.info('Notifying respondent and party service that account is locked')
    url = f'{app.config["PARTY_URL"]}/party-api/v1/respondents/edit-account-status/{respondent_id}'

    data = {
        'respondent_id': respondent_id,
        'email_address': email_address,
        'status_change': status
    }

    response = requests.put(url, json=data, auth=app.config['PARTY_AUTH'])

    try:
        response.raise_for_status()
    except requests.exceptions.HTTPError:
        logger.error('Failed to notify party', respondent_id=respondent_id, status=status)
        raise ApiError(logger, response)

    logger.info('Successfully notified respondent and party service that account is locked', respondent_id=respondent_id, status=status) 
********************************************
********************************************
def changeAlbumName(self,gid,name,albumId):
        header = {
            "Content-Type" : "application/json",
            "X-Line-Mid" : self.mid,
            "x-lct": self.channel_access_token,

        }
        payload = {
            "title": name
        }
        r = requests.put(
            "http://" + self.host + "/mh/album/v3/album/" + albumId + "?homeId=" + gid,
            headers = header,
            data = json.dumps(payload),
        )
        return r.json() 
********************************************
********************************************
def _publish(etag):
  """Publish local template to Firebase server.

  Args:
    etag: ETag for safe (avoid race conditions) template updates.
        * can be used to force template replacement.
  """
  with open('config.json', 'r', encoding='utf-8') as f:
    content = f.read()
  headers = {
    'Authorization': 'Bearer ' + _get_access_token(),
    'Content-Type': 'application/json; UTF-8',
    'If-Match': etag
  }
  resp = requests.put(REMOTE_CONFIG_URL, data=content.encode('utf-8'), headers=headers)
  if resp.status_code == 200:
    print('Template has been published.')
    print('ETag from server: {}'.format(resp.headers['ETag']))
  else:
    print('Unable to publish template.')
    print(resp.text) 
********************************************
********************************************
def changeAlbumName(self,gid,name,albumId):
        header = {
            "Content-Type" : "application/json",
            "X-Line-Mid" : self.mid,
            "x-lct": self.channel_access_token,

        }
        payload = {
            "title": name
        }
        r = requests.put(
            "http://" + self.host + "/mh/album/v3/album/" + albumId + "?homeId=" + gid,
            headers = header,
            data = json.dumps(payload),
        )
        return r.json() 
********************************************
********************************************
def run(self):
        try:
            self.sse = ClosableSSEClient(self.url)
            for msg in self.sse:
                event = msg.event
                if event is not None and event in ('put', 'patch'):
                    response = json.loads(msg.data)
                    if response is not None:
                        # Default to CHILD_CHANGED event
                        occurred_event = FirebaseEvents.CHILD_CHANGED
                        if response['data'] is None:
                            occurred_event = FirebaseEvents.CHILD_DELETED

                        # Get the event I'm trying to listen to
                        ev = FirebaseEvents.id(self.event_name)
                        if occurred_event == ev or ev == FirebaseEvents.CHILD_CHANGED:
                            self.callback(event, response)
        except socket.error:
            pass 
********************************************
********************************************
def keepalive_listen_key(self, listen_key):
        """
        Ping a listenkey to keep it alive

        :param listen_key: the listenkey you want to keepalive
        :type listen_key: str

        :return: the response
        :rtype: str or False
        """
        logging.debug("BinanceWebSocketApiRestclient->keepalive_listen_key(" + str(listen_key) + ")")
        method = "put"
        try:
            return self._request(method, self.path_userdata, False, {'listenKey': str(listen_key)})
        except KeyError:
            return False
        except TypeError:
            return False 
********************************************
********************************************
def on_teams_file_consent_accept(
            self,
            turn_context: TurnContext,
            file_consent_card_response: FileConsentCardResponse
    ):
        """
        The user accepted the file upload request.  Do the actual upload now.
        """

        file_path = "files/" + file_consent_card_response.context["filename"]
        file_size = os.path.getsize(file_path)

        headers = {
            "Content-Length": f"\"{file_size}\"",
            "Content-Range": f"bytes 0-{file_size-1}/{file_size}"
        }
        response = requests.put(
            file_consent_card_response.upload_info.upload_url, open(file_path, "rb"), headers=headers
        )

        if response.status_code != 200:
            await self._file_upload_failed(turn_context, "Unable to upload file.")
        else:
            await self._file_upload_complete(turn_context, file_consent_card_response) 
********************************************
********************************************
def send_response(event, context, status, reason, data):
    import requests

    body = json.dumps(
        {
            'Status': status,
            'RequestId': event['RequestId'],
            'StackId': event['StackId'],
            'PhysicalResourceId': context.log_stream_name,
            'Reason': reason,
            'LogicalResourceId': event['LogicalResourceId'],
            'Data': data
        }
    )

    headers = {
        'content-type': '',
        'content-length': len(body)
    }

    r = requests.put(event['ResponseURL'], data=body, headers=headers) 
********************************************
********************************************
def setupDomain(domain, folder=False):
    endpoint = config.get("hsds_endpoint")
    headers = getRequestHeaders(domain=domain)
    req = endpoint + "/"
    rsp = requests.get(req, headers=headers)
    if rsp.status_code == 200:
        return  # already have domain
    if rsp.status_code != 404:
        # something other than "not found"
        raise ValueError(f"Unexpected get domain error: {rsp.status_code}")
    parent_domain = getParentDomain(domain)
    if parent_domain is None:
        raise ValueError(f"Invalid parent domain: {domain}")
    # create parent domain if needed
    setupDomain(parent_domain, folder=True)

    headers = getRequestHeaders(domain=domain)
    body=None
    if folder:
        body = {"folder": True}
        rsp = requests.put(req, data=json.dumps(body), headers=headers)
    else:
        rsp = requests.put(req, headers=headers)
    if rsp.status_code != 201:
        raise ValueError(f"Unexpected put domain error: {rsp.status_code}") 
********************************************
********************************************
def testPutInvalid(self):
        print("testPutInvalid", self.base_domain)
        headers = helper.getRequestHeaders(domain=self.base_domain)
        req = self.endpoint + '/'

        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        root_id = rspJson["root"]

        # try creating an attribute with an invalid type
        attr_name = "attr1"
        attr_payload = {'type': 'H5T_FOOBAR', 'value': 42}
        req = self.endpoint + "/groups/" + root_id + "/attributes/" + attr_name
        rsp = requests.put(req, data=json.dumps(attr_payload), headers=headers)
        self.assertEqual(rsp.status_code, 400)  # invalid request 
********************************************
********************************************
def setupDomain(domain):
    endpoint = config.get("hsds_endpoint")
    print("setupdomain: ", domain)
    headers = getRequestHeaders(domain=domain)
    req = endpoint + "/"
    rsp = requests.get(req, headers=headers)
    if rsp.status_code == 200:
        return  # already have domain
    if rsp.status_code != 404:
        # something other than "not found"
        raise ValueError("Unexpected get domain error: {}".format(rsp.status_code))

    parent_domain = getParentDomain(domain)
    if parent_domain is None:
        raise ValueError("Invalid parent domain: {}".format(domain))
    # create parent domain if needed
    setupDomain(parent_domain)  
     
    headers = getRequestHeaders(domain=domain)
    rsp = requests.put(req, headers=headers)
    if rsp.status_code != 201:
        raise ValueError("Unexpected put domain error: {}".format(rsp.status_code)) 
********************************************
********************************************
def api_call_put(self, uri, data='{}'):
        headers = {'X-Auth-Email': self.EMAIL, 'X-Auth-Key': self.TOKEN, 'Content-Type': 'application/json'}
        try:
            r = requests.put(cf_api_url + uri, data=json.dumps(data), headers=headers)
        except (requests.ConnectionError,
                requests.RequestException,
                requests.HTTPError,
                requests.Timeout,
                requests.TooManyRedirects) as e:
            raise self.CONNError(str(e))
        try:
            api_result = json.loads(r.text)
        except ValueError:
            raise self.APIError('JSON parse failed.')
        if api_result['result'] == 'error':
            raise self.APIError(api_result['msg'])
        elif api_result['errors']:
            raise self.APIError(str(api_result['errors']))
        return api_result

    ################################################################
    #  Zone (https://api.cloudflare.com/#zone)                     #
    ################################################################

    #  Get all zones 
********************************************
********************************************
def set_settings(self, settings):
        h=headers
        h.update({
          'Authorization'   : 'OAuth="'+ self.oauth + '"',
          'Content-Length'  :  '1089', #@TODO figure out length calculation
          'Content-Type'    : 'application/json'})

        # Happn preferences
        url = 'https://api.happn.fr/api/users/' + self.id
        try:
            r = requests.put(url, headers=h, data = json.dumps(settings))
        except Exception as e:
            raise HTTP_MethodError('Error Setting Settings: {}'.format(e))

        if r.status_code == 200: #200 = 'OK'
            logging.debug('Updated Settings')
        else:
            # Unable to fetch distance
            raise HTTP_MethodError(httpErrors[r.status_code]) 
********************************************
********************************************
def update_activity(self):
        """ Updates User activity """

        # Create and send HTTP PUT to Happn server
        h = headers
        h.update({
            'Authorization' : 'OAuth="'+ self.oauth + '"',
            'Content-Type'  : 'application/x-www-form-urlencoded; charset=UTF-8',
            'Content-Length': '20'
        })
        payload = {
            'update_activity' :  'true'
        }
        url = 'https://api.happn.fr/api/users/'+self.id
        try:
            r = requests.put(url, headers=h, data = payload)
        except Exception as e:
            raise HTTP_MethodError('Error Connecting to Happn Server: {}'.format(e))

        if r.status_code == 200: #200 = 'OK'
            logging.debug('Updated User activity')
        else:
            # Unable to fetch distance
            raise HTTP_MethodError(httpErrors[r.status_code]) 
********************************************
********************************************
def _try_upload(url, chunk, heads):
    response = None
    for n in range(0, 11):
        if n > 0:
            print("Upload re-try #{}...".format(n))
        try:
            print("Uploading...".format(n))
            response = requests.put(url, data=chunk, headers=heads)
            print("Done.")
            if response.status_code not in [requests.codes.ok, requests.codes.accepted, requests.codes.created]:
                print("Response status: {} ({}).".format(response.status_code, response.reason))
                raise IOError
            return response
        except IOError as e:
            print("Exception: " + str(e))
            time.sleep(2 ** n)
        finally:
            if response is not None:
                response.close()
    raise IOError("Filed to upload file") 
********************************************
********************************************
def put(self, *args, **kwargs):
        kwargs['method'] = 'put'
        return self.do(*args, **kwargs) 
********************************************
********************************************
def api_submit(request, user=None, password=None):

    # Fetch the list of deploys for the application
    # This becomes the api call
    api_url = (api_protocol
               + '://'
               + api_host
               + request)

    if user:
        logging.info('Submitting data to API: %s' % api_url)
        r = requests.put(api_url, verify=verify_ssl, auth=HTTPBasicAuth(user, password))
    else:
        logging.info('Requesting data from API: %s' % api_url)
        r = requests.get(api_url, verify=verify_ssl)

    if r.status_code == requests.codes.ok:

        logging.debug('Response data: %s' % r.json())
        return r.json()

    elif r.status_code == requests.codes.conflict:

        logging.info('Artifact location/revision combination '
                     'is not unique. Nothing to do.')

        logging.info('twoni-plete')
        print ""
        sys.exit(0)

    else:

        logging.error('There was an error querying the API: '
                      'http_status_code=%s,reason=%s,request=%s'
                      % (r.status_code, r.reason, api_url))
        logging.info('twoni-plete')
        print ""
        sys.exit(2) 
********************************************
********************************************
def change_password(password, token):
    logger.info('Attempting to change password through the party service')

    data = {'new_password': password}
    url = f"{app.config['PARTY_URL']}/party-api/v1/respondents/change_password/{token}"
    response = requests.put(url, auth=app.config['PARTY_AUTH'], json=data)

    try:
        response.raise_for_status()
    except requests.exceptions.HTTPError:
        logger.error('Failed to send change password request to party service', token=token)
        raise ApiError(logger, response)

    logger.info('Successfully changed password through the party service') 
********************************************
********************************************
def verify_email(token):
    logger.info('Attempting to verify email address', token=token)

    url = f"{app.config['PARTY_URL']}/party-api/v1/emailverification/{token}"
    response = requests.put(url, auth=app.config['PARTY_AUTH'])

    try:
        response.raise_for_status()
    except requests.exceptions.HTTPError:
        logger.error('Failed to verify email', token=token)
        raise ApiError(logger, response)

    logger.info('Successfully verified email address', token=token) 
********************************************
********************************************
def set(self, payload):
        return requests.put(self.current_url, json=payload) 
********************************************
********************************************
def _put_request(
        request_url: str,
        requests_data: typing.Dict[str, typing.Any],
        request_header: typing.Dict[str, str],
    ) -> requests.Response:
        return requests.put(request_url, json=requests_data, headers=request_header) 
********************************************
********************************************
def __on_or_off(self, operation):
		url = 'http://'+self.IP+'/api/'+self.username+'/lights/'+self.ID+'/state'
		if operation == 'on':
			payload = '{"on": true}'
		else:
			payload = '{"on": false}'
		r = requests.put(url, data=payload)
		if r.status_code == 200:
			HueBridge.log.success(self.name + ' ' + operation)
		else:
			HueBridge.log.err(self.name + ' ' + operation)
		return r.json() 
********************************************
********************************************
def __init__(self, config):
        if not have_requests:
            raise ImportError('requests module required for RequestsTransport')
        TransportBase.__init__(self, config, self.__module__)
        self.REQ_MAP = {
            'GET': requests.get,
            'POST': requests.post,
            'DELETE': requests.delete,
            'PUT': requests.put
        }
        self._timeout = self._config.get('timeout', None)
        if isinstance(self._timeout, list) and len(self._timeout) == 2:
            self._timeout = tuple(self._timeout) 
********************************************
********************************************
def update(self, api_obj, data, obj_id=0, url_params=''):
        if not data:
            raise TypeError("Missing object data.")
        if url_params:
            url_params = '?' + url_params.lstrip("?")
        if not obj_id:
            obj_id = data['id']
        url = self.server + '/' + api_obj + '/' + str(obj_id) + url_params
        r = requests.put(url, data=json.dumps(data), headers=self.tokenHeaderJson, verify=self.sslVerify)
        return self.__check_result(r)


    # Delete object using HTTP DELETE request. 
********************************************
********************************************
def rest_api_put(self, rest_url, body, api_version):
        """
        PUT request to the REST API - Not tested
        :return: JSON string of the PUT response
        """
        response = requests.put(
            "{org_instance}/services/data/v{api_version}/{rest_url}".format(
                org_instance=self.instance, api_version=api_version, rest_url=rest_url
            ),
            data=body,
            headers=self.sf_headers
        )

        return response 
********************************************
********************************************
def _put(self, endpoint, params=None, data=None):
        response = requests.put(self.BASE_URL + endpoint, params=params, json=data, auth=(self.user, self.password))
        return self._parse(response) 
********************************************
********************************************
def put(url):
    url = url.strip('/')
    text = random.randint(100000000, 200000000)
    payload = '/{}.txt'.format(text)
    url = url + payload
    data = {'{}'.format(text): '{}'.format(text)}
    r = requests.put(url, data=data, allow_redirects=False, verify=False, headers=get_ua())
    if r.status_code == 201:
        return 'HTTP METHOD PUT url: {}'.format(url) 
********************************************
********************************************
def check(url, ip, ports, apps):
    result = ''
    try:
        probe = get_list(ip, ports)
        for url in probe:
            result = put(url)
    except Exception as e:
        pass
    if result:
        return result 
********************************************
********************************************
def clean_consul(port, token=''):
    # remove all data from the instance, to have a clean start
    base_uri = 'http://127.0.0.1:%s/v1/' % port
    params = {'recurse': 1}
    if token:
        params['token'] = token
    requests.delete(base_uri + 'kv/', params=params)
    services = requests.get(base_uri + 'agent/services',
                            params=params).json().keys()
    for s in services:
        requests.put(base_uri + 'agent/service/deregister/%s' % s)

    if token:
        acl_tokens = requests.get(base_uri + 'acl/list', params=params).json()
        for t in acl_tokens:
            if t['ID'] != token:
                requests.put(base_uri + 'acl/destroy/%s' % t['ID'],
                             params=params)

        acl_policys = requests.get(base_uri + 'acl/policies',
                                   params=params).json()
        for pls in acl_policys:
            if pls['ID'] != token:
                requests.delete(base_uri + 'acl/policy/%s' % pls['ID'],
                                params=params)

        acl_roles = requests.get(base_uri + 'acl/roles',
                                 params=params).json()
        for role in acl_roles:
            if role['ID'] != token:
                requests.delete(base_uri + 'acl/role/%s' % role['ID'],
                                params=params) 
********************************************
********************************************
def handle(event, context):
    response = {
        "Status": "SUCCESS",
        "StackId": event["StackId"],
        "RequestId": event["RequestId"],
        "LogicalResourceId": event["LogicalResourceId"]
    }

    try:
        converted = cogito.to_json(
            event["ResourceProperties"]["Policy"],
            event["ResourceProperties"].get("Substitutions", {})
        )

        response["PhysicalResourceId"] = hashlib.md5(converted.encode("utf-8")).hexdigest()
        response["Data"] = {}
        response["Data"]["PolicyDocument"] = json.dumps({
            "Version": "2012-10-17",
            "Statement": json.loads(converted)
        })
    except cogito.CogitoError as exception:
        response["Status"] = "FAILED"
        response["Resource"] = exception.message

    requests.put(event["ResponseURL"], json.dumps(response))
    return response 
********************************************
********************************************
def _put(self, request, data, headers=None):
        url = '{0}{1}'.format(self._url(), request)
        headers = self.headers if headers is None else headers
        response = requests.put(url, data=json.dumps(data), headers=headers, verify=self.verify_cert,
                                timeout=self.timeout)
        return self._validate(response) 
********************************************
********************************************
def jsonput(url, data={}, *arg):
    """
    自动将 PUT 请求转换为 JSON
    """
    if url.__class__.__name__ == 'function':
        url = url()
    try:
        return json_moudle.loads(requests.put(url, *arg, data=data, headers=Session.headers,
                                              verify=verify, timeout=3).text)
    except json_moudle.decoder.JSONDecodeError as e:
        log(f'JSON 解析错误,请检查知乎 API 的 URL 是否变化,当前 URL 为:{url}') 
********************************************
********************************************
def testCompound(self):
        # test Dataset with compound type
        domain = self.base_domain + "/testCompound.h5"
        helper.setupDomain(domain)
        print("testCompound", domain)
        headers = helper.getRequestHeaders(domain=domain)

        # get domain
        req = helper.getEndpoint() + '/'
        rsp = requests.get(req, headers=headers)
        rspJson = json.loads(rsp.text)
        self.assertTrue("root" in rspJson)
        root_uuid = rspJson["root"]

        fields = ({'name': 'temp', 'type': 'H5T_STD_I32LE'},
                    {'name': 'pressure', 'type': 'H5T_IEEE_F32LE'})
        datatype = {'class': 'H5T_COMPOUND', 'fields': fields }
        payload = {'type': datatype, 'shape': 10}
        req = self.endpoint + "/datasets"
        rsp = requests.post(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)  # create dataset
        rspJson = json.loads(rsp.text)
        dset_uuid = rspJson['id']
        self.assertTrue(helper.validateId(dset_uuid))

        # link the new dataset
        name = "dset"
        req = self.endpoint + "/groups/" + root_uuid + "/links/" + name
        payload = {"id": dset_uuid}
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201) 
********************************************
********************************************
def testEmptyShapeAttr(self):
        print("testEmptyShapeAttr", self.base_domain)
        headers = helper.getRequestHeaders(domain=self.base_domain)
        req = helper.getEndpoint() + '/'

        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        root_uuid = rspJson["root"]
        helper.validateId(root_uuid)

        attr_name = "attr_empty_shape"
        attr_payload = {'type': 'H5T_STD_I32LE', 'shape': [], 'value': 42}
        req = self.endpoint + "/groups/" + root_uuid + "/attributes/" + attr_name
        rsp = requests.put(req, headers=headers, data=json.dumps(attr_payload))
        self.assertEqual(rsp.status_code, 201)  # created

        # read back the attribute
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)  # OK
        rspJson = json.loads(rsp.text)
        self.assertTrue("name" in rspJson)
        self.assertEqual(rspJson["name"], "attr_empty_shape")
        self.assertTrue("type" in rspJson)
        attr_type = rspJson["type"]
        self.assertEqual(attr_type["base"], "H5T_STD_I32LE")
        self.assertTrue("hrefs" in rspJson)
        self.assertEqual(len(rspJson["hrefs"]), 3)
        self.assertTrue("value" in rspJson)
        self.assertEqual(rspJson["value"], 42)
        self.assertTrue("shape" in rspJson)
        attr_shape = rspJson["shape"]
        self.assertTrue("class" in attr_shape)
        self.assertEqual(attr_shape["class"], "H5S_SCALAR") 
********************************************
********************************************
def testPutVLenString(self):
        # Test PUT value for 1d attribute with variable length string types
        print("testPutVLenString", self.base_domain)

        headers = helper.getRequestHeaders(domain=self.base_domain)
        req = self.endpoint + '/'

        # Get root uuid
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        root_uuid = rspJson["root"]
        helper.validateId(root_uuid)

        # create attr
        words = ["Parting", "is such", "sweet", "sorrow."]
        fixed_str_type = {"charSet": "H5T_CSET_ASCII",
                "class": "H5T_STRING",
                "length": "H5T_VARIABLE",
                "strPad": "H5T_STR_NULLTERM" }
        data = { "type": fixed_str_type, "shape": 4,
            "value": words}
        attr_name = "str_attr"
        req = self.endpoint + "/groups/" + root_uuid + "/attributes/" + attr_name
        rsp = requests.put(req, data=json.dumps(data), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # read attr
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("value" in rspJson)
        self.assertEqual(rspJson["value"], words)
        self.assertTrue("type" in rspJson)
        type_json = rspJson["type"]
        self.assertTrue("class" in type_json)
        self.assertEqual(type_json["class"], "H5T_STRING")
        self.assertTrue("length" in type_json)
        self.assertEqual(type_json["length"], "H5T_VARIABLE") 
********************************************
********************************************
def testGetAttributeJsonValue(self):
        # Test GET Attribute value with JSON response
        print("testGetAttributeJsonValue", self.base_domain)

        headers = helper.getRequestHeaders(domain=self.base_domain)
        req = self.endpoint + '/'

        # Get root uuid
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        root_uuid = rspJson["root"]
        helper.validateId(root_uuid)

        # create attr
        value = [2,3,5,7,11,13]
        data = { "type": 'H5T_STD_I32LE', "shape": 6, "value": value}
        attr_name = "int_arr_attr"
        req = self.endpoint + "/groups/" + root_uuid + "/attributes/" + attr_name
        rsp = requests.put(req, data=json.dumps(data), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # read attr
        req = self.endpoint + "/groups/" + root_uuid + "/attributes/" + attr_name + "/value"
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("value" in rspJson)
        self.assertFalse("type" in rspJson)
        self.assertFalse("shape" in rspJson)
        self.assertEqual(rspJson["value"], value) 
********************************************
********************************************
def testCreateFolder(self):
        domain = self.base_domain + "/newfolder"
        print("testCreateFolder", domain)
        headers = helper.getRequestHeaders(domain=domain)
        req = helper.getEndpoint() + '/'
        body = {"folder": True}
        rsp = requests.put(req, data=json.dumps(body), headers=headers)
        self.assertEqual(rsp.status_code, 201)
        rspJson = json.loads(rsp.text)
        for k in ("owner", "acls", "created", "lastModified"):
             self.assertTrue(k in rspJson)
        self.assertFalse("root" in rspJson)  # no root -> folder

        # verify that putting the same domain again fails with a 409 error
        rsp = requests.put(req, data=json.dumps(body), headers=headers)
        self.assertEqual(rsp.status_code, 409)

        # do a get on the new folder
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)

        self.assertTrue("owner" in rspJson)
        self.assertTrue("class" in rspJson)
        self.assertEqual(rspJson["class"], "folder")

        # try doing a un-authenticated request
        if config.get("test_noauth") and config.get("default_public"):
            headers = helper.getRequestHeaders()
            req = helper.getEndpoint() + "/?host=" + domain
            # do a get on the folder with a query arg for host
            rsp = requests.get(req)
            self.assertEqual(rsp.status_code, 200)
            rspJson = json.loads(rsp.text)
            for k in ("class", "owner"):
                self.assertTrue(k in rspJson)
            self.assertFalse("root" in rspJson) 
********************************************
********************************************
def testInvalidChildDomain(self):
        domain = self.base_domain + "/notafolder/newdomain.h5"
        # should fail assuming "notafolder" doesn't exist
        headers = helper.getRequestHeaders(domain=domain)
        req = helper.getEndpoint() + '/'

        rsp = requests.put(req, headers=headers)
        self.assertEqual(rsp.status_code, 404) 
********************************************
********************************************
def add_users_to_siteAccess(token, access_mode, allowed_principal_ids):
    headers = {'Authorization': 'Bearer ' + token}
    r = requests.put(CATTLE_AUTH_PROVIDER_URL, json={
        'allowedPrincipalIds': allowed_principal_ids,
        'accessMode': access_mode,
        'responseType': 'json',
    }, verify=False, headers=headers)
    print(r.json()) 
********************************************
********************************************
def set_position(self, latitude, longitude):
        """ Set the position of the user using Happn's API
            :param latitude Latitude to position the User
            :param longitude Longitude to position the User
        """

        # Create & Send the HTTP Post to Happn server
        h=headers
        h.update({
            'Authorization' : 'OAuth="'+ self.oauth + '"',
            'Content-Length': '342', #@TODO figure out length calculation
            'Content-Type'  : 'application/json'
            })

        url = 'https://api.happn.fr/api/users/' + self.id + '/devices/'+config('DEVICE_ID')
        payload = {
            "alt"       : 20 + random.uniform(-10,10),
            "latitude"  : round(latitude,7),
            "longitude" : round(longitude,7),
        }
        r = requests.put(url,headers=h,data=json.dumps(payload))

        # Check status of Position Update
        if r.status_code == 200:    #OK HTTP status
            self.lat = latitude
            self.lon = longitude
            logging.debug('Set User position at %f, %f', self.lat, self.lon)
        else:
            # Status failed, get the current location according to the server
            #@TODO IMPLEMENT ^
            self.lat = latitude
            self.lon = longitude

            logging.warning("""Server denied request for position change: %s,
                                will revert to server known location""", httpErrors[r.status_code])

            # If unable to change location raise an exception
            raise HTTP_MethodError(httpErrors[r.status_code]) 
********************************************
********************************************
def set_matching_age_min(self, age):
        """ Set matching min. age
            :mininum age to like
        """

        # Create and send HTTP PUT to Happn server
        h = headers
        h.update({
            'Authorization' : 'OAuth="'+ self.oauth + '"',
            'Content-Type'  : 'application/x-www-form-urlencoded; charset=UTF-8',
            'Content-Length': '20'
        })
        payload = {
            'matching_age_min' : age
        }
        url = 'https://api.happn.fr/api/users/' + self.id
        try:
            r = requests.put(url, headers=h, data = payload,verify=False)
        except Exception as e:
            raise HTTP_MethodError('Error Connecting to Happn Server: {}'.format(e))

        if r.status_code == 200: #200 = 'OK'
            logging.debug('Set minimum accept age to '+str(age))
        else:
            # Unable to fetch distance
            raise HTTP_MethodError(httpErrors[r.status_code]) 
********************************************
********************************************
def set_matching_age_max(self, age):
        """ Set matching max. age
            :maximum age to like
        """

        # Create and send HTTP PUT to Happn server
        h = headers
        h.update({
            'Authorization' : 'OAuth="'+ self.oauth + '"',
            'Content-Type'  : 'application/x-www-form-urlencoded; charset=UTF-8',
            'Content-Length': '20'
        })
        payload = {
            'matching_age_max' : age
        }
        url = 'https://api.happn.fr/api/users/' + self.id
        try:
            r = requests.put(url, headers=h, data = payload,verify=False)
        except Exception as e:
            raise HTTP_MethodError('Error Connecting to Happn Server: {}'.format(e))

        if r.status_code == 200: #200 = 'OK'
            logging.debug('Set maximum accept age to '+str(age))
        else:
            # Unable to fetch distance
            raise HTTP_MethodError(httpErrors[r.status_code]) 
********************************************
********************************************
def updateticket(ticket,priority,status):
  """api call for update a tiket in freshdesk"""
  headers = {
    'Content-Type': 'application/json',
  }
  data = { "priority": priority, "status": status }
  json_data=json.dumps(data);
  url=ticket_url+'/'+str(ticket)

  response = requests.put(url, headers=headers, data=json_data, auth=(apikey, 'X'))
  return response 
********************************************
********************************************
def state_save(self):
        try:
            data = '{"put": {"state":' + str(json.dumps(self.state)) + '}}'
            response = requests.put(self.config['database']['url'], headers=self.config['database']['headers'], data=data)
            self.log_add_text('helper', str(response.text))
            self.log_add_text('helper', 'saved state ' + str(self.state))
            return
        except Exception as e:
            self.log_add_text('helper', str(e)) 
********************************************
********************************************
def upload_file(body, fileName, contentType, contentLength):

    # 1. GET FILE STORAGE URI
    fileUploadPartsResponse = requests.post(fileUploadRequestURI, 
        headers={
            'Authorization': iotHubSasToken,
            'Content-Type': 'application/json'
        },
        data = '{ "blobName": "%s"}' % (fileName)
    )

    print(fileUploadRequestURI)
    print(fileUploadPartsResponse.status_code)
    print(fileUploadPartsResponse.text)

    if fileUploadPartsResponse.status_code == 200:
 
        fileUploadParts = fileUploadPartsResponse.json()
        fileUploadURI = fileUploadURITemplate % (fileUploadParts["hostName"], fileUploadParts["containerName"], fileUploadParts["blobName"], fileUploadParts["sasToken"])
        
        # 2. UPLOAD FILE TO BLOB STORAGE
        uploadResponse = requests.put(fileUploadURI, 
            headers={
                'Content-Type': contentType,
                'Content-Length': contentLength,
                'x-ms-blob-type': 'BlockBlob',
                
            },
            data = body
        )

        print(fileUploadURI)
        print(uploadResponse.status_code)
        print(uploadResponse.text)
        
        if uploadResponse.status_code == 201:

            # 3. GET UPLOAD FILE NOTIFICATION
            notificationResponse = requests.post(notificationURI, 
                headers={
                    'Authorization': iotHubSasToken,
                    'Content-Type': 'application/json'
                },
                data = '{"correlationId": "%s" }' % (fileUploadParts["correlationId"])
            )
    
            print(notificationURI)
            print(notificationResponse.status_code)
            print(notificationResponse.text) 
********************************************
********************************************
def _request(self, method, path, query=False, data=False):
        """
        Do the request

        :param method: choose the method to use (post, put or delete)
        :type method: str

        :param path: choose the path to use
        :type path: str

        :param query: choose the query to use
        :type query: str

        :param data: the payload for the post method
        :type data: str

        :return: the response
        :rtype: str or False
        """
        requests_headers = {'Accept': 'application/json',
                            'User-Agent': 'oliver-zehentleitner/unicorn-binance-websocket-api/' +
                                          self.unicorn_binance_websocket_api_version,
                            'X-MBX-APIKEY': str(self.api_key)}
        if query is not False:
            uri = self.restful_base_uri + path + "?" + query
        else:
            uri = self.restful_base_uri + path
        try:
            if method == "post":
                request_handler = requests.post(uri, headers=requests_headers)
            elif method == "put":
                request_handler = requests.put(uri, headers=requests_headers, data=data)
            elif method == "delete":
                request_handler = requests.delete(uri, headers=requests_headers)
            else:
                request_handler = False
        except requests.exceptions.ConnectionError as error_msg:
            logging.critical("BinanceWebSocketApiRestclient->_request() - error_msg: " + str(error_msg))
            return False
        except socket.gaierror as error_msg:
            logging.critical("BinanceWebSocketApiRestclient->_request() - error_msg: " + str(error_msg))
            return False
        if request_handler.status_code == "418":
            logging.critical("BinanceWebSocketApiRestclient->_request() received status_code 418 from binance! You got"
                             "banned from the binance api! Read this: https://github.com/binance-exchange/binance-"
                             "official-api-sphinx/blob/master/rest-api.md#limits")
        elif request_handler.status_code == "429":
            logging.critical("BinanceWebSocketApiRestclient->_request() received status_code 429 from binance! Back off"
                             "or you are going to get banned! Read this: https://github.com/binance-exchange/binance-"
                             "official-api-sphinx/blob/master/rest-api.md#limits")

        try:
            respond = request_handler.json()
        except simplejson.errors.JSONDecodeError as error_msg:
            logging.critical(str(error_msg))
            return False
        self.binance_api_status['weight'] = request_handler.headers.get('X-MBX-USED-WEIGHT')
        self.binance_api_status['timestamp'] = time.time()
        self.binance_api_status['status_code'] = request_handler.status_code
        request_handler.close()
        return respond 
********************************************
********************************************
def push_modification(self, modification):
        url = self.API_ROOT + "/repos/{}/{}/contents/{}".format(
            self.head_owner, self.head_repo, modification.file_path
        )

        r = requests.get(url, params={"ref": self.head_ref})
        if not r.ok:
            raise Exception("Can not access to the {}/{}'s content.".format(
                self.head_owner, self.head_repo
            ))
        encoding = r.encoding
        body = r.json()
        content = body["content"]
        content = base64.b64decode(content).decode(encoding)
        sha = body["sha"]
        fix_position = int(modification.line_no) - 1  # read file lines start with 0
        fixed = content
        with StringIO(content) as c:
            lines = c.readlines()
            words = lines[fix_position].split(" ")
            for i, w in enumerate(words):
                _w = SpellChecker.strip(w.strip())
                if _w == modification.target_word:
                    words[i] = words[i].replace(_w, modification.candidates[0])
            
            fixed = " ".join(words) + "\n"
            lines[fix_position] = fixed
            fixed = "".join(lines)
        
        if content != fixed:
            encoded = base64.b64encode(fixed.encode(encoding)).decode(encoding)
            message = "fix typo: {} to {}, line {}".format(
                modification.target_word,
                modification.candidates[0],
                modification.line_no
                )

            payload = {
                "message": message,
                "content": encoded,
                "sha": sha,
                "branch": self.head_ref
            }
            r = requests.put(url, json=payload,headers=make_auth_header(self.installation_id))

            if not r.ok:
                print(r.json())
                r.raise_for_status()
            return True
        
        return False 
********************************************
********************************************
def testPostNullSpace(self):
        # test Dataset with null dataspace type
        domain = self.base_domain + "/testPostNullSpace.h5"
        helper.setupDomain(domain)

        print("testNullSpace", domain)
        headers = helper.getRequestHeaders(domain=domain)

        # get domain
        req = helper.getEndpoint() + '/'
        rsp = requests.get(req, headers=headers)
        rspJson = json.loads(rsp.text)
        self.assertTrue("root" in rspJson)
        root_uuid = rspJson["root"]

        # pass H5S_NULL for shape
        payload = {'type': 'H5T_IEEE_F32LE', 'shape': 'H5S_NULL'}
        req = self.endpoint + "/datasets"
        rsp = requests.post(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)  # create dataset
        rspJson = json.loads(rsp.text)
        dset_uuid = rspJson['id']
        self.assertTrue(helper.validateId(dset_uuid))

        # link new dataset as 'dset1'
        name = 'dset1'
        req = self.endpoint + "/groups/" + root_uuid + "/links/" + name
        payload = {"id": dset_uuid}
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # verify the dataspace is has a null dataspace
        req = self.endpoint + "/datasets/" + dset_uuid
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        shape = rspJson['shape']
        self.assertEqual(shape['class'], 'H5S_NULL')
        # verify type
        type_json = rspJson["type"]
        self.assertEqual(type_json["class"], 'H5T_FLOAT')
        self.assertEqual(type_json['base'], 'H5T_IEEE_F32LE') 
********************************************
********************************************
def testAutoChunk1dDataset(self):
        # test Dataset where chunk layout is set automatically
        domain = self.base_domain + "/testAutoChunk1dDataset.h5"
        helper.setupDomain(domain)
        print("testAutoChunk1dDataset", domain)
        headers = helper.getRequestHeaders(domain=domain)
        # get domain
        req = helper.getEndpoint() + '/'
        rsp = requests.get(req, headers=headers)
        rspJson = json.loads(rsp.text)
        self.assertTrue("root" in rspJson)
        root_uuid = rspJson["root"]

        # create the dataset
        req = self.endpoint + "/datasets"
        # 50K x 80K dataset
        extent = 1000 * 1000 * 1000
        dims = [extent,]
        fields = (  {'name': 'x', 'type': 'H5T_IEEE_F64LE'},
                    {'name': 'y', 'type': 'H5T_IEEE_F64LE'},
                    {'name': 'z', 'type': 'H5T_IEEE_F64LE'})
        datatype = {'class': 'H5T_COMPOUND', 'fields': fields }

        payload = {'type': datatype, 'shape': dims }
        # the following should get ignored as too small
        payload['creationProperties'] = {'layout': {'class': 'H5D_CHUNKED', 'dims': [10,] }}
        req = self.endpoint + "/datasets"
        rsp = requests.post(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)  # create dataset
        rspJson = json.loads(rsp.text)

        dset_uuid = rspJson['id']
        self.assertTrue(helper.validateId(dset_uuid))

        # link new dataset as 'dset'
        name = 'dset'
        req = self.endpoint + "/groups/" + root_uuid + "/links/" + name
        payload = {"id": dset_uuid}
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # verify layout
        req = helper.getEndpoint() + "/datasets/" + dset_uuid
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("layout" in rspJson)
        layout_json = rspJson["layout"]
        self.assertTrue("class" in layout_json)
        self.assertEqual(layout_json["class"], 'H5D_CHUNKED')
        self.assertTrue("dims" in layout_json)
        self.assertTrue("partition_count" not in layout_json)
        layout = layout_json["dims"]
        self.assertEqual(len(layout), 1)
        self.assertTrue(layout[0] < dims[0])
        chunk_size = layout[0] * 8 * 3  # three 64bit
        # chunk size should be between chunk min and max
        self.assertTrue(chunk_size >= CHUNK_MIN)
        self.assertTrue(chunk_size <= CHUNK_MAX) 
********************************************
********************************************
def testAutoChunk2dDataset(self):
        # test Dataset where chunk layout is set automatically
        domain = self.base_domain + "/testAutoChunk2dDataset.h5"
        helper.setupDomain(domain)
        print("testAutoChunk2dDataset", domain)
        headers = helper.getRequestHeaders(domain=domain)
        # get domain
        req = helper.getEndpoint() + '/'
        rsp = requests.get(req, headers=headers)
        rspJson = json.loads(rsp.text)
        self.assertTrue("root" in rspJson)
        root_uuid = rspJson["root"]

        # create the dataset
        req = self.endpoint + "/datasets"
        # 50K x 80K dataset
        dims = [50000, 80000]
        payload = {'type': 'H5T_IEEE_F32LE', 'shape': dims }

        req = self.endpoint + "/datasets"
        rsp = requests.post(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)  # create dataset
        rspJson = json.loads(rsp.text)

        dset_uuid = rspJson['id']
        self.assertTrue(helper.validateId(dset_uuid))

        # link new dataset as 'dset'
        name = 'dset'
        req = self.endpoint + "/groups/" + root_uuid + "/links/" + name
        payload = {"id": dset_uuid}
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # verify layout
        req = helper.getEndpoint() + "/datasets/" + dset_uuid
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("layout" in rspJson)
        layout_json = rspJson["layout"]
        self.assertTrue("class" in layout_json)
        self.assertEqual(layout_json["class"], 'H5D_CHUNKED')
        self.assertTrue("dims" in layout_json)
        layout = layout_json["dims"]
        self.assertEqual(len(layout), 2)
        self.assertTrue(layout[0] < dims[0])
        self.assertTrue(layout[1] < dims[1])
        chunk_size = layout[0] * layout[1] * 4
        # chunk size should be between chunk min and max
        self.assertTrue(chunk_size >= CHUNK_MIN)
        self.assertTrue(chunk_size <= CHUNK_MAX) 
********************************************
********************************************
def testMinChunkSizeDataset(self):
        # test Dataset where chunk layout is adjusted if provided
        # layout is too small
        domain = self.base_domain + "/testMinChunkSizeDataset.h5"
        helper.setupDomain(domain)
        print("testMinChunkSizeDataset", domain)
        headers = helper.getRequestHeaders(domain=domain)
        # get domain
        req = helper.getEndpoint() + '/'
        rsp = requests.get(req, headers=headers)
        rspJson = json.loads(rsp.text)
        self.assertTrue("root" in rspJson)
        root_uuid = rspJson["root"]

        # create the dataset
        req = self.endpoint + "/datasets"
        # 50K x 80K dataset
        dims = [50000, 80000]
        payload = {'type': 'H5T_IEEE_F32LE', 'shape': dims }
        # define a chunk layout with lots of small chunks
        payload['creationProperties'] = {'layout': {'class': 'H5D_CHUNKED', 'dims': [10, 10] }}

        req = self.endpoint + "/datasets"
        rsp = requests.post(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)  # create dataset
        rspJson = json.loads(rsp.text)
        dset_uuid = rspJson['id']
        self.assertTrue(helper.validateId(dset_uuid))

        # link new dataset as 'dset'
        name = 'dset'
        req = self.endpoint + "/groups/" + root_uuid + "/links/" + name
        payload = {"id": dset_uuid}
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # verify layout
        req = helper.getEndpoint() + "/datasets/" + dset_uuid
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("layout" in rspJson)
        layout_json = rspJson["layout"]
        self.assertTrue("class" in layout_json)
        self.assertEqual(layout_json["class"], 'H5D_CHUNKED')
        self.assertTrue("dims" in layout_json)
        layout = layout_json["dims"]
        self.assertEqual(len(layout), 2)
        self.assertTrue(layout[0] < dims[0])
        self.assertTrue(layout[1] < dims[1])
        chunk_size = layout[0] * layout[1] * 4
        # chunk size should be between chunk min and max
        self.assertTrue(chunk_size >= CHUNK_MIN)
        self.assertTrue(chunk_size <= CHUNK_MAX) 
********************************************
********************************************
def testContiguousRefDataset(self):
        # test Dataset where H5D_CONTIGUOUS_REF layout is used
        domain = self.base_domain + "/testContiguousRefDataset.h5"
        helper.setupDomain(domain)
        print("testContiguousRefDataset", domain)
        headers = helper.getRequestHeaders(domain=domain)
        # get domain
        req = helper.getEndpoint() + '/'
        rsp = requests.get(req, headers=headers)
        rspJson = json.loads(rsp.text)
        self.assertTrue("root" in rspJson)
        root_uuid = rspJson["root"]

        # create the dataset
        req = self.endpoint + "/datasets"
        # 50K x 80K dataset
        dims = [50000, 8000000]
        payload = {'type': 'H5T_IEEE_F32LE', 'shape': dims }
        file_uri = "s3://a-storage-bucket/some-file.h5"

        offset = 1234
        size = dims[0] * dims[1] * 4  # uncompressed size

        payload['creationProperties'] = {'layout': {'class': 'H5D_CONTIGUOUS_REF', 'file_uri': file_uri, 'offset': offset, 'size': size }}
        req = self.endpoint + "/datasets"
        rsp = requests.post(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)  # create dataset
        rspJson = json.loads(rsp.text)

        dset_uuid = rspJson['id']
        self.assertTrue(helper.validateId(dset_uuid))

        # link new dataset as 'dset'
        name = 'dset'
        req = self.endpoint + "/groups/" + root_uuid + "/links/" + name
        payload = {"id": dset_uuid}
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # verify layout
        req = helper.getEndpoint() + "/datasets/" + dset_uuid
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("layout" in rspJson)
        layout_json = rspJson["layout"]
        self.assertTrue("class" in layout_json)
        self.assertEqual(layout_json["class"], 'H5D_CONTIGUOUS_REF')
        self.assertEqual(layout_json["file_uri"], file_uri)
        self.assertEqual(layout_json["offset"], offset)
        self.assertEqual(layout_json["size"], size)
        self.assertTrue("dims" in layout_json)
        chunk_dims = layout_json["dims"]
        self.assertEqual(len(chunk_dims), 2)
        chunk_size = chunk_dims[0] * chunk_dims[1] * 4
        # chunk size should be between chunk min and max
        self.assertTrue(chunk_size >= CHUNK_MIN)
        self.assertTrue(chunk_size <= CHUNK_MAX) 
********************************************
********************************************
def testDatasetChunkPartitioning(self):
        # test Dataset partitioning logic for large datasets
        domain = self.base_domain + "/testDatasetChunkPartitioning.h5"
        helper.setupDomain(domain)
        print("testDatasetChunkPartitioning", domain)
        headers = helper.getRequestHeaders(domain=domain)
        # get domain
        req = helper.getEndpoint() + '/'
        rsp = requests.get(req, headers=headers)
        rspJson = json.loads(rsp.text)
        self.assertTrue("root" in rspJson)
        root_uuid = rspJson["root"]

        # create the dataset
        req = self.endpoint + "/datasets"
        # 50K x 80K x 90K dataset
        dims = [50000, 80000, 90000]
        payload = {'type': 'H5T_IEEE_F32LE', 'shape': dims }

        req = self.endpoint + "/datasets"
        rsp = requests.post(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)  # create dataset
        rspJson = json.loads(rsp.text)

        dset_uuid = rspJson['id']
        self.assertTrue(helper.validateId(dset_uuid))

        # link new dataset as 'dset'
        name = 'dset'
        req = self.endpoint + "/groups/" + root_uuid + "/links/" + name
        payload = {"id": dset_uuid}
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # verify layout
        req = helper.getEndpoint() + "/datasets/" + dset_uuid
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("layout" in rspJson)
        layout_json = rspJson["layout"]
        self.assertTrue("class" in layout_json)
        self.assertEqual(layout_json["class"], 'H5D_CHUNKED')
        self.assertTrue("dims" in layout_json)
        self.assertTrue("partition_count" in layout_json)
        self.assertTrue(layout_json["partition_count"] > 1000)  # will change if max_chunks_per_folder is updated

        layout = layout_json["dims"]

        self.assertEqual(len(layout), 3)
        self.assertTrue(layout[0] < dims[0])
        self.assertTrue(layout[1] < dims[1])
        self.assertTrue(layout[2] < dims[2])
        chunk_size = layout[0] * layout[1] * layout[2] * 4
        # chunk size should be between chunk min and max
        self.assertTrue(chunk_size >= CHUNK_MIN)
        self.assertTrue(chunk_size <= CHUNK_MAX) 
********************************************
********************************************
def testExtendibleDatasetChunkPartitioning(self):
        # test Dataset partitioning logic for large datasets
        domain = self.base_domain + "/testExtendibleDatasetChunkPartitioning.h5"
        helper.setupDomain(domain)
        print("testExtendibleDatasetChunkPartitioning", domain)
        headers = helper.getRequestHeaders(domain=domain)
        # get domain
        req = helper.getEndpoint() + '/'
        rsp = requests.get(req, headers=headers)
        rspJson = json.loads(rsp.text)
        self.assertTrue("root" in rspJson)
        root_uuid = rspJson["root"]

        # create the dataset
        req = self.endpoint + "/datasets"
        # 50K x 80K x 90K dataset
        dims = [0, 80000, 90000]
        # unlimited extend in dim 0, fixeed in dimension 2, extenbile by 10x in dim 3
        max_dims = [0,80000,900000]
        payload = {'type': 'H5T_IEEE_F32LE', 'shape': dims, 'maxdims': max_dims }

        req = self.endpoint + "/datasets"
        rsp = requests.post(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)  # create dataset
        rspJson = json.loads(rsp.text)

        dset_uuid = rspJson['id']
        self.assertTrue(helper.validateId(dset_uuid))

        # link new dataset as 'dset'
        name = 'dset'
        req = self.endpoint + "/groups/" + root_uuid + "/links/" + name
        payload = {"id": dset_uuid}
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # verify layout
        req = helper.getEndpoint() + "/datasets/" + dset_uuid
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("layout" in rspJson)
        layout_json = rspJson["layout"]
        self.assertTrue("class" in layout_json)
        self.assertEqual(layout_json["class"], 'H5D_CHUNKED')
        self.assertTrue("dims" in layout_json)
        self.assertTrue("partition_count" in layout_json)

        layout = layout_json["dims"]

        self.assertEqual(len(layout), 3)
        chunk_size = layout[0] * layout[1] * layout[2] * 4
        # chunk size should be between chunk min and max
        self.assertTrue(chunk_size >= CHUNK_MIN)
        self.assertTrue(chunk_size <= CHUNK_MAX) 
********************************************
********************************************
def testPutVLenInt(self):
        # Test PUT value for 1d attribute with variable length int types
        print("testPutVLenInt", self.base_domain)

        headers = helper.getRequestHeaders(domain=self.base_domain)
        req = self.endpoint + '/'

        # Get root uuid
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        root_uuid = rspJson["root"]
        helper.validateId(root_uuid)

        # create dataset
        vlen_type = {"class": "H5T_VLEN", "base": { "class": "H5T_INTEGER", "base": "H5T_STD_I32LE"}}
        payload = {'type': vlen_type, 'shape': [4,]}
        req = self.endpoint + "/datasets"
        rsp = requests.post(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)  # create dataset
        rspJson = json.loads(rsp.text)
        dset_uuid = rspJson['id']
        self.assertTrue(helper.validateId(dset_uuid))

        # link new dataset as 'dset'
        name = 'dset'
        req = self.endpoint + "/groups/" + root_uuid + "/links/" + name
        payload = {"id": dset_uuid}
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # write values to dataset
        data = [[1,], [1,2], [1,2,3], [1,2,3,4]]
        req = self.endpoint + "/datasets/" + dset_uuid + "/value"
        payload = { 'value': data }
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 200)

        # read values from dataset
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("value" in rspJson)
        value = rspJson["value"]
        self.assertEqual(len(value), 4)
        for i in range(4):
            self.assertEqual(value[i], data[i])

        # read back a selection
        params = {"select": "[2:3]"}
        rsp = requests.get(req, headers=headers, params=params)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("value" in rspJson)
        value = rspJson["value"]
        self.assertEqual(len(value), 1)
        self.assertEqual(value[0], data[2]) 
********************************************
********************************************
def testPutVLenString(self):
        # Test PUT value for 1d attribute with variable length string types
        print("testPutVLenString", self.base_domain)

        headers = helper.getRequestHeaders(domain=self.base_domain)
        req = self.endpoint + '/'

        # Get root uuid
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        root_uuid = rspJson["root"]
        helper.validateId(root_uuid)

        # create dataset
        vlen_type =  {"class": "H5T_STRING", "charSet": "H5T_CSET_ASCII",
                    "strPad": "H5T_STR_NULLTERM", "length": "H5T_VARIABLE"}
        payload = {'type': vlen_type, 'shape': [4,]}
        req = self.endpoint + "/datasets"
        rsp = requests.post(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)  # create dataset
        rspJson = json.loads(rsp.text)
        dset_uuid = rspJson['id']
        self.assertTrue(helper.validateId(dset_uuid))

        # link new dataset as 'dset'
        name = 'dset'
        req = self.endpoint + "/groups/" + root_uuid + "/links/" + name
        payload = {"id": dset_uuid}
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # write values to dataset
        data = ["This is", "a variable length", "string", "array"]
        req = self.endpoint + "/datasets/" + dset_uuid + "/value"
        payload = { 'value': data }
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 200)

        # read values from dataset
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("value" in rspJson)
        value = rspJson["value"]
        self.assertEqual(len(value), 4)
        for i in range(4):
            self.assertEqual(value[i], data[i]) 
********************************************
********************************************
def testPutScalarDataset(self):
        # Test read/write to scalar dataset
        print("testPutScalarDataset", self.base_domain)

        headers = helper.getRequestHeaders(domain=self.base_domain)
        req = self.endpoint + '/'

        # Get root uuid
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        root_uuid = rspJson["root"]
        helper.validateId(root_uuid)

        # create a dataset obj
        str_type = { 'charSet':   'H5T_CSET_ASCII',
                     'class':  'H5T_STRING',
                     'strPad': 'H5T_STR_NULLPAD',
                     'length': 40}
        data = { "type": str_type}
        req = self.endpoint + '/datasets'
        rsp = requests.post(req, data=json.dumps(data), headers=headers)
        self.assertEqual(rsp.status_code, 201)
        rspJson = json.loads(rsp.text)
        self.assertEqual(rspJson["attributeCount"], 0)
        dset_id = rspJson["id"]
        self.assertTrue(helper.validateId(dset_id))

        # link new dataset as 'dset_scalar'
        name = "dset_scalar"
        req = self.endpoint + "/groups/" + root_uuid + "/links/" + name
        payload = {"id": dset_id}
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # read unintialized value from dataset
        req = self.endpoint + "/datasets/" + dset_id + "/value"
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("value" in rspJson)
        self.assertEqual(rspJson["value"], '')

        # write to the dataset
        data = "Hello, world"
        payload = { 'value': data }
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 200)

        # read back the value
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("value" in rspJson)
        self.assertEqual(rspJson["value"], "Hello, world") 
********************************************
********************************************
def testNullSpaceDataset(self):
        # Test attempted read/write to null space dataset
        print("testNullSpaceDataset", self.base_domain)

        headers = helper.getRequestHeaders(domain=self.base_domain)
        req = self.endpoint + '/'

        # Get root uuid
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        root_uuid = rspJson["root"]
        helper.validateId(root_uuid)

        # create a dataset obj
        str_type = { 'charSet':   'H5T_CSET_ASCII',
                     'class':  'H5T_STRING',
                     'strPad': 'H5T_STR_NULLPAD',
                     'length': 40}
        data = { "type": str_type, 'shape': 'H5S_NULL'}
        req = self.endpoint + '/datasets'
        rsp = requests.post(req, data=json.dumps(data), headers=headers)
        self.assertEqual(rsp.status_code, 201)
        rspJson = json.loads(rsp.text)
        dset_id = rspJson["id"]
        self.assertTrue(helper.validateId(dset_id))

        # link new dataset as 'dset_null'
        name = "dset_null"
        req = self.endpoint + "/groups/" + root_uuid + "/links/" + name
        payload = {"id": dset_id}
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # try reading from the dataset
        req = self.endpoint + "/datasets/" + dset_id + "/value"
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 400)

        # try writing to the dataset
        data = "Hello, world"
        payload = { 'value': data }
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 400) 
********************************************
********************************************
def testSimpleTypeFillValue(self):
        # test Dataset with simple type and fill value
        print("testSimpleTypeFillValue", self.base_domain)
        headers = helper.getRequestHeaders(domain=self.base_domain)

        # get domain
        req = helper.getEndpoint() + '/'
        rsp = requests.get(req, headers=headers)
        rspJson = json.loads(rsp.text)
        self.assertTrue("root" in rspJson)
        root_uuid = rspJson["root"]

        # create the dataset
        req = self.endpoint + "/datasets"
        payload = {'type': 'H5T_STD_I32LE', 'shape': 10}
        creation_props = {'fillValue': 42 }
        payload['creationProperties'] = creation_props

        req = self.endpoint + "/datasets"
        rsp = requests.post(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)  # create dataset
        rspJson = json.loads(rsp.text)
        dset_uuid = rspJson['id']
        self.assertTrue(helper.validateId(dset_uuid))

        # link new dataset as 'dset'
        name = 'dset'
        req = self.endpoint + "/groups/" + root_uuid + "/links/" + name
        payload = {"id": dset_uuid}
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # read back the data
        req = self.endpoint + "/datasets/" + dset_uuid + "/value"
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("value" in rspJson)
        self.assertEqual(rspJson["value"], [42,]*10)

        # write some values
        payload = { 'start': 0, 'stop': 5, 'value': [24,]*5 }
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 200)

        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("value" in rspJson)
        ret_values = rspJson["value"]
        for i in range(5):
            self.assertEqual(ret_values[i], 24)
            self.assertEqual(ret_values[i+5], 42) 
********************************************
********************************************
def testDeflateCompression(self):
        # test Dataset with creation property list
        print("testDefalteCompression", self.base_domain)
        headers = helper.getRequestHeaders(domain=self.base_domain)
        # get domain
        req = helper.getEndpoint() + '/'
        rsp = requests.get(req, headers=headers)
        rspJson = json.loads(rsp.text)
        self.assertTrue("root" in rspJson)
        root_uuid = rspJson["root"]

        # create the dataset
        req = self.endpoint + "/datasets"

        # Create ~1MB dataset

        payload = {'type': 'H5T_STD_I8LE', 'shape': [1024, 1024]}
        # define deflate compression
        gzip_filter = {'class': 'H5Z_FILTER_DEFLATE', 'id': 1, 'level': 9, 'name': 'deflate'}
        payload['creationProperties'] = {'filters': [gzip_filter,] }
        req = self.endpoint + "/datasets"
        rsp = requests.post(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)  # create dataset
        rspJson = json.loads(rsp.text)
        dset_uuid = rspJson['id']
        self.assertTrue(helper.validateId(dset_uuid))

        # link new dataset as 'dset'
        name = 'dset'
        req = self.endpoint + "/groups/" + root_uuid + "/links/" + name
        payload = {"id": dset_uuid}
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # write a horizontal strip of 22s
        req = self.endpoint + "/datasets/" + dset_uuid + "/value"
        data = [22,] * 1024
        payload = { 'start': [512, 0], 'stop': [513, 1024], 'value': data }
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 200)

        # read back the 512,512 element
        req = self.endpoint + "/datasets/" + dset_uuid + "/value"  # test
        params = {"select": "[512:513,512:513]"} # read  1 element
        rsp = requests.get(req, params=params, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("value" in rspJson)
        value = rspJson["value"]
        self.assertEqual(len(value), 1)
        row = value[0]
        self.assertEqual(len(row), 1)
        self.assertEqual(row[0], 22) 
********************************************
********************************************
def testShuffleFilter(self):
        # test Dataset with creation property list
        print("testShuffleFilter", self.base_domain)
        headers = helper.getRequestHeaders(domain=self.base_domain)
        # get domain
        req = helper.getEndpoint() + '/'
        rsp = requests.get(req, headers=headers)
        rspJson = json.loads(rsp.text)
        self.assertTrue("root" in rspJson)
        root_uuid = rspJson["root"]

        # create the dataset
        req = self.endpoint + "/datasets"

        # Create ~4MB dataset

        payload = {'type': 'H5T_STD_I32LE', 'shape': [1024, 1024]}
        # define sshufle compression
        shuffle_filter = {'class': 'H5Z_FILTER_SHUFFLE', 'id': 2, 'name': 'shuffle'}
        payload['creationProperties'] = {'filters': [shuffle_filter,] }
        req = self.endpoint + "/datasets"
        rsp = requests.post(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)  # create dataset
        rspJson = json.loads(rsp.text)
        dset_uuid = rspJson['id']
        self.assertTrue(helper.validateId(dset_uuid))

        # link new dataset as 'dset'
        name = 'dset'
        req = self.endpoint + "/groups/" + root_uuid + "/links/" + name
        payload = {"id": dset_uuid}
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # write a horizontal strip of 22s
        req = self.endpoint + "/datasets/" + dset_uuid + "/value"
        data = [22,] * 1024
        payload = { 'start': [512, 0], 'stop': [513, 1024], 'value': data }
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 200)

        # read back the 512,512 element
        req = self.endpoint + "/datasets/" + dset_uuid + "/value"  # test
        params = {"select": "[512:513,512:513]"} # read  1 element
        rsp = requests.get(req, params=params, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("value" in rspJson)
        value = rspJson["value"]
        self.assertEqual(len(value), 1)
        row = value[0]
        self.assertEqual(len(row), 1)
        self.assertEqual(row[0], 22) 
********************************************
********************************************
def testLargeCreationProperties(self):
        # test Dataset with artifically large creation_properties data
        print("testLargeCreationProperties", self.base_domain)
        headers = helper.getRequestHeaders(domain=self.base_domain)

        # get domain
        req = helper.getEndpoint() + '/'
        rsp = requests.get(req, headers=headers)
        rspJson = json.loads(rsp.text)
        self.assertTrue("root" in rspJson)
        root_uuid = rspJson["root"]

        # create the dataset
        req = self.endpoint + "/datasets"
        payload = {'type': 'H5T_STD_I32LE', 'shape': 10}
        creation_props = {'fillValue': 42 }
        foo_bar = {}
        for i in range(500):
            foo_bar[i] = f"this is a test {i}"
        creation_props['foo_bar'] = foo_bar

        payload['creationProperties'] = creation_props

        req = self.endpoint + "/datasets"
        rsp = requests.post(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)  # create dataset
        rspJson = json.loads(rsp.text)
        dset_uuid = rspJson['id']
        self.assertTrue(helper.validateId(dset_uuid))

        # link new dataset as 'dset'
        name = 'dset'
        req = self.endpoint + "/groups/" + root_uuid + "/links/" + name
        payload = {"id": dset_uuid}
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # read back the data
        req = self.endpoint + "/datasets/" + dset_uuid + "/value"
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("value" in rspJson)
        self.assertEqual(rspJson["value"], [42,]*10)

        # write some values
        payload = { 'start': 0, 'stop': 5, 'value': [24,]*5 }
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 200)

        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("value" in rspJson)
        ret_values = rspJson["value"]
        for i in range(5):
            self.assertEqual(ret_values[i], 24)
            self.assertEqual(ret_values[i+5], 42) 
********************************************
********************************************
def testNullShapeAttr(self):
        print("testNullShapeAttr", self.base_domain)
        headers = helper.getRequestHeaders(domain=self.base_domain)
        req = helper.getEndpoint() + '/'

        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        root_uuid = rspJson["root"]
        helper.validateId(root_uuid)

        attr_name = "attr_null_shape"
        attr_payload = {'type': 'H5T_STD_I32LE', 'shape': 'H5S_NULL', 'value': 42}
        req = self.endpoint + "/groups/" + root_uuid + "/attributes/" + attr_name
        rsp = requests.put(req, headers=headers, data=json.dumps(attr_payload))
        self.assertEqual(rsp.status_code, 400)  # can't include data

        # try again without the data
        attr_payload = {'type': 'H5T_STD_I32LE', 'shape': 'H5S_NULL'}
        req = self.endpoint + "/groups/" + root_uuid + "/attributes/" + attr_name
        rsp = requests.put(req, headers=headers, data=json.dumps(attr_payload))
        self.assertEqual(rsp.status_code, 201)  # Created

        # read back the attribute
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)  # OK
        rspJson = json.loads(rsp.text)
        self.assertTrue("name" in rspJson)
        self.assertEqual(rspJson["name"], attr_name)
        self.assertTrue("type" in rspJson)
        attr_type = rspJson["type"]
        self.assertEqual(attr_type["base"], "H5T_STD_I32LE")
        self.assertTrue("hrefs" in rspJson)
        self.assertEqual(len(rspJson["hrefs"]), 3)
        self.assertTrue("value" in rspJson)
        self.assertEqual(rspJson["value"], None)
        self.assertTrue("shape" in rspJson)
        attr_shape = rspJson["shape"]
        self.assertTrue("class" in attr_shape)
        self.assertEqual(attr_shape["class"], "H5S_NULL")

        # read value should fail with 400
        rsp = requests.get(req+"/value", headers=headers)
        self.assertEqual(rsp.status_code, 400)  # Bad Request 
********************************************
********************************************
def testNoShapeAttr(self):
        print("testNoShapeAttr", self.base_domain)
        headers = helper.getRequestHeaders(domain=self.base_domain)
        req = helper.getEndpoint() + '/'

        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        root_uuid = rspJson["root"]
        helper.validateId(root_uuid)

        attr_name = "attr_no_shape"
        attr_payload = {'type': 'H5T_STD_I32LE'}
        req = self.endpoint + "/groups/" + root_uuid + "/attributes/" + attr_name
        rsp = requests.put(req, headers=headers, data=json.dumps(attr_payload))
        self.assertEqual(rsp.status_code, 201)  # created


        # read back the attribute
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)  # OK
        rspJson = json.loads(rsp.text)
        self.assertTrue("name" in rspJson)
        self.assertEqual(rspJson["name"], attr_name)
        self.assertTrue("type" in rspJson)
        attr_type = rspJson["type"]
        self.assertEqual(attr_type["base"], "H5T_STD_I32LE")
        self.assertTrue("hrefs" in rspJson)
        self.assertEqual(len(rspJson["hrefs"]), 3)
        self.assertTrue("value" in rspJson)
        self.assertEqual(rspJson["value"], None)
        self.assertTrue("shape" in rspJson)
        attr_shape = rspJson["shape"]
        self.assertTrue("class" in attr_shape)
        self.assertEqual(attr_shape["class"], "H5S_SCALAR")
        self.assertFalse("dims" in attr_shape)

        # read value should return None
        rsp = requests.get(req+"/value", headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("value" in rspJson)
        self.assertEqual(rspJson["value"], None) 
********************************************
********************************************
def testPutFixedStringNullTerm(self):
        # Test PUT value for 1d attribute with fixed length string/null terminated types
        print("testPutFixedStringNullTerm", self.base_domain)

        headers = helper.getRequestHeaders(domain=self.base_domain)
        req = self.endpoint + '/'

        # Get root uuid
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        root_uuid = rspJson["root"]
        helper.validateId(root_uuid)

        # create attr
        text = "I'm an ASCII null terminated string"
        text_length = len(text) + 1
        fixed_str_type = {
                "charSet": "H5T_CSET_ASCII",
                "class": "H5T_STRING",
                "length": text_length,
                "strPad": "H5T_STR_NULLTERM" }
        scalar_shape = { "class": "H5S_SCALAR" }
        data = { "type": fixed_str_type, "shape": scalar_shape,
            "value": text}
        attr_name = "str_attr"
        req = self.endpoint + "/groups/" + root_uuid + "/attributes/" + attr_name
        rsp = requests.put(req, data=json.dumps(data), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # read attr
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("value" in rspJson)
        self.assertEqual(rspJson["value"], text)
        self.assertTrue("type" in rspJson)
        type_json = rspJson["type"]
        self.assertTrue("class" in type_json)
        self.assertEqual(type_json["class"], "H5T_STRING")
        self.assertTrue("length" in type_json)
        self.assertEqual(type_json["length"], text_length)
        self.assertTrue("strPad" in type_json)
        self.assertEqual(type_json["strPad"], "H5T_STR_NULLTERM")
        self.assertTrue("charSet" in type_json)
        self.assertEqual(type_json["charSet"], "H5T_CSET_ASCII") 
********************************************
********************************************
def testPutVLenUTF8String(self):
        # Test PUT value for 1d attribute with fixed length UTF-8 string
        print("testPutFixedUTF8String", self.base_domain)

        headers = helper.getRequestHeaders(domain=self.base_domain)
        req = self.endpoint + '/'

        # Get root uuid
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        root_uuid = rspJson["root"]
        helper.validateId(root_uuid)

        # create attr
        text = "I'm an UTF-8 null terminated string"
        text_length = len(text) + 1
        fixed_str_type = {
                "charSet": "H5T_CSET_UTF8",
                "class": "H5T_STRING",
                "length": text_length,
                "strPad": "H5T_STR_NULLTERM" }
        variable_str_type = {
                "charSet": "H5T_CSET_UTF8",
                "class": "H5T_STRING",
                "length": "H5T_VARIABLE",
                "strPad": "H5T_STR_NULLTERM" }
        scalar_shape = { "class": "H5S_SCALAR" }
        data = { "type": fixed_str_type, "shape": scalar_shape,
            "value": text}
        attr_name = "str_attr"
        req = self.endpoint + "/groups/" + root_uuid + "/attributes/" + attr_name
        # Should fail since UTF8 with fixed width is not supported
        rsp = requests.put(req, data=json.dumps(data), headers=headers)
        self.assertEqual(rsp.status_code, 400)

        data = { "type": variable_str_type, "shape": scalar_shape,
            "value": text}
        attr_name = "str_attr"
        req = self.endpoint + "/groups/" + root_uuid + "/attributes/" + attr_name
        rsp = requests.put(req, data=json.dumps(data), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # read attr
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("value" in rspJson)
        self.assertEqual(rspJson["value"], text)
        self.assertTrue("type" in rspJson)
        type_json = rspJson["type"]
        self.assertTrue("class" in type_json)
        self.assertEqual(type_json["class"], "H5T_STRING")
        self.assertTrue("length" in type_json)
        self.assertEqual(type_json["length"], "H5T_VARIABLE")
        self.assertTrue("strPad" in type_json)
        self.assertEqual(type_json["strPad"], "H5T_STR_NULLTERM")
        self.assertTrue("charSet" in type_json)
        self.assertEqual(type_json["charSet"], "H5T_CSET_UTF8") 
********************************************
********************************************
def testPutVLenInt(self):
        # Test PUT value for 1d attribute with variable length int types
        print("testPutVLenInt", self.base_domain)

        headers = helper.getRequestHeaders(domain=self.base_domain)
        req = self.endpoint + '/'

        # Get root uuid
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        root_uuid = rspJson["root"]
        helper.validateId(root_uuid)

        # create attr
        vlen_type = {"class": "H5T_VLEN", "base": { "class": "H5T_INTEGER", "base": "H5T_STD_I32LE"}}
        value = [[1,], [1,2], [1,2,3], [1,2,3,4]]
        data = { "type": vlen_type, "shape": 4, "value": value}
        attr_name = "vlen_int_attr"
        req = self.endpoint + "/groups/" + root_uuid + "/attributes/" + attr_name
        rsp = requests.put(req, data=json.dumps(data), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # read attr
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("value" in rspJson)
        self.assertEqual(rspJson["value"], value)
        self.assertTrue("type" in rspJson)
        type_json = rspJson["type"]
        self.assertTrue("class" in type_json)
        self.assertEqual(type_json["class"], "H5T_VLEN")
        self.assertTrue("base" in type_json)
        base_type = type_json["base"]
        self.assertTrue("class" in base_type)
        self.assertEqual(base_type["class"], "H5T_INTEGER")
        self.assertTrue("base" in base_type)
        self.assertEqual(base_type["base"], "H5T_STD_I32LE")
        self.assertTrue("shape" in rspJson)
        shape_json = rspJson["shape"]
        self.assertTrue("class" in shape_json)
        self.assertEqual(shape_json["class"], "H5S_SIMPLE")
        self.assertTrue("dims" in shape_json)
        self.assertEqual(shape_json["dims"], [4,]) 
********************************************
********************************************
def testPutCompound(self):
        print("testPutCompoundType", self.base_domain)
        headers = helper.getRequestHeaders(domain=self.base_domain)
        req = self.endpoint + '/'

        # Get root uuid
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        root_id = rspJson["root"]
        helper.validateId(root_id)

        fields = ({'name': 'temp', 'type': 'H5T_STD_I32LE'},
                  {'name': 'pressure', 'type': 'H5T_IEEE_F32LE'})
        datatype = {'class': 'H5T_COMPOUND', 'fields': fields }
        value = (42, 0.42)

        #
        #create compound scalar attribute
        #
        attr_name = "attr0d"
        payload = {'type': datatype, "value": value}
        req = self.endpoint + "/groups/" + root_id + "/attributes/" + attr_name
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)  # create attribute


        # read back the attribute
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("value" in rspJson)
        self.assertTrue("type" in rspJson)
        rsp_type = rspJson["type"]
        self.assertTrue("class" in rsp_type)
        self.assertTrue(rsp_type["class"], 'H5T_COMPOUND')
        self.assertTrue("fields" in rsp_type)
        rsp_fields = rsp_type["fields"]
        self.assertEqual(len(rsp_fields), 2)
        rsp_field_0 = rsp_fields[0]
        self.assertTrue("type" in rsp_field_0)
        self.assertEqual(rsp_field_0["type"], 'H5T_STD_I32LE')
        self.assertTrue("name" in rsp_field_0)
        self.assertEqual(rsp_field_0["name"], "temp")
        rsp_field_1 = rsp_fields[1]
        self.assertTrue("type" in rsp_field_1)
        self.assertEqual(rsp_field_1["type"], 'H5T_IEEE_F32LE')
        self.assertTrue("name" in rsp_field_1)
        self.assertEqual(rsp_field_1["name"], "pressure")

        self.assertTrue("shape" in rspJson)
        rsp_shape = rspJson["shape"]
        self.assertTrue("class" in rsp_shape)
        self.assertEqual(rsp_shape["class"], 'H5S_SCALAR')

        self.assertTrue("value" in rspJson)
        self.assertEqual(rspJson["value"], [42, 0.42]) 
********************************************
********************************************
def testPutObjReference(self):
        print("testPutObjReference", self.base_domain)
        headers = helper.getRequestHeaders(domain=self.base_domain)
        req = self.endpoint + '/'

        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        root_id = rspJson["root"]

        # create group "g1"
        payload = { 'link': { 'id': root_id, 'name': 'g1' } }
        req = helper.getEndpoint() + "/groups"
        rsp = requests.post(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)
        rspJson = json.loads(rsp.text)
        g1_id = rspJson["id"]
        self.assertTrue(helper.validateId(g1_id))
        self.assertTrue(g1_id != root_id)

        # create group "g2"
        payload = { 'link': { 'id': root_id, 'name': 'g2' } }
        req = helper.getEndpoint() + "/groups"
        rsp = requests.post(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)
        rspJson = json.loads(rsp.text)
        g2_id = rspJson["id"]
        self.assertTrue(helper.validateId(g1_id))
        self.assertTrue(g1_id != g2_id)

        # create attr of g1 that is a reference to g2
        ref_type = {"class": "H5T_REFERENCE",
                    "base": "H5T_STD_REF_OBJ"}
        attr_name = "g1_ref"
        value = "groups/" + g2_id
        data = { "type": ref_type, "value": value }
        req = self.endpoint + "/groups/" + g1_id + "/attributes/" + attr_name
        rsp = requests.put(req, data=json.dumps(data), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # read back the attribute and verify the type, space, and value
        req = self.endpoint + "/groups/" + g1_id + "/attributes/" + attr_name
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("type" in rspJson)
        rsp_type = rspJson["type"]
        self.assertTrue("base" in rsp_type)
        self.assertEqual(rsp_type["base"], 'H5T_STD_REF_OBJ')
        self.assertTrue("class" in rsp_type)
        self.assertTrue(rsp_type["class"], 'H5T_REFERENCE')
        self.assertTrue("shape" in rspJson)
        rsp_shape = rspJson["shape"]
        self.assertTrue("class" in rsp_shape)
        self.assertEqual(rsp_shape["class"], 'H5S_SCALAR')
        self.assertTrue("value" in rspJson)
        self.assertEqual(rspJson["value"], value) 
********************************************
********************************************
def testPutNoData(self):
        # Test PUT value for 1d attribute without any data provided
        print("testPutNoData", self.base_domain)

        headers = helper.getRequestHeaders(domain=self.base_domain)
        req = self.endpoint + '/'

        # Get root uuid
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        root_uuid = rspJson["root"]
        helper.validateId(root_uuid)

        # create attr
        fixed_str_type = {"charSet": "H5T_CSET_ASCII",
                "class": "H5T_STRING",
                "length": 7,
                "strPad": "H5T_STR_NULLPAD" }
        data = { "type": fixed_str_type, "shape": 4 }
        attr_name = "str_attr"
        req = self.endpoint + "/groups/" + root_uuid + "/attributes/" + attr_name
        rsp = requests.put(req, data=json.dumps(data), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # read attr
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("value" in rspJson)
        self.assertTrue(rspJson["value"] is None)
        self.assertTrue("type" in rspJson)
        type_json = rspJson["type"]
        self.assertTrue("class" in type_json)
        self.assertEqual(type_json["class"], "H5T_STRING")
        self.assertTrue("length" in type_json)
        self.assertEqual(type_json["length"], 7)

        # create attr with 2D float type
        data = {"type": {"class": "H5T_FLOAT", "base": "H5T_IEEE_F32LE"},"shape": [2,3]}
        attr_name = "float_attr"
        req = self.endpoint + "/groups/" + root_uuid + "/attributes/" + attr_name
        rsp = requests.put(req, data=json.dumps(data), headers=headers)
        self.assertEqual(rsp.status_code, 201) 
********************************************
********************************************
def testPutIntegerArray(self):
        # Test PUT value for 1d attribute with list of integers
        print("testPutIntegerArray", self.base_domain)

        headers = helper.getRequestHeaders(domain=self.base_domain)
        req = self.endpoint + '/'

        # Get root uuid
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        root_uuid = rspJson["root"]
        helper.validateId(root_uuid)

        # create attr
        value = [2,3,5,7,11,13]
        data = { "type": 'H5T_STD_I32LE', "shape": 6, "value": value}
        attr_name = "int_arr_attr"
        req = self.endpoint + "/groups/" + root_uuid + "/attributes/" + attr_name
        rsp = requests.put(req, data=json.dumps(data), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # read attr
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("value" in rspJson)
        self.assertEqual(rspJson["value"], value)
        self.assertTrue("type" in rspJson)
        type_json = rspJson["type"]
        self.assertTrue("class" in type_json)
        self.assertEqual(type_json["base"], "H5T_STD_I32LE")
        self.assertTrue("shape" in rspJson)
        shape_json = rspJson["shape"]
        self.assertTrue("class" in shape_json)
        self.assertTrue(shape_json["class"], 'H5S_SIMPLE')
        self.assertTrue("dims" in shape_json)
        self.assertTrue(shape_json["dims"], [6])

        # try creating an array where the shape doesn't match data values
        data = { "type": 'H5T_STD_I32LE', "shape": 5, "value": value}
        attr_name = "badarg_arr_attr"
        req = self.endpoint + "/groups/" + root_uuid + "/attributes/" + attr_name
        rsp = requests.put(req, data=json.dumps(data), headers=headers)
        self.assertEqual(rsp.status_code, 400)  # Bad request 
********************************************
********************************************
def testCreateLinkedDomain(self):
        target_domain = self.base_domain + "/target_domain.h5"
        print("testCreateLinkedDomain", target_domain)
        headers = helper.getRequestHeaders(domain=target_domain)
        req = helper.getEndpoint() + '/'

        rsp = requests.put(req, headers=headers)
        self.assertEqual(rsp.status_code, 201)
        rspJson = json.loads(rsp.text)
        for k in ("root", "owner", "acls", "created", "lastModified"):
             self.assertTrue(k in rspJson)

        root_id = rspJson["root"]

        # do a get on the new domain
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        for k in ("root", "owner"):
             self.assertTrue(k in rspJson)
        # we should get the same value for root id
        self.assertEqual(root_id, rspJson["root"])

        # create new domain linked with the existing root
        linked_domain = self.base_domain + "/linked_domain.h5"
        print("testCreateLinkedDomain - linked domain", linked_domain)
        headers = helper.getRequestHeaders(domain=linked_domain)
        body = {"linked_domain": target_domain }
        rsp = requests.put(req, data=json.dumps(body), headers=headers)

        self.assertEqual(rsp.status_code, 201)
        rspJson = json.loads(rsp.text)
        for k in ("root", "owner", "acls", "created", "lastModified"):
             self.assertTrue(k in rspJson)
        self.assertEqual(rspJson["root"], root_id)

        # delete the target domain but keep the root
        headers =  helper.getRequestHeaders(domain=target_domain)
        body = { "keep_root": 1}
        rsp = requests.delete(req, data=json.dumps(body), headers=headers)
        self.assertEqual(rsp.status_code, 200)

        # verify we can access the root group under the linked domain
        headers = helper.getRequestHeaders(domain=linked_domain)
        root_req =  helper.getEndpoint() + "/groups/" + root_id
        rsp = requests.get(root_req, headers=headers)
        self.assertEqual(rsp.status_code, 200) 
********************************************
********************************************
def testDeleteFolderWithChildren(self):

        folder_name = "testDeleteFolder"
        domain_name = "myfile"
        domain = self.base_domain + "/" + folder_name
        print("testCreateFolder", domain)
        headers = helper.getRequestHeaders(domain=domain)
        req = helper.getEndpoint() + '/'
        body = {"folder": True}
        rsp = requests.put(req, data=json.dumps(body), headers=headers)
        self.assertEqual(rsp.status_code, 201)
        rspJson = json.loads(rsp.text)
        for k in ("owner", "acls", "created", "lastModified"):
             self.assertTrue(k in rspJson)
        self.assertFalse("root" in rspJson)  # no root -> folder

        # verify that putting the same domain again fails with a 409 error
        rsp = requests.put(req, data=json.dumps(body), headers=headers)
        self.assertEqual(rsp.status_code, 409)

        # do a get on the new folder
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)

        self.assertTrue("owner" in rspJson)
        self.assertTrue("class" in rspJson)
        self.assertEqual(rspJson["class"], "folder")

        # create a child domain
        domain = self.base_domain + "/" + folder_name + "/" + domain_name
        headers = helper.getRequestHeaders(domain=domain)
        rsp = requests.put(req, headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # try delete the folder
        domain = self.base_domain + "/" + folder_name
        headers = helper.getRequestHeaders(domain=domain)
        req = helper.getEndpoint() + '/'
        body = {"folder": True}
        rsp = requests.delete(req, headers=headers)
        # should get 409
        self.assertEqual(rsp.status_code, 409)

        # delete the child domain
        domain = self.base_domain + "/" + folder_name + "/" + domain_name
        headers = helper.getRequestHeaders(domain=domain)
        rsp = requests.delete(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)

        # try delete the folder
        domain = self.base_domain + "/" + folder_name
        headers = helper.getRequestHeaders(domain=domain)
        req = helper.getEndpoint() + '/'
        body = {"folder": True}
        rsp = requests.delete(req, headers=headers)
        self.assertEqual(rsp.status_code, 200) 
********************************************
********************************************
def testPutAcl(self):
        print("testPutAcl", self.base_domain)
        headers = helper.getRequestHeaders(domain=self.base_domain)

        # create an ACL for "test_user2" with read and update access
        req = helper.getEndpoint() + '/acls/test_user2'
        perm = {"read": True, "update": True}

        rsp = requests.put(req, headers=headers, data=json.dumps(perm))
        self.assertEqual(rsp.status_code, 201)

        # fetch the acl and verify it has been updated
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rsp_json = json.loads(rsp.text)
        self.assertTrue("acl" in rsp_json)
        self.assertTrue("hrefs" in rsp_json)
        acl = rsp_json["acl"]
        self.assertEqual(len(acl.keys()), len(acl_keys) + 2)  # acl_keys + "domain" + "username"

        for k in acl_keys:
            self.assertTrue(k in acl)
            if k in ("read", "update"):
                self.assertEqual(acl[k], True)
            else:
                self.assertEqual(acl[k], False)

        # The ACL should be fetchable by test_user2...
        req = helper.getEndpoint() + '/acls/test_user2'
        headers = helper.getRequestHeaders(domain=self.base_domain, username="test_user2")
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200) # ok

        # The default ACL should be fetchable by test_user2 as well...
        if config.get("default_public"):
            req = helper.getEndpoint() + '/acls/default'
            headers = helper.getRequestHeaders(domain=self.base_domain, username="test_user2")
            rsp = requests.get(req, headers=headers)
            self.assertEqual(rsp.status_code, 200) # ok

        # test_user2 shouldn't be able to read test_user1's ACL
        req = helper.getEndpoint() + '/acls/test_user1'
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 403) # Forbidden 
********************************************
********************************************
def testPutAttributeDatatype(self):
        # Test creation/deletion of datatype obj

        print("testPutAttributeDatatype", self.base_domain)
        headers = helper.getRequestHeaders(domain=self.base_domain)
        req = self.endpoint + '/'

        # Get root uuid
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        root_uuid = rspJson["root"]
        helper.validateId(root_uuid)

        # create a committed type obj
        data = { "type": "H5T_IEEE_F32LE" }
        req = self.endpoint + '/datatypes'
        rsp = requests.post(req, data=json.dumps(data), headers=headers)
        self.assertEqual(rsp.status_code, 201)
        rspJson = json.loads(rsp.text)
        self.assertEqual(rspJson["attributeCount"], 0)
        ctype_id = rspJson["id"]
        self.assertTrue(helper.validateId(ctype_id))

        # link the new datatype
        name = "dtype_with_attribute"
        req = self.endpoint + "/groups/" + root_uuid + "/links/" + name
        payload = {"id": ctype_id}
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)

        # add an attribute
        attr_name = "attr"
        attr_payload = {'type': 'H5T_STD_I32LE', 'value': 42}
        req = self.endpoint + "/datatypes/" + ctype_id + "/attributes/" + attr_name
        rsp = requests.put(req, data=json.dumps(attr_payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)  # created

        # read back the obj
        req = self.endpoint + '/datatypes/' + ctype_id
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("id" in rspJson)
        self.assertEqual(rspJson["id"], ctype_id)
        self.assertFalse("attributes" in rspJson)

        # read back the obj with attributes
        params = {"include_attrs": 1}
        rsp = requests.get(req, headers=headers, params=params)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertTrue("id" in rspJson)
        self.assertEqual(rspJson["id"], ctype_id)
        self.assertTrue("attributes" in rspJson)
        attrs = rspJson["attributes"]
        self.assertTrue("attr" in attrs) 
********************************************
********************************************
def testSoftLink(self):
        domain = self.base_domain + "/testSoftLink.h5"
        print("testSoftLink", domain)
        helper.setupDomain(domain)
        headers = helper.getRequestHeaders(domain=domain)
        req = helper.getEndpoint() + '/'

        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        root_id = rspJson["root"]

        # get root group and check it has no links
        req = helper.getEndpoint() + "/groups/" + root_id
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertEqual(rspJson["linkCount"], 0)  # no links

        # create softlink
        link_title = 'softlink'
        target_path = 'somewhere'
        req = helper.getEndpoint() + "/groups/" + root_id + "/links/" + link_title
        payload = {"h5path": target_path}
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)  # created

        # get root group and check it has one link
        req = helper.getEndpoint() + "/groups/" + root_id
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertEqual(rspJson["linkCount"], 1)  # no links

        # get the link
        req = helper.getEndpoint() + "/groups/" + root_id + "/links/" + link_title
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)  # should get the softlink
        rspJson = json.loads(rsp.text)
        self.assertTrue("created" in rspJson)
        self.assertTrue("lastModified" in rspJson)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("link" in rspJson)
        rspLink = rspJson["link"]
        self.assertEqual(rspLink["title"], link_title)
        self.assertEqual(rspLink["class"], "H5L_TYPE_SOFT")
        self.assertEqual(rspLink["h5path"], target_path) 
********************************************
********************************************
def testExternalLink(self):
        domain = self.base_domain + "/testExternalLink.h5"
        print("testExternalLink", domain)
        helper.setupDomain(domain)
        headers = helper.getRequestHeaders(domain=domain)
        req = helper.getEndpoint() + '/'

        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        root_id = rspJson["root"]

        # get root group and check it has no links
        req = helper.getEndpoint() + "/groups/" + root_id
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertEqual(rspJson["linkCount"], 0)  # no links

        # create external link
        target_domain = self.base_domain + '/external_target.h5'
        target_path = 'somewhere'
        link_title = 'external_link'
        req = helper.getEndpoint() + "/groups/" + root_id + "/links/" + link_title
        payload = {"h5path": target_path, "h5domain": target_domain}
        rsp = requests.put(req, data=json.dumps(payload), headers=headers)
        self.assertEqual(rsp.status_code, 201)  # created

        # get root group and check it has one link
        req = helper.getEndpoint() + "/groups/" + root_id
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)
        rspJson = json.loads(rsp.text)
        self.assertEqual(rspJson["linkCount"], 1)  # no links

        # get the link
        req = helper.getEndpoint() + "/groups/" + root_id + "/links/" + link_title
        rsp = requests.get(req, headers=headers)
        self.assertEqual(rsp.status_code, 200)  # should get the softlink
        rspJson = json.loads(rsp.text)
        self.assertTrue("created" in rspJson)
        self.assertTrue("lastModified" in rspJson)
        self.assertTrue("hrefs" in rspJson)
        self.assertTrue("link" in rspJson)
        rspLink = rspJson["link"]
        self.assertEqual(rspLink["title"], link_title)
        self.assertEqual(rspLink["class"], "H5L_TYPE_EXTERNAL")
        self.assertEqual(rspLink["h5path"], target_path)
        self.assertEqual(rspLink["h5domain"], target_domain) 
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
********************************************
